\section{A hierarchy of provers}

What we want to build is a prover able to prove not only equalities for \code{Nat}, \code{List}, or any specific type, but for various datatypes, and for various properties. With some abstraction, we can solve the problem of generating proof of equality for many datatypes like natural numbers, lists, matrices, and many others \emph{all at once} by implementing a very generic hierarchy of provers that can prove equalities of terms for semi-groups, monoids, commutative-monoids, groups, commutative-groups, semi-rings and rings. These tactics will be usable on any type that satisfies the properties of one of these algebraic structure. The properties of a given algebraic algebraic structure will be expressed in a corresponding interface (sometimes called typeclass in other progamming languages). This interface will extend the interface from which it naturally inherits. For example, the \code{Group} interface will extend the \code{Monoid} one. We will end up with a hierarchy of interfaces and will write one tactic for each of these interface. At every level of the hierarchy, we will be able to work on any type, being given that an implementation of the corresponding interface is provided.

	\subsection{Proving equivalences instead of equalities}
	
With some reasonable additional effort, we could produce a collection of tactics for proving equivalences instead of only equalities. The machinery would be very similar, and if we do the right abstraction from the start, we could gain one more degree of genericity, with the freedom of choosing the equivalence relation (which can of course be the usual equality). The user could define his own equivalence notion, as long as he's able to provide the proofs of the properties of the algebraic structure he wants to use. Let's call \code{c} the \emph{carrier} type, ie, the type on which we want to prove equivalences. The equivalence relation on \code{c} has the following profile $(\simeq) : \code{c} \rightarrow \code{c} \rightarrow \code{Type}$\footnote{This \code{Type} would be a \code{Prop} in systems --like Coq-- that make a distinction between the world of computations and the world of logical statements}, and has to be accompanied by the usual properties of reflexivity, symmetry and transitivity.

All of our tactics will require to have a way of testing this equivalence between elements of the underlying set, that is to say, a way to test equivalence between constants. For this reason, we define a notion of \code{Set}\footnote{This notion of set isn't a formalisation of sets and their elements, but is only a way to talk about the carrier type and an equivalence relation, sometimes called Setoid}, which only requires the definition of the equivalence relation and this equivalence test \code{set\_eq}. All the interfaces representing the algebraic structures will later extend \code{Set} :

\begin{lstlisting}
interface Set c where
    ($\simeq$) : c $\rightarrow$ c $\rightarrow$ Type
    refl : (x:c) $\rightarrow$ x $\simeq$ x
    sym : {x:c} $\rightarrow$ {y:c} $\rightarrow$ (x $\simeq$ y) $\rightarrow$ (y $\simeq$ x)
    trans : {x:c} $\rightarrow$ {y:c} $\rightarrow$ {z:c} $\rightarrow$ (x $\simeq$ y) $\rightarrow$ (y $\simeq$ z) $\rightarrow$ (x $\simeq$ z)    
    set_eq : (x:c) $\rightarrow$ (y:c) $\rightarrow$ Maybe (x $\simeq$ y)
\end{lstlisting}

If the user wants to prove propositional equalities, they will instantiate $(\simeq)$ with the built-in $(=)$ during the definition of the \code{Set} implementation.
Note that $(\simeq)$ is only weakly decidable in the sense that \code{set\_eq} only produces a proof when the two elements are equivalent, but it doesn't produce a proof of dis-equivalence when they are different -- instead, it simply produces the value \code{Nothing}--. That is normal, as we want to generate proof of equivalence, and not to generate counter examples for proving dis-equivalence, which is another problem.

There is no tactic associated to \code{Set}, since we have no operations and no properties associated to this structure. Therefore, equivalences in a \code{Set} are just the ``syntactic equivalences", and they can simply be proven with \code{refl}\footnote{\code{refl} should not be confused with \code{Refl}, the only constructor of $=$. But of course, when $(\simeq)$ is instantiated with the equality $=$, the instantiation of \code{refl} will be \code{Refl}. Therefore, \code{refl} of the interface \code{Set} is a generalisation of \code{Refl}}.

%The kernel of our machinery will be a function $norm$ that will be composed of %multiples functions composed together, that each do a part of the normalisation %and that generate the proof of equivalence between old and new concrete values %for this part of the normalisation.

Working with equivalences instead of equalities will bring one complication. These proofs of correctness that we will have to produce by hand will be slightly more complicated, as we won't be able to use Idris' ``rewrite" command, that enables to rewrite a subterm of a term by another one, provided that the two subterms are equal. More precisely, if we need to prove $P\ x$, and have a proof of equivalence $pr:x \simeq x'$, we can't use ``rewrite pr" in order to transform the goal into $P\ x'$, as we would normally do if the available proof would be an equality $x=x'$. This is a classical problem of working within a setoid, which is slightly mitigated when the programming language offers a good support for rewriting terms in setoids. However, Idris isn't equipped with any support for setoids, and everything will have to be done by hand. For this reason, we define the lemma :

$\code{eq\_preserves\_eq} : \{c:\code{Type}\} \rightarrow \{\code{Set}\ c\} \rightarrow (x:c) \rightarrow (y:c) \rightarrow (c1:c) \rightarrow (c2:c) \rightarrow (x \simeq c1) \rightarrow (y \simeq c2) \rightarrow (c1 \simeq c2) \rightarrow (x \simeq y)$.

\begin{proof}
We have $x \simeq c1$ and we have $c1 \simeq c2$, therefore we have $x \simeq c2$ by the use of the axiom of transitivity (of the interface \code{Set}). \

We also have $y \simeq c2$ and therefore we have $c2 \simeq y$ by the use of the symmetry axiom (of the interface \code{Set}). \

Now that we have these fresh proofs of $x \simeq c2$ and of $c2 \simeq y$, we can use one last time the property of transitivity, in order to get a proof of $x \simeq y$.
\qed
\end{proof}

This lemma says that the equivalence preserves the equivalence, which means that in order to prove $x \simeq y$, we can prove the (hopefully) simplest problem $c1 \simeq c2$, provided that $x \simeq c1$ and that $y \simeq c2$. This lemma will be intensively used for building the proofs of correctness of the normalisation functions that will help to build the desired proof of $x \simeq y$. It will be basically used as a replacement to the ``rewrite" tactic that we can no longer use since we are dealing with equivalence and not equality.

		\subsection{Hierarchy of interfaces}

The operations, constants and properties of each algebraic structure will be described in a dedicated interface. The first structure, completely trivial, is the magma. A magma is a structure built on top of \code{Set}, that just adds a \code{Plus} operation, and no specific properties about it :

\begin{lstlisting}
interface Set c => Magma c where
    + : c $\rightarrow$ c $\rightarrow$ c
\end{lstlisting}

This code means that a type \code{c} (for $carrier$) is a \code{Magma} if it is already a \code{Set} (ie, it is equipped with the equivalence relation $\simeq$ and the equivalence test \code{set\_eq}), and if it has a $+$ operation.
In fact, there is an additional requirement that will apply to all operators (in this case, the $+$ operation), which is that they need to be ``compatible" with the equivalence relation, which is expressed by the following axiom for $+$ : \\ $\code{Plus\_preserves\_equiv} : \{c:\code{Type}\} \rightarrow \{\code{Magma}\ c\} \rightarrow\{c1:c\} \rightarrow \{c2:c\} \rightarrow \{c1':c\} \rightarrow \{c2':c\} \rightarrow (c1 \simeq c1') \rightarrow (c2 \simeq c2') \rightarrow ((c1+c2) \simeq (c1'+c2'))$ \\ 
This requirement comes from the fact that we're dealing with any equivalence relation. The user is free to use his own equivalence relation, but it should be compatible with the operations that he is using.
As it was the case for \code{Set}, there is no tactic to write for \code{Magma}, because there is no property at all : all equivalences are again syntactic equivalences, and they can all be proven by \code{refl}. 

A semi-group is a magma (ie, it still has a \code{Plus} operation), but moreover it has the property of associativity for this operation. 

\begin{lstlisting}
interface Magma c => SemiGroup c where
    Plus_assoc : (c1:c) $\rightarrow$ (c2:c) $\rightarrow$ (c3:c) $\rightarrow$ ((c1+c2)+c3 $\simeq$ c1+(c2+c3))
\end{lstlisting}

As example of magma, we can cite \code{Nat} equipped with its operation of addition and \code{List} with the operation of concatenation.

The monoid structure is a semi-group with the property of neutral element for a distinguished element called \code{Zero}.

\begin{lstlisting}
interface SemiGroup c => Monoid c where
    Zero : c    
    Plus_neutral_1 : (c1:c) $\rightarrow$ (Zero+c1 $\simeq$ c1)    
    Plus_neutral_2 : (c1:c) $\rightarrow$ (c1+Zero $\simeq$ c1)
\end{lstlisting}

The hierarchy of interfaces continues, but we only show the \code{Group} interface for reasons of space :

\begin{lstlisting}
interface Monoid c => Group c where
    Minus : c $\rightarrow$ c $\rightarrow$ c
    Neg : c $\rightarrow$ c
    Minus_simpl : (c1:c) $\rightarrow$ (c2:c) $\rightarrow$ Minus c1 c2 $\simeq$ c1 + (Neg c2) 
    Plus_inverse : (c1:c) $\rightarrow$ (c1 + (Neg c1) $\simeq$ Zero, (Neg c1) + c1 $\simeq$ Zero)
\end{lstlisting}

The notion of group uses two new operations (\code{Neg} and \code{Minus}), but \code{Minus} is not primitive and can be simplified with $+$ and \code{Neg}. The important property of a group is that every element \code{c1} must admit \code{Neg\ c1} as inverse element for the $+$ operation.

\subsection{Type-safe reflection}
\label{sect:typeSafeReflection}
			
When trying to prove an equivalence $x \simeq y$, the universally-quantified variables have been abstracted, and they are now part of the context. Our tatics will work by normalising both sides of the ``potential equivalence" $x \simeq y$, and afterwards comparing them with a syntactic equality test. The problem is that such a normalisation function will need to do different treatments for a a variable, for a constant and for an operator. For this reason, we will need to work by reflection. There will be one type of reflected terms for each algebraic structure. The novelty is not the use of reflected terms (this has already been done, as discussed in section~\ref{sect:relatedWork}), but is the use of a type-safe reflection mechanism where we index the reflected terms by the concrete value that they represent. Each of these datatype is parametrised over a type \code{c}, which is the real type on which we want to prove equalities (the \emph{carrier} type). It is also indexed over an implementation of the corresponding interface for \code{c} (we usually call it \code{p}, because it behaves as a \emph{proof} telling that the structure \code{c} has the desired properties), indexed over a context of abstracted variables (a vector $\Gamma$ of \code{n} elements of type \code{c}), and most important it is also indexed over a value of type \code{c}, which is precisely the concrete value being encoded.

A magma is only equipped with the operation of addition. Thus, we only have three concepts to express in order to reflect terms in a magma : constants, variables, and additions.

\begin{lstlisting}
data ExprMa : Magma c $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
    ConstMa : (p : Magma c) $\rightarrow$ ($\Gamma$:Vect n c) $\rightarrow$ (c1:c)  $\rightarrow$ ExprMa p $\Gamma$ c1 
    PlusMa : {p : Magma c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprMa p $\Gamma$ c1 $\rightarrow$ ExprMa p $\Gamma$ c2 $\rightarrow$ ExprMa p $\Gamma$ (c1+c2) 
    VarMa : (p:Magma c) $\rightarrow$ ($\Gamma$:Vect n c)
         $\rightarrow$ (i:Fin n) $\rightarrow$ ExprMa p $\Gamma$ (index i $\Gamma$)
\end{lstlisting}


For an expression $e_x\ :\ \code{ExprMa}\ \Gamma\ x$, we will say that $e_x$ denotes (or encodes) the value $x$ in the context $\Gamma$.
When an expression is a variable \code{(VarMa\ \_\ $\Gamma$\ i)}, the denoted value is the corresponding variable in the context, ie, \code{(index\ i\ $\Gamma$)}.
The expression \code{(ConstMa\ \_\ $\Gamma$\ k)} denotes the constant $k$ of type $c$. Finally, if $e_x$ is an expression encoding the concrete value $x$, and $e_y$ is an expression encoding the concrete value $y$, then the expression \code{PlusMa\ $e_x$\ $e_y$} denotes the concrete value $(x + y)$. Because the reflected terms embed their corresponding inputs, they are guarantee to be faithful representations of them.

There is no additional operations in semi-groups and monoids, so the datatypes that reflect terms in these two structures will have exactly the same shape as the one for magma that we have given above.
However, the datatype for reflected terms in groups will have to introduce two new constructors for the \code{Neg} and \code{Minus} operations :

\begin{lstlisting}
data ExprG :  Group c $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
    ConstG : (p : Group c) $\rightarrow$ ($\Gamma$:Vect n c) $\rightarrow$ (c1:c) $\rightarrow$ ExprG p $\Gamma$ c1
    PlusG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ c2 $\rightarrow$ ExprG p $\Gamma$ (c1+c2)
    MinusG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ c2 $\rightarrow$ ExprG p $\Gamma$ (Minus c1 c2)
    NegG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ (Neg c1)
    VarG : (p : Group c) $\rightarrow$ {$\Gamma$:Vect n c} 
         $\rightarrow$ (i:Fin n) $\rightarrow$ ExprG p $\Gamma$ (index i $\Gamma$)
\end{lstlisting}


The index of type $c$ (the real value encoded by an expression) is always expressed by using the lookup function \code{index} and the available operations in the implementation $p$, which for a group are $+$, \code{Minus} and \code{Neg}.

	\subsection{A correct by construction approach}
\label{sect:correctByConstruction}

We want to write the normalisation function on a \emph{correct by construction} way, which means that no additional proof should be required after the definition of the function. Thus, \code{norm} will produce the proof that the new expression freshly produced has the same interpretation as the original expression, and this will be made easier by the fact that all the datatypes for reflecting terms (\code{ExprMa}, \code{ExprG}, \code{ExprR}, etc) are indexed over the real --concrete-- value : a term of type $\code{Expr}\ \Gamma\ x$ is the encoding of the concrete value $x$ in the context $\Gamma$.
Thus, for each algebraic structure, the type of the function \code{norm} will look like this : \\
$\code{norm} :\ \code{Expr}\ \Gamma\ x\ \rightarrow\ (x'\ \code{**}\ (\code{Expr}\ \Gamma\ x',\ x\ \simeq\ x'))$ \\
Every function of normalisation \code{norm} will produce a dependent pair : the new concrete value $x'$, and a pair made of an $\code{Expr}\ \Gamma\ x'$ which is the new encoded term indexed over the new concrete value we have just produced, and a proof that old and new concrete values $x$ and $x'$ are equivalent.

This function doesn't only produce an $\code{Expr}\ \Gamma\ x$ (with an internal conversion of type) because the proof of $x \simeq x'$ is an essential component for building the desired proof of $x \simeq y$, which is the main problem that we are trying to solve.


We will explain how the tactic works by commenting the construction of the tactic for groups, but the other algebraic structures will be equipped similarly. The equivalence we are trying to prove is $x \simeq y$, where $x$ and $y$ are elements of the type $c$, which  simulates a group. The reflected term for $x$ will be denoted $e_x$, and this term will have the type $\code{ExprG}\ p\ \Gamma\ x$, which means that $e_x$ is guaranteed to be the encoding of $x$ because $e_x$ is precisely indexed over the real value $x$.
We've got similar things for $y$, which is encoded by $e_y$, and its type is indexed over the real value $y$.
Running the normalisation procedure on $e_x$ will produce the normal form $e_{x'}$ of type $\code{ExprG}\ p\ \Gamma\ x'$ and a proof $p_x$ of $x \simeq x'$ while running the normalisation procedure on $e_y$ will produce the normal form $e_{y'}$ of type $\code{ExprG}\ p\ \Gamma\ y'$ and a proof $p_y$ of $y \simeq y'$.
It is now enough to compare $e_{x'}$ and $e_{y'}$ using a standard syntactic equality test \code{exprG\_eq} because these two expressions are now supposed to be in normal form.

Now, if the two normalised expressions $e_{x'}$ and $e_{y'}$ are equal, then they necessary have the same type\footnote{We are working with the heterogeneous equality JMeq by default in Idris, but as always, the only way to have a proof of a:A = b:B is when A=B}, and therefore $x'=y'$, which implies $x' \simeq y'$ because the propositional equality is more restrictive than an equivalence relation.
By using the two equivalences $x \simeq x'$ and $y \simeq y'$ that we obtained during the normalisations in the new equivalence $x' \simeq y'$ with the use of \code{eq\_preserves\_eq}, we can get a proof of $x \simeq y$. This is what the function \code{buildProof} is doing :

\begin{lstlisting}
buildProofGroup : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {x : c} $\rightarrow$ {y : c} 
  $\rightarrow$ {x':c} $\rightarrow$ {y':c} $\rightarrow$ (ExprG p $\Gamma$ x') $\rightarrow$ (ExprG p $\Gamma$ y') 
  $\rightarrow$ (x $\simeq$ x') $\rightarrow$ (y $\simeq$ y') $\rightarrow$ (Maybe (x $\simeq$ y))
buildProofGroup p ex' ey' px py with (exprG_eq p ex' ey')
    buildProofGroup p ex' ex' px py | Just refl = 
                Just(eq_preserves_eq x y x' y' lp rp e1_equiv_e2)
    buildProofGroup p ex' ey' px py | Nothing = Nothing
\end{lstlisting}

The argument of type $\code{ExprG}\ p\ \Gamma\ x'$ is the normalised reflected left hand size of the equivalence, which represents the value $x'$. Before the normalisation, the reflected LHS was reflecting the value $x$. The same applies to the right-hand side. The function also expects proofs of $x \simeq x'$ and of $y \simeq y'$, that we will be able to provide because the normalisation function has produced them while it was building the normalised reflected terms.


Finally, the main function which tries to prove the equivalence $x \simeq y$ has to normalise the two reflected terms encoding the left and the right hand side, and to use the function \code{buildProof} in order to compose the two proofs that have been obtained :

\begin{lstlisting}
groupDecideEq : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {x : c} $\rightarrow$ {y : c} 
               $\rightarrow$ (ExprG p $\Gamma$ x) $\rightarrow$ (ExprG p $\Gamma$ y) $\rightarrow$ Maybe (x $\simeq$ y)
groupDecideEq p ex ey =
  let (x' ** (ex', px)) = groupNormalise p ex in
  let (y' ** (ey', py)) = groupNormalise p ey in
	     buildProofGroup p ex' ey' px py
\end{lstlisting}


The remaining part is to define the function \code{norm}. Each algebraic structure will have a function for reducing the reflected terms into their normal forms. The fact that all of these algebraic structures admit a canonical representative for any element is in fact a very nice property that we are using in order to decide equalities. Without this property, it would become a lot more complicated to decide the equivalence without brute-forcing a series of rewritings, that would have no guarantee to terminate.
As said before, the normalisation functions also produce the proof of equivalence between the initial real value (the index of the input term) and the final real value (the index of the produced term). Thus, the type of the reduction function for group is :


\begin{lstlisting}
groupNormalise : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} 
       $\rightarrow$ {x:c} $\rightarrow$ (ExprG p $\Gamma$ x) $\rightarrow$ (x' ** (ExprG p $\Gamma$ x', x $\simeq$ x'))
\end{lstlisting}


The function of normalisation has more work to do for structures with many axioms (commutative-monoids, groups, commutative-groups, semi-rings and rings), than for the easier structure (semi-groups and monoids).
The operations performed by the normalisation functions are detailed in the next section.


\subsection{Normalisation functions}
\label{sect:normalFormShape}

For the sake of completeness, we describe the normal form for rings which is our most sophisticated structure.
The normalisation function takes in input an expression expressed with sums, products, constants (zero, one...) and variables that belong to an ordered set $\mathcal{V}$ of variables. In short, the normalisation function takes in input a polynomial of multiple variables. In output, it must produce a normal form representing the same polynomial. Therefore, we have to decide a canonical representation of polynomials. Many canonical representations can be used. We decide to stick with classical mathematical conventions. The first one, is that the polynomial will be completely developed, ie, the distributivity of $*$ over $+$ will be applied until it can't be applied anymore. The advantage is the simplicity, as factorising would be significantly more complicated and we're looking for simplicity as the normalisation functions will also have to produce the proof of equivalence between the old and the new index. Because the polynomial is completely developed, at the toplevel, it is a sum :

\[
P = \sum_{i=1}^{a}\ (\prod_{j=1}^{b} Monomial_{i}^j)
\text{ where } 
Monomial_{i}^j = C_{i}^j * \prod_{k=1}^{c} Var_{i,k}^{j}
\]
with $C_{i}^j$ a constant, and $Var_{i,k}^{j}$ one of the variable that belong to $\mathcal{V}$.

One could be surprised by the fact that the normal form is a sum of product of monomial, and not directly a sum of monomials. The reason is the following. A monomial is a product of a constant $C_{i}^j$ (like $4$) and of a product of variables (like $x*y*z$). For example,  $5*(x*(y*z))$ is a monomial. Now let's consider the term $(5*(x*(y*z))) * (4*(z*z))$. This term is not a monomial, but we could be tempted to simplify it into the monomial $20*(x*(y*(z*(z*z))))$. However, that would assume that the product is always commutative, ie, that we can re-organize the subterms of a product as we want. This is the case in a commutative ring, but this does not hold for any ring. Therefore, after the full development, the polynomial is a sum of \emph{product of monomials}, and not directly a sum of monomials. The only rearrangement that can be done towards the multiplication is to check if two constants are consecutive in a product, and if so, to replace them by the constant that represents the product of them.

However, because $+$ is always a commutative operator in a ring, the different products of monomials themselves can be rearranged in different ways in this sum. That will be done at the underneath level for commutative groups if we can provide an ordering on products of monomials. This ordering will be defined by using an ordering on monomials, that we will call \code{isBefore\_mon}, which works by looking at the order of variables for comparing two monomials $Monomial_{i}^{j}$ and $Monomial_{i'}^{j'}$.
\[
Monomial_{i}^{j} = C_{i}^{j} * (Var_{i,1}^{j} * \prod_{k=2}^{c} Var_{i,k}^{j})
\text{ and }
Monomial_{i'}^{j'} = C_{i'}^{j'} * (Var_{i',1}^{j'} * \prod_{k=2}^{c'} Var_{i',k}^{j'})
\]

The order between these two monomials will be decided by looking at the order between the first variables $Var_{i,1}^{j}$ and $Var_{i',1}^{j'}$. In the case where both monomials start with the same variable, we continue by inspecting the remaining variables. If we can't continue because one of the two monomials has less variables than the other, then we decide that the one with fewer variables comes first (exactly like the word ``house" comes before the word ``housemate" with the lexicographic order).

We can now build the order on \emph{product of monomials} that we needed. We name it \code{isBefore}. Given two product of monomials $Prod_{i}$ and $Prod_{i'}$ we need to decide which one comes first. We will obviously use the order \code{isBefore\_mon} on the first monomials of these two products. If it says that $Monomial_{i}^{1}$ comes before $Monomial_{i'}^{1}$, then we decide that $Prod_{i}$ comes before $Prod_{i'}$. Conversely, if that's $Monomial_{i'}^{1}$ that comes first, then we decide that $Prod_{i'}$ comes first. However, if $Monomial_{i}^{1}$ and $Monomial_{i'}^{1}$ have exactly the same position in the order, then we continue by inspecting the remaining monomials with a recursive call. As previously, if we can't continue because one of the two products has less monomials than the other, then the one with fewer monomials will come first.

A few additional conventions had to be decided about the normal form :
\begin{itemize}

\item The top-level sum of the polynomial will be put in right-associative form : \\
$prodMon_1\ +\ (prodMon_2\ + (prodMon_3\ +\ (...\ +\ prodMon_a)))$
\item All the products that we have (the products of monomials and the products of variables), will also be written on the completely right-associative form.
\item We always simplify as much as possible with constants. That includes simplifying the addition with zero and the multiplication with the constants zero and one, doing the computations between two nearby constants, etc...
\item We always simplify the sum of an expression $e$ and its inverse $-e$ into the constant zero. 

\end{itemize}

\subsection{Reusing the provers}
\label{sect:reusabilityOfTheProvers}

One of the novelty of our work is that we haven't built just a prover for a specific algebraic structure, but we have instead built a hierarchy of provers for many algebraic structures. Morover, each prover is not isolated from the others, and the various simplifications are not duplicated at the different levels : the normalisation function of each structure uses as much as possible the normalisation function of the structure from which it inherits. For example, the normalisation on monoids reuses the normalisation on semi-groups in such a way that it does not have to deal with the associativity. For this case, it's not a problem since the datatype reflecting terms in semi-groups has the same expressivity power than the one for monoids. However, that's not the case with groups and monoids: the function of normalisation for groups that will need to call the normalisation for monoids will have to encode the negations, because they can't be directly expressed in a monoid. For this reason, we will develop some specific encodings.

The idea is that we will encode negations as variables, and we will let the monoid prover deal with them as ordinary variables. In order to achieve this goal, we need the following datatype that will help us to distinguish between a real variable and the encoding of a negation :

\begin{lstlisting}
data Variable : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
  RealVariable : ($\Gamma$:Vect n c) $\rightarrow$ (i:Fin n) $\rightarrow$ Variable $\Gamma$ (index i $\Gamma$)
  EncodingNeg : ($\Gamma$:Vect n c) $\rightarrow$ (i:Fin n) $\rightarrow$ Variable $\Gamma$ (Neg (index i $\Gamma$))
\end{lstlisting}

We only need to encode negations of variables, as the negation of a constant can be simplified into a constant. Also, there can't be a negation of something different than an atom (a variable or a constant), because the normalisation in groups has first systematically propagated \code{Neg} inside the parenthesis, following the simplification\footnote{Note that we have to be careful and not simplify it to $(-a) + (-b)$ as it would assume that $+$ is commutative} $-(a+b) = -b + -a$.

All the constructors for variables now take a \code{Variable} as parameter, instead of taking directly an element of $(\code{Fin}\ n)$. That gives the following, for groups :

\begin{lstlisting}
    VarG : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {val:c} 
           $\rightarrow$ (Variable $\Gamma$ val) $\rightarrow$ ExprG p $\Gamma$ val
\end{lstlisting}


Thanks to this encoding, we can now transform an \code{ExprG} from the group level to an \code{ExprMo} at the monoid level. A constant $(\code{ConstG}\ p\ \Gamma\ c1)$ will be transformed into the corresponding constant $(\code{ConstMo}\ p\ \Gamma\ c1)$, a \code{PlusG} into the corresponding \code{PlusMo}, a real variable into the same real variable, the negation of a constant into the resulting constant, and finally the negation of a variable $i$ into a $(\code{VarMo}\ p\ (\code{EncodingNegOfVar}\ \Gamma\ i))$.

We've used a similar technique for converting an expression from the ring level to the commutative-group level, where we have encoded the product of monomials, because the product is not an operation defined at the commutative-group level. That enables the function of normalisation for rings to benefit from the normalisation function for commutative-groups.


\subsection{Automatic reflection}
		
We've built an automatic reflection mechanism which enables to let the machine build automatically the reflected terms for us. This is not something absolutely vital, as it only simplifies the usage of the tactics, avoiding to write long encodings by hand. To do so, we've used Idris' reflection mechanism, which enables to do pattern matching on syntax, rather than on constructors. 



