\section{A hierarchy of provers}

We aim to build a prover not only for equalities on \code{Nat}, \code{List}, or
any specific type, but for generic datatypes and properties. Using
the right
abstraction, we can generate proofs of equalities for
many datatypes \emph{at once} by implementing a generic hierarchy of provers
for semi-groups, monoids,
commutative-monoids, groups, commutative-groups, semi-rings and rings.
The properties of an algebraic structure are expressed in an
interface (an interface in Idris is similar to a type class
in Haskell). This interface will extend the interface from which it
inherits; for example, \code{Group} extends \code{Monoid}.
This leads to a hierarchy of interfaces, with one tactic
for each. At every level of the hierarchy, we will be able
to work on any type, as long as there is a corresponding implementation of
the interface.

\subsection{Proving equivalences instead of equalities}
	
With some additional effort, we can produce a collection of tactics for proving
\emph{equivalences}, rather than only equalities. 
The machinery is very similar and
we gain another
degree of genericity, with the freedom of choosing the equivalence relation
(which can be the usual equality). 
The user can define their own
notion of equivalence, as long as they provide the proofs of the
properties of the relevant algebraic structure. Let's call \code{c} the
\emph{carrier} type, i.e., the type on which we want to prove equivalences. 
The
equivalence relation on \code{c} has the following profile $(\simeq) : \code{c}
\rightarrow \code{c} \rightarrow \code{Type}$\footnote{This \code{Type} would
be a \code{Prop} in systems, like Coq, that make a distinction between the
world of computations and the world of logical statements}, and must be reflexive, symmetric and transitive.

Our tactics need to be able to test this equivalence
between elements of the underlying set, that is a way of testing
equivalence of constants. We therefore define a notion of
\code{Set}\footnote{This notion of set 
is a way to talk about the carrier type and an equivalence
relation, sometimes called Setoid}, which requires the definition of the
equivalence relation and an equivalence test \code{set\_eq}. All the interfaces
representing the algebraic structures will later extend \code{Set}:

\begin{lstlisting}
interface Set c where
    ($\simeq$) : c $\rightarrow$ c $\rightarrow$ Type
    refl : (x : c) $\rightarrow$ x $\simeq$ x
    sym : {x, y : c} $\rightarrow$ (x $\simeq$ y) $\rightarrow$ (y $\simeq$ x)
    trans : {x, y, z : c} $\rightarrow$ (x $\simeq$ y) $\rightarrow$ (y $\simeq$ z) $\rightarrow$ (x $\simeq$ z)    
    set_eq : (x : c) $\rightarrow$ (y : c) $\rightarrow$ Maybe (x $\simeq$ y)
\end{lstlisting}

To prove propositional equalities, the user instantiates
$(\simeq)$ with the built-in $(=)$ when implementing \code{Set}.
Note that $(\simeq)$ is only weakly decidable in the sense
that \code{set\_eq} only produces a proof when the two elements are equivalent,
but it doesn't produce a proof of dis-equivalence when they are different,
instead producing the value \code{Nothing}. Our goal is only to generate
proofs of equivalance, not to produce counter-examples.
%
There is no tactic associated with \code{Set}, since we have no operations or
properties associated to this structure. Equivalences in a
\code{Set} are ``syntactic equivalences'' and can be
proven with \code{refl}\footnote{\code{refl} is not to be confused with
\code{Refl}, the constructor of $=$, but when $(\simeq)$ is
instantiated with the equality $=$, \code{refl} is implemented by
\code{Refl}. Therefore, \code{refl} of the interface \code{Set} is a
generalisation of \code{Refl}}.

%The kernel of our machinery will be a function $norm$ that will be composed of %multiples functions composed together, that each do a part of the normalisation %and that generate the proof of equivalence between old and new concrete values %for this part of the normalisation.

Working with equivalences instead of equalities brings one complication : the proofs
of correctness that we produce by hand cannot use Idris' ``rewrite'' mechanism,
which enables rewriting of a subterm by another one, provided that the two
subterms are propositionally equal. 
%
% EB - I think we can skip this detail
%If we need to prove $P\ x$,
%and have a proof of equivalence $pr:x \simeq x'$, we can't use ``rewrite pr" in
%order to transform the goal into $P\ x'$, as we would normally do if the
%available proof would be an equality $x=x'$. 
%
This is a classical problem of
working within a setoid, which can be mitigated by programming
language support for rewriting terms in setoids. However, Idris
is not equipped with any such support. For this reason, we define the 
following lemma, using the methods of the \code{Set} interface:

$\code{eq\_preserves\_eq} : \{c:\code{Type}\} \rightarrow \{\code{Set}\ c\} \rightarrow (x:c) \rightarrow (y:c) \rightarrow (c1:c) \rightarrow (c2:c) \rightarrow (x \simeq c1) \rightarrow (y \simeq c2) \rightarrow (c1 \simeq c2) \rightarrow (x \simeq y)$.

% EB - I took this out because, as the reviewer says, it's odd to have a
% hand proof here, and I don't think the code itself is interesting enough to
% include since it's a fairly routine use of the interface

%\begin{proof}
%We have $x \simeq c1$ and we have $c1 \simeq c2$, therefore we have $x \simeq c2$ by the use of the axiom of transitivity (of the interface \code{Set}). \

%We also have $y \simeq c2$ and therefore we have $c2 \simeq y$ by the use of the symmetry axiom (of the interface \code{Set}). \

%Now that we have these fresh proofs of $x \simeq c2$ and of $c2 \simeq y$, we can use one last time the property of transitivity, in order to get a proof of $x \simeq y$.
%\qed
%\end{proof}

This lemma says that the equivalence preserves the equivalence, which means
that in order to prove $x \simeq y$, we can prove a smaller
problem $c1 \simeq c2$, provided that $x \simeq c1$ and that $y \simeq c2$.
We will use this lemma extensively.

\subsection{Hierarchy of interfaces}

We describe operations, constants and properties of each algebraic structure 
in an interface. The first algebraic theory is \code{Magma}, which is a structure built on top of \code{Set} that adds
\code{Plus} operation, and no specific properties:

\begin{lstlisting}
interface Set c => Magma c where
    + : c $\rightarrow$ c $\rightarrow$ c
\end{lstlisting}

This code means that a type \code{c} (for $carrier$) is a \code{Magma} if it is
already a \code{Set} (ie, it is equipped with the equivalence relation $\simeq$
and the equivalence test \code{set\_eq}), and if it has a $+$ operation.  In
fact, there is an additional requirement that will apply to all operations (in
this case, the $+$ operation), which is that they need to be ``compatible'' with
the equivalence relation, which is expressed by the following axiom for $+$:
\\ $\code{Plus\_preserves\_equiv} : \{c:\code{Type}\} \rightarrow
\{\code{Magma}\ c\} \rightarrow\{c1:c\} \rightarrow \{c2:c\} \rightarrow
\{c1':c\} \rightarrow \{c2':c\} \rightarrow (c1 \simeq c1') \rightarrow (c2
\simeq c2') \rightarrow ((c1+c2) \simeq (c1'+c2'))$ 

We have this requirement because we support
any equivalence relation. The user is free to define the equivalence relation of their choice, but it should be compatible with the operations that they are using.
As with \code{Set}, there is no
tactic for \code{Magma}, because there is no property; all
equivalences are again syntactic equivalences, and can thus be proven by
\code{refl}. 

A semi-group is a magma (ie, it still has a \code{Plus} operation), but moreover it has the property of associativity for this operation. 

\begin{lstlisting}
interface Magma c => SemiGroup c where
  Plus_assoc : (c1 : c) $\rightarrow$ (c2 : c) $\rightarrow$ (c3 : c) $\rightarrow$ 
               ((c1 + c2) + c3 $\simeq$ c1 + (c2 + c3))
\end{lstlisting}

\noindent
Examples of magma are \code{Nat} equipped with addition, and \code{List} with
concatenation.  Next, a monoid is a semi-group with the property of neutral element for a distinguished element called \code{Zero}.

\begin{lstlisting}
interface SemiGroup c => Monoid c where
  Zero : c    
  Plus_neutral_1 : (c1 : c) $\rightarrow$ (Zero + c1 $\simeq$ c1)    
  Plus_neutral_2 : (c1 : c) $\rightarrow$ (c1 + Zero $\simeq$ c1)
\end{lstlisting}

\noindent
The hierarchy of interfaces continues with \code{Group}:

\begin{lstlisting}
interface Monoid c => Group c where
  Minus : c $\rightarrow$ c $\rightarrow$ c
  Neg : c $\rightarrow$ c
  Minus_simpl : (c1 : c) $\rightarrow$ (c2 : c) $\rightarrow$ Minus c1 c2 $\simeq$ c1 + (Neg c2) 
  Plus_inverse : (c1 : c) $\rightarrow$ (c1 + (Neg c1) $\simeq$ Zero, 
                              (Neg c1) + c1 $\simeq$ Zero)
\end{lstlisting}

The notion of group uses two new operations (\code{Neg} and \code{Minus}), but
\code{Minus} can be simplified with $+$ and \code{Neg}.
The important property of a group is that every element \code{c1} must admit
\code{Neg\ c1} as inverse element for $+$.
For reasons of space, we elide the remaining details of the hierarchy.

\subsection{Type-safe reflection}
\label{sect:typeSafeReflection}
			
When proving an equivalence $x \simeq y$, the universally-quantified
variables are abstracted and they become part of the context. Our
tatics normalise both sides of the ``potential equivalence'' $x
\simeq y$, and compare the results by syntactic equivalence. The
difficulty is that the normalisation function needs to consider
variables, constants and operators. For this
reason, we work by reflection, which allows us to work on syntax instead of values. We define one type of reflected
terms for each algebraic structure. The novelty is not the use of reflected
terms, but the use of a type-safe reflection
mechanism where we index the reflected terms by the concrete value that they
represent. Each of these datatype is parametrised over a type \code{c}, which
is the type on which we want to prove equalities (the \emph{carrier}
type). It is also indexed over an implementation of the corresponding interface
for \code{c} (we usually call it \code{p}, because it behaves as a \emph{proof}
telling that the structure \code{c} has the desired properties), indexed over a
context of abstracted variables (a vector $\Gamma$ of \code{n} elements of type
\code{c}). Most importantly, it is indexed over a value of type \code{c},
which is the concrete value being encoded.

A magma is equipped with one operation, addition. Thus, to reflect terms in a
magma we express constants, variables, and addition:

\begin{lstlisting}
data ExprMa : Magma c $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
  ConstMa : (p : Magma c) $\rightarrow$ ($\Gamma$:Vect n c) $\rightarrow$ (c1:c) $\rightarrow$ ExprMa p $\Gamma$ c1 
  PlusMa : {p : Magma c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
       $\rightarrow$ ExprMa p $\Gamma$ c1 $\rightarrow$ ExprMa p $\Gamma$ c2 $\rightarrow$ ExprMa p $\Gamma$ (c1+c2) 
  VarMa : (p:Magma c) $\rightarrow$ ($\Gamma$:Vect n c)
       $\rightarrow$ (i:Fin n) $\rightarrow$ ExprMa p $\Gamma$ (index i $\Gamma$)
\end{lstlisting}

For an expression $e_x\ :\ \code{ExprMa}\ \Gamma\ x$, we say that $e_x$
denotes the value $x$ in the context $\Gamma$.  When an expression
is a variable \code{(VarMa\ \_\ $\Gamma$\ i)}, the denoted value is the
corresponding variable in the context, i.e., \code{(index\ i\ $\Gamma$)}.  The
expression \code{(ConstMa\ \_\ $\Gamma$\ k)} denotes the constant $k$ in any context $\Gamma$. Finally, if $e_x$ is an expression encoding the concrete value $x$, and
$e_y$ is an expression encoding the concrete value $y$, then the expression
\code{PlusMa\ $e_x$\ $e_y$} denotes the concrete value $(x + y)$. Because the
reflected terms embed their corresponding inputs, they are guaranteed to be
sound representations.
This is a \emph{local} approach to syntax representation~\cite{Farmer13}
in that the reflected representation will \emph{only} represent terms in
a magma.

There are no additional operations in semi-groups or monoids, so the reflected
datatypes have the same shape as that for magma.  However, the datatype for
reflected terms in groups introduces two new constructors for \code{Neg}
and \code{Minus}:

\begin{lstlisting}
data ExprG :  Group c $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
    ConstG : (p : Group c) $\rightarrow$ ($\Gamma$:Vect n c) $\rightarrow$ (c1:c) $\rightarrow$ ExprG p $\Gamma$ c1
    PlusG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ c2 $\rightarrow$ ExprG p $\Gamma$ (c1+c2)
    MinusG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ c2 $\rightarrow$ ExprG p $\Gamma$ (Minus c1 c2)
    NegG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ (Neg c1)
    VarG : (p : Group c) $\rightarrow$ ($\Gamma$:Vect n c) 
         $\rightarrow$ (i:Fin n) $\rightarrow$ ExprG p $\Gamma$ (index i $\Gamma$)
\end{lstlisting}


The index of type $c$ (the value encoded by an expression) is always expressed
by using the lookup function \code{index} and the available operations in the
implementation $p$, which for a group are $+$, \code{Minus} and \code{Neg}.

\subsection{A correct-by-construction approach}
\label{sect:correctByConstruction}

We take a \emph{correct-by-construction} approach to implementing
normalisation, which means that no additional proof will be required after
defining normalisation.
The normalisation function \code{norm} produces a new expression,
and a proof that it
has the same interpretation as the original.
This will be enforced by the fact that all the datatypes for
reflected terms (\code{ExprMa}, \code{ExprG}, \code{ExprR}, etc) are indexed
over the concrete value: a term of type $\code{Expr}\ \Gamma\ x$ is
the encoding of the concrete value $x$ in the context $\Gamma$.  For each
structure, the type of \code{norm} has
the following form:
\\ $\code{norm} :\ \code{Expr}\ \Gamma\ x\ \rightarrow\ (x'\ \code{**}\
(\code{Expr}\ \Gamma\ x',\ x\ \simeq\ x'))$ \\ 
Every instance of
\code{norm} produces a dependent pair: the new concrete value $x'$, and a
pair made of an $\code{Expr}\ \Gamma\ x'$ which is the new encoded term indexed
over the new concrete value we have just produced, and a proof that old and new
concrete values $x$ and $x'$ are equivalent.
This proof of $x \simeq y$ is the crucial component which allows us to
automatically produce proofs of equivalences.

We will explain how to implement the tactic for \code{Group} specifically,
and the other structures are implemented similarly.
The equivalence we are trying to prove is $x \simeq y$, where $x$ and $y$ are
elements of the type $c$, which implements a group. The reflected term for $x$
is denoted $e_x$, and has type $\code{ExprG}\ p\
\Gamma\ x$, which means that $e_x$ is guaranteed to be the encoding of $x$.  
Similarly,
$y$, is encoded by $e_y$, and its type is indexed by the
value $y$.  Evaluating \code{norm} on $e_x$ produces the
normal form $e_{x'}$ of type $\code{ExprG}\ p\ \Gamma\ x'$ and a proof $p_x$ of
$x \simeq x'$.
Similarly, for $e_y$, it produces
the normal form $e_{y'}$ of type $\code{ExprG}\ p\ \Gamma\ y'$ and a proof
$p_y$ of $y \simeq y'$.  It now suffices to compare the normal forms $e_{x'}$ and $e_{y'}$ using a syntactic equivalence test \code{ExprG\_eq}, because once everything is in normal form, being equivalent is just a matter of being syntactically equivalent.

\begin{lstlisting}
exprG_eq : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (p:Group c) $\rightarrow$ ($\Gamma$:Vect n c) 
           $\rightarrow$ {x' : c} $\rightarrow$ {y' : c} 
           $\rightarrow$ (ex' : ExprG p $\Gamma$ x') $\rightarrow$ (ey' : ExprG p $\Gamma$ y') 
           $\rightarrow$ Maybe(x' $\simeq$ y')
\end{lstlisting}
This syntactical equivalence test checks if the two input terms $ex'$ and $ey'$ are \emph{syntactically} the same, and if they do, it directly builds a proof of equivalence between their indices $x' \simeq y'$, which is what we need, because we can use it 
with the two equivalences $x \simeq x'$ and $y\simeq y'$ that we already have, in order to get the desired proof of $x \simeq y$ with \code{eq\_preserves\_eq}. We put all of this together in a function \code{buildProofGroup}:

\begin{lstlisting}
buildProofGroup : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {x : c} $\rightarrow$ {y : c} 
  $\rightarrow$ {x':c} $\rightarrow$ {y':c} $\rightarrow$ (ExprG p $\Gamma$ x') $\rightarrow$ (ExprG p $\Gamma$ y') 
  $\rightarrow$ (x $\simeq$ x') $\rightarrow$ (y $\simeq$ y') $\rightarrow$ (Maybe (x $\simeq$ y))
buildProofGroup p ex' ey' px py with (exprG_eq p ex' ey')
    buildProofGroup p ex' ey' px py | Just ex'_equiv_ey' = 
                Just(eq_preserves_eq x y x' y' px py ex'_equiv_ey')
    buildProofGroup p ex' ey' px py | Nothing = Nothing
\end{lstlisting}

The arguments of type $\code{ExprG}\ p\ \Gamma\ x'$ and $\code{ExprG}\ p\ \Gamma\ y'$ are the normalised reflected
left and right hand sides of the equivalence, which respectively represent the value $x'$ and $y'$. 
This function also expects proofs of $x \simeq x'$ and
of $y \simeq y'$, which are built by the normalisation process.

Finally, the main function which tries to prove the equivalence $x \simeq y$
has to normalise the two reflected terms encoding the left and the right hand
side, and use the function \code{buildProof} to compose the two
proofs:

\begin{lstlisting}
groupDecideEq : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {x : c} $\rightarrow$ {y : c} 
               $\rightarrow$ (ExprG p $\Gamma$ x) $\rightarrow$ (ExprG p $\Gamma$ y) $\rightarrow$ Maybe (x $\simeq$ y)
groupDecideEq p ex ey =
  let (x' ** (ex', px)) = groupNormalise p ex in
  let (y' ** (ey', py)) = groupNormalise p ey in
	     buildProofGroup p ex' ey' px py
\end{lstlisting}


It remains to define the function \code{groupNormalise}, which is an
instance of \code{norm} for groups:

\begin{lstlisting}
groupNormalise : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} 
       $\rightarrow$ {x:c} $\rightarrow$ (ExprG p $\Gamma$ x) $\rightarrow$ (x' ** (ExprG p $\Gamma$ x', x $\simeq$ x'))
\end{lstlisting}

Each algebraic structure is equipped with a function for reducing reflected terms to their normal form.
The algebraic theories which concern us admit a canonical representative\footnote{It only holds for ``pure'' algebraic structures, ie, in the absence of additional axioms}  for
any element, a property which we use to decide equalities. Without this
property, it would be more complicated to decide equivalence without
brute-forcing a series of rewritings, that would have no termination guarantee.  

The normalisation function has more work to do for structures with many
axioms (commutative-monoids, groups, commutative-groups, semi-rings and rings),
than for the simpler structures (semi-groups and monoids). In the next
section, we describe the normalisation process.


\subsection{Normalisation functions}
\label{sect:normalFormShape}

We describe the normal form for rings, which is our most sophisticated
structure. The input to the normalisation function is an expression 
with sums, products, constants and variables belonging to an
ordered set $\mathcal{V}$ of variables. In short, the normalisation function takes in input a polynomial of multiple variables. As output, it produces a
normal form representing the same polynomial. Therefore, we need a
canonical representation of polynomials. 
There are several possibilities, but we choose
classical mathematical conventions: 
the polynomial will be completely developed, i.e., the
distributivity of $*$ over $+$ will be applied until it cannot be applied
further.
This is a simple but effective approach: the benefit of simplicity is that we
can directly produce a proof of equivalence between the new and old concrete values
during normalisation.
Because the polynomial is completely developed, at the
toplevel, it is a sum:

\[
P = \sum_{i=1}^{a}\ (\prod_{j=1}^{b} Monomial_{i}^j)
\text{ where } 
Monomial_{i}^j = C_{i}^j * \prod_{k=1}^{c} Var_{i,k}^{j}
\]
with $C_{i}^j$ a constant, and $Var_{i,k}^{j}$ one of the variable that belong to $\mathcal{V}$.

It may be surprising that the normal form is a sum of product of
monomials, and not directly a sum of monomials. This is because a
monomial is a product of a constant $C_{i}^j$ (e.g. $5$) and of a product of
variables (e.g. $x*y*z$). For example, $5*(x*(y*z))$ is a monomial. Now let's
consider the term $(5*(x*(y*z))) * (4*(z*z))$. This term is not a monomial, but
we could be tempted to simplify it into the monomial $20*(x*(y*(z*(z*z))))$.
However, that would assume that the product is always commutative, which
is not the case for \emph{every} ring.
Therefore, after 
development, the polynomial is a sum of \emph{product of monomials}, and
not directly a sum of monomials. The only rearrangement that can and needs to be done
towards the multiplication is to check if two constants are consecutive in a
product, and if so, to replace them by the constant that represents 
their product.

However, because $+$ is always commutative in a ring, the different
products of monomials themselves can be rearranged in different ways in this
sum. This will be done at the lower level for commutative groups if we can
provide an ordering on products of monomials. This ordering will be defined by using an ordering on monomials, called \code{isBefore\_mon}, which
looks at the order of variables for comparing two monomials
$Monomial_{i}^{j}$ and $Monomial_{i'}^{j'}$.
\[
Monomial_{i}^{j} = C_{i}^{j} * (Var_{i,1}^{j} * \prod_{k=2}^{c} Var_{i,k}^{j})
\text{ and }
Monomial_{i'}^{j'} = C_{i'}^{j'} * (Var_{i',1}^{j'} * \prod_{k=2}^{c'} Var_{i',k}^{j'})
\]

The order between these two monomials is decided by looking at the order
between the variables $Var_{i,1}^{j}$ and $Var_{i',1}^{j'}$. 
If both monomials start with the same variable, we continue by inspecting
the remaining variables. If one of the two monomials
has fewer variables, that one comes first.

We can now build the order on \emph{product of monomials}, named
\code{isBefore}. Given two products of monomials $Prod_{i}$ and
$Prod_{i'}$ we need to decide which one comes first. 
We will use the
order \code{isBefore\_mon} on the first monomials of these two products. If it
says that $Monomial_{i}^{1}$ comes before $Monomial_{i'}^{1}$, then we decide
that $Prod_{i}$ comes before $Prod_{i'}$. Conversely, if
$Monomial_{i'}^{1}$ comes first, then $Prod_{i'}$ comes
first. However, if $Monomial_{i}^{1}$ and $Monomial_{i'}^{1}$ have exactly the
same position in the order, then we continue by inspecting the remaining
monomials recursively. As previously, if 
one of the two products has fewer monomials than the other, then that one 
comes first.

Additionally, we use the following conventions when deciding on
a normal form:
\begin{itemize}

\item The top-level sum of the polynomial is in right-associative form: \\
$prodMon_1\ +\ (prodMon_2\ + (prodMon_3\ +\ (...\ +\ prodMon_a)))$
\item All the products that we have (the products of monomials and the products
of variables), are written in right-associative form.
\item We simplify as much as possible with constants. This includes
simplifying addition with zero and multiplication with the constants
zero and one, doing the computations between two nearby constants, etc\ldots
\item We simplify the sum of an expression $e$ and its inverse $-e$ to
zero. 

\end{itemize}

\subsection{Reusing the provers}
\label{sect:reusabilityOfTheProvers}

A novelty of our work is that instead of building a prover for a
specific algebraic structure (like ring), we have built a hierarchy of provers
for many algebraic structures. Moreover, each prover reuses components
of the others so that
the various simplifications are not duplicated at the different
levels: the normalisation function of each structure uses as much as possible
the normalisation function of the structure from which it inherits. For
example, the normalisation on monoids reuses the normalisation on semi-groups
in such a way that it does not have to deal with associativity. In this
case, the datatype reflecting terms in semi-groups has
the same expressive power as that for monoids, so a term valid in a monoid can easily be transformed into a corresponding term of a semi-group. However, 
there is a difficulty with
groups and monoids: normalisation for groups
needs to reuse the normalisation for monoids, meaning that
we will have to encode 
negations somehow, which can't be directly expressed in a monoid. For this
reason, we develop some specific encodings.

The idea is that we encode negations as variables, and let the
monoid prover consider them as ordinary variables. To achieve this,
we use the following datatype that helps us distinguish between a
variable and the encoding of a negation:

\begin{lstlisting}
data Variable : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
  RealVariable : ($\Gamma$:Vect n c) $\rightarrow$ 
                 (i:Fin n) $\rightarrow$ Variable $\Gamma$ (index i $\Gamma$)
  EncodingNeg : ($\Gamma$:Vect n c) $\rightarrow$ 
                (i:Fin n) $\rightarrow$ Variable $\Gamma$ (Neg (index i $\Gamma$))
\end{lstlisting}

We only need to encode negations of variables, as we can simplify 
the negation of a constant into a constant.
Also, there cannot be a negation of something
different non-atomic (i.e. a term that is not a variable or a constant), because normalisation 
of
groups has systematically propagated \code{Neg} inside the parentheses,
following simplification\footnote{Note that we have to be careful and not
simplify it to $(-a) + (-b)$ as it would assume that $+$ is commutative}
$-(a+b) = -b + -a$.

All the constructors for variables now take a \code{Variable} as parameter,
instead of taking directly an element of $(\code{Fin}\ n)$. That gives the
following, for groups:

\begin{lstlisting}
    VarG : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {val:c} 
           $\rightarrow$ (Variable $\Gamma$ val) $\rightarrow$ ExprG p $\Gamma$ val
\end{lstlisting}


Thanks to this encoding, we can now transform an \code{ExprG} from the group
level to an \code{ExprMo} at the monoid level. A constant $(\code{ConstG}\ p\
\Gamma\ c1)$ is transformed into the corresponding constant
$(\code{ConstMo}\ p\ \Gamma\ c1)$, a \code{PlusG} into the corresponding
\code{PlusMo}, a variable into the same variable, the negation of a
constant into the resulting constant, and finally the negation of a variable
$i$ into a $(\code{VarMo}\ p\ (\code{EncodingNegOfVar}\ \Gamma\ i))$.

We use a similar technique for converting an expression from the ring level
to the commutative-group level, where we encode the product of monomials,
because the product is not defined at the commutative-group level.
That enables the function of normalisation for rings to benefit from the
normalisation function for commutative-groups.


\subsection{Automatic reflection}
		
We have built an automatic reflection mechanism which enables the machine to
build the reflected terms for us automatically. This is not essential to our
approach, but it simplifies the usage of the tactics by removing the need to
write long encodings by hand.  To do so, we used Idris' reflection mechanism,
which enables pattern matching on syntax, rather than on constructors. 
While we omit the full details due to space constraints, reflecting values
involves defining functions of the following form:

\begin{lstlisting}
%reflection reflectGroup : (p : Group c) $\rightarrow$ ($\Gamma$ : Vect n c) $\rightarrow$ 
                           (x : c) $\rightarrow$ ($\Gamma'$ ** ExprG p ($\Gamma'$ ++ $\Gamma$) x)
\end{lstlisting}

The \texttt{\%reflection} annotation means that Idris treats this as a 
compile time function on \emph{syntax}. Given a value of type
\texttt{c}, in some context $\Gamma$, it constructs a reflected expression 
in an extended context \texttt{$\Gamma'$ ++ $\Gamma$}. The context
contains references to subexpressions 
which are not themselves representable by \texttt{ExprG}.

