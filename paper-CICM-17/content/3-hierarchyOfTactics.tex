\section{A hierarchy of provers}

We aim to build a prover not only for equalities on \code{Nat}, \code{List}, or
any specific type, but for generic datatypes and properties. Using
the right
abstraction, we can generate proofs of equalities for
many datatypes \emph{at once} by implementing a generic hierarchy of provers
for semi-groups, monoids,
commutative-monoids, groups, commutative-groups, semi-rings and rings. 
The
properties of an algebraic structure are expressed in an
interface (an interface in Idris is similar to a type class
in Haskell). This interface will extend the interface from which it
inherits; for example, \code{Group} extends \code{Monoid}.
This leads to a hierarchy of interfaces, with one tactic
for each. At every level of the hierarchy, we will be able
to work on any type, as long as there is a corresponding implementation of
the interface.

\subsection{Proving equivalences instead of equalities}
	
With some additional effort, we can produce a collection of tactics for proving
\emph{equivalences}, rather than only equalities. 
The machinery is very similar and
we gain another
degree of genericity, with the freedom of choosing the equivalence relation
(which can be the usual equality). 
The user can define their own
notion of equivalence, as long as they provide the proofs of the
properties of the relevant algebraic structure. Let's call \code{c} the
\emph{carrier} type, i.e., the type on which we want to prove equivalences. 
The
equivalence relation on \code{c} has the following profile $(\simeq) : \code{c}
\rightarrow \code{c} \rightarrow \code{Type}$\footnote{This \code{Type} would
be a \code{Prop} in systems, like Coq, that make a distinction between the
world of computations and the world of logical statements}, and must be
be reflexive, symmetric and transitive.

Our tactics need to be able to test this equivalence
between elements of the underlying set, that is a way of testing
equivalence of constants. We therefore define a notion of
\code{Set}\footnote{This notion of set 
is a way to talk about the carrier type and an equivalence
relation, sometimes called Setoid}, which requires the definition of the
equivalence relation and an equivalence test \code{set\_eq}. All the interfaces
representing the algebraic structures will later extend \code{Set}:

\begin{lstlisting}
interface Set c where
    ($\simeq$) : c $\rightarrow$ c $\rightarrow$ Type
    refl : (x : c) $\rightarrow$ x $\simeq$ x
    sym : {x, y : c} $\rightarrow$ (x $\simeq$ y) $\rightarrow$ (y $\simeq$ x)
    trans : {x, y, z : c} $\rightarrow$ (x $\simeq$ y) $\rightarrow$ (y $\simeq$ z) $\rightarrow$ (x $\simeq$ z)    
    set_eq : (x : c) $\rightarrow$ (y:c) $\rightarrow$ Maybe (x $\simeq$ y)
\end{lstlisting}

To prove propositional equalities, the user instantiates
$(\simeq)$ with the built-in $(=)$ when implementing \code{Set}.
Note that $(\simeq)$ is only weakly decidable in the sense
that \code{set\_eq} only produces a proof when the two elements are equivalent,
but it doesn't produce a proof of dis-equivalence when they are different,
instead producing the value \code{Nothing}. Our goal is only to generate
proofs of equivalance, not to produce counter-examples.
%
There is no tactic associated with \code{Set}, since we have no operations or
properties associated to this structure. Equivalences in a
\code{Set} are ``syntactic equivalences'' and can be
proven with \code{refl}\footnote{\code{refl} is not to be confused with
\code{Refl}, the constructor of $=$, but when $(\simeq)$ is
instantiated with the equality $=$, \code{refl} is implemented by
\code{Refl}. Therefore, \code{refl} of the interface \code{Set} is a
generalisation of \code{Refl}}.

%The kernel of our machinery will be a function $norm$ that will be composed of %multiples functions composed together, that each do a part of the normalisation %and that generate the proof of equivalence between old and new concrete values %for this part of the normalisation.

Working with equivalences instead of equalities brings one complication: Proofs
of correctness that we produce by hand cannot use Idris' ``rewrite'' mechanism,
which enables rewriting of a subterm by another one, provided that the two
subterms are propositionally equal. 
%
% EB - I think we can skip this detail
%If we need to prove $P\ x$,
%and have a proof of equivalence $pr:x \simeq x'$, we can't use ``rewrite pr" in
%order to transform the goal into $P\ x'$, as we would normally do if the
%available proof would be an equality $x=x'$. 
%
This is a classical problem of
working within a setoid, which can be mitigated by programming
language support for rewriting terms in setoids. However, Idris
is not equipped with any such support. For this reason, we define the 
following lemma, using the methods of the \code{Set} interface:

$\code{eq\_preserves\_eq} : \{c:\code{Type}\} \rightarrow \{\code{Set}\ c\} \rightarrow (x:c) \rightarrow (y:c) \rightarrow (c1:c) \rightarrow (c2:c) \rightarrow (x \simeq c1) \rightarrow (y \simeq c2) \rightarrow (c1 \simeq c2) \rightarrow (x \simeq y)$.

% EB - I took this out because, as the reviewer says, it's odd to have a
% hand proof here, and I don't think the code itself is interesting enough to
% include since it's a fairly routine use of the interface

%\begin{proof}
%We have $x \simeq c1$ and we have $c1 \simeq c2$, therefore we have $x \simeq c2$ by the use of the axiom of transitivity (of the interface \code{Set}). \

%We also have $y \simeq c2$ and therefore we have $c2 \simeq y$ by the use of the symmetry axiom (of the interface \code{Set}). \

%Now that we have these fresh proofs of $x \simeq c2$ and of $c2 \simeq y$, we can use one last time the property of transitivity, in order to get a proof of $x \simeq y$.
%\qed
%\end{proof}

This lemma says that the equivalence preserves the equivalence, which means
that in order to prove $x \simeq y$, we can prove a smaller
problem $c1 \simeq c2$, provided that $x \simeq c1$ and that $y \simeq c2$.
We will use this lemma extensively.

\subsection{Hierarchy of interfaces}

We describe operations, constants and properties of each algebraic structure
in an interface. The first structure is
the magma. which is a structure built on top of \code{Set} that adds
\code{Plus} operation, and no specific properties:

\begin{lstlisting}
interface Set c => Magma c where
    + : c $\rightarrow$ c $\rightarrow$ c
\end{lstlisting}

This code means that a type \code{c} (for $carrier$) is a \code{Magma} if it is
already a \code{Set} (ie, it is equipped with the equivalence relation $\simeq$
and the equivalence test \code{set\_eq}), and if it has a $+$ operation.  In
fact, there is an additional requirement that will apply to all operators (in
this case, the $+$ operation), which is that they need to be ``compatible" with
the equivalence relation, which is expressed by the following axiom for $+$:
\\ $\code{Plus\_preserves\_equiv} : \{c:\code{Type}\} \rightarrow
\{\code{Magma}\ c\} \rightarrow\{c1:c\} \rightarrow \{c2:c\} \rightarrow
\{c1':c\} \rightarrow \{c2':c\} \rightarrow (c1 \simeq c1') \rightarrow (c2
\simeq c2') \rightarrow ((c1+c2) \simeq (c1'+c2'))$ 

We have this requirement because we support
any equivalence relation.
As with \code{Set}, there is no
tactic for \code{Magma}, because there is no property; all
equivalences are again syntactic equivalences, and can thus be proven by
\code{refl}. 

A semi-group is a magma (ie, it still has a \code{Plus} operation), but moreover it has the property of associativity for this operation. 

\begin{lstlisting}
interface Magma c => SemiGroup c where
  Plus_assoc : (c1 : c) $\rightarrow$ (c2 : c) $\rightarrow$ (c3 : c) $\rightarrow$ 
               ((c1 + c2) + c3 $\simeq$ c1 + (c2 + c3))
\end{lstlisting}

\noindent
Examples of magma are \code{Nat} equipped with addition, and \code{List} with
concatenation.  Next, a monoid is a semi-group with a ``neutral``, or
distinguished, element called \code{Zero}.

\begin{lstlisting}
interface SemiGroup c => Monoid c where
  Zero : c    
  Plus_neutral_1 : (c1 : c) $\rightarrow$ (Zero + c1 $\simeq$ c1)    
  Plus_neutral_2 : (c1 : c) $\rightarrow$ (c1 + Zero $\simeq$ c1)
\end{lstlisting}

\noindent
The hierarchy of interfaces continues with \code{Group}:

\begin{lstlisting}
interface Monoid c => Group c where
  Minus : c $\rightarrow$ c $\rightarrow$ c
  Neg : c $\rightarrow$ c
  Minus_simpl : (c1 : c) $\rightarrow$ (c2 : c) $\rightarrow$ Minus c1 c2 $\simeq$ c1 + (Neg c2) 
  Plus_inverse : (c1 : c) $\rightarrow$ (c1 + (Neg c1) $\simeq$ Zero, 
                              (Neg c1) + c1 $\simeq$ Zero)
\end{lstlisting}

The notion of group uses two new operations (\code{Neg} and \code{Minus}), but
\code{Minus} can be simplified with $+$ and \code{Neg}.
The important property of a group is that every element \code{c1} must admit
\code{Neg\ c1} as inverse element for $+$.
For reasons of space, we elide the remaining details of the hierarchy.

\subsection{Type-safe reflection}
\label{sect:typeSafeReflection}
			
When proving an equivalence $x \simeq y$, the universally-quantified
variables are abstracted. Our
tatics normalise both sides of the ``potential equivalence'' $x
\simeq y$, and compare the results by syntactic equality. The
difficulty is that the normalisation function needs to consider
variables, constants and operators. For this
reason, we work by reflection. We have one type of reflected
terms for each algebraic structure. The novelty is not the use of reflected
terms, but the use of a type-safe reflection
mechanism where we index the reflected terms by the concrete value that they
represent. Each of these datatype is parametrised over a type \code{c}, which
is the type on which we want to prove equalities (the \emph{carrier}
type). It is also indexed over an implementation of the corresponding interface
for \code{c} (we usually call it \code{p}, because it behaves as a \emph{proof}
telling that the structure \code{c} has the desired properties), indexed over a
context of abstracted variables (a vector $\Gamma$ of \code{n} elements of type
\code{c}). Most importantly, it is indexed over a value of type \code{c},
which is the concrete value being encoded.

A magma is equipped with one operation, addition. Thus, to reflect terms in a
magma we express constants, variables, and addition:

\begin{lstlisting}
data ExprMa : Magma c $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
  ConstMa : (p : Magma c) $\rightarrow$ ($\Gamma$:Vect n c) $\rightarrow$ (c1:c) $\rightarrow$ ExprMa p $\Gamma$ c1 
  PlusMa : {p : Magma c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
       $\rightarrow$ ExprMa p $\Gamma$ c1 $\rightarrow$ ExprMa p $\Gamma$ c2 $\rightarrow$ ExprMa p $\Gamma$ (c1+c2) 
  VarMa : (p:Magma c) $\rightarrow$ ($\Gamma$:Vect n c)
       $\rightarrow$ (i:Fin n) $\rightarrow$ ExprMa p $\Gamma$ (index i $\Gamma$)
\end{lstlisting}


For an expression $e_x\ :\ \code{ExprMa}\ \Gamma\ x$, we say that $e_x$
denotes the value $x$ in the context $\Gamma$.  When an expression
is a variable \code{(VarMa\ \_\ $\Gamma$\ i)}, the denoted value is the
corresponding variable in the context, i.e., \code{(index\ i\ $\Gamma$)}.  The
expression \code{(ConstMa\ \_\ $\Gamma$\ k)} denotes the constant $k$ of type
$c$. Finally, if $e_x$ is an expression encoding the concrete value $x$, and
$e_y$ is an expression encoding the concrete value $y$, then the expression
\code{PlusMa\ $e_x$\ $e_y$} denotes the concrete value $(x + y)$. Because the
reflected terms embed their corresponding inputs, they are guaranteed to be
sound representations.

There are no additional operations in semi-groups or monoids, so the reflected
datatypes have the same shape as that for magma.  However, the datatype for
reflected terms in groups introduces two new constructors for \code{Neg}
and \code{Minus}:

\begin{lstlisting}
data ExprG :  Group c $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
    ConstG : (p : Group c) $\rightarrow$ ($\Gamma$:Vect n c) $\rightarrow$ (c1:c) $\rightarrow$ ExprG p $\Gamma$ c1
    PlusG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ c2 $\rightarrow$ ExprG p $\Gamma$ (c1+c2)
    MinusG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} $\rightarrow$ {c2:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ c2 $\rightarrow$ ExprG p $\Gamma$ (Minus c1 c2)
    NegG : {p : Group c} $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {c1:c} 
         $\rightarrow$ ExprG p $\Gamma$ c1 $\rightarrow$ ExprG p $\Gamma$ (Neg c1)
    VarG : (p : Group c) $\rightarrow$ {$\Gamma$:Vect n c} 
         $\rightarrow$ (i:Fin n) $\rightarrow$ ExprG p $\Gamma$ (index i $\Gamma$)
\end{lstlisting}


The index of type $c$ (the real value encoded by an expression) is always expressed by using the lookup function \code{index} and the available operations in the implementation $p$, which for a group are $+$, \code{Minus} and \code{Neg}.

	\subsection{A correct by construction approach}
\label{sect:correctByConstruction}

We want to write the normalisation function on a \emph{correct by construction} way, which means that no additional proof should be required after the definition of the function. Thus, \code{norm} will produce the proof that the new expression freshly produced has the same interpretation as the original expression, and this will be made easier by the fact that all the datatypes for reflecting terms (\code{ExprMa}, \code{ExprG}, \code{ExprR}, etc) are indexed over the real --concrete-- value : a term of type $\code{Expr}\ \Gamma\ x$ is the encoding of the concrete value $x$ in the context $\Gamma$.
Thus, for each algebraic structure, the type of the function \code{norm} will look like this : \\
$\code{norm} :\ \code{Expr}\ \Gamma\ x\ \rightarrow\ (x'\ \code{**}\ (\code{Expr}\ \Gamma\ x',\ x\ \simeq\ x'))$ \\
Every function of normalisation \code{norm} will produce a dependent pair : the new concrete value $x'$, and a pair made of an $\code{Expr}\ \Gamma\ x'$ which is the new encoded term indexed over the new concrete value we have just produced, and a proof that old and new concrete values $x$ and $x'$ are equivalent.

This function doesn't only produce an $\code{Expr}\ \Gamma\ x$ (with an internal conversion of type) because the proof of $x \simeq x'$ is an essential component for building the desired proof of $x \simeq y$, which is the main problem that we are trying to solve.


We will explain how the tactic works by commenting the construction of the tactic for groups, but the other algebraic structures will be equipped similarly. The equivalence we are trying to prove is $x \simeq y$, where $x$ and $y$ are elements of the type $c$, which  simulates a group. The reflected term for $x$ will be denoted $e_x$, and this term will have the type $\code{ExprG}\ p\ \Gamma\ x$, which means that $e_x$ is guaranteed to be the encoding of $x$ because $e_x$ is precisely indexed over the real value $x$.
We've got similar things for $y$, which is encoded by $e_y$, and its type is indexed over the real value $y$.
Running the normalisation procedure on $e_x$ will produce the normal form $e_{x'}$ of type $\code{ExprG}\ p\ \Gamma\ x'$ and a proof $p_x$ of $x \simeq x'$ while running the normalisation procedure on $e_y$ will produce the normal form $e_{y'}$ of type $\code{ExprG}\ p\ \Gamma\ y'$ and a proof $p_y$ of $y \simeq y'$.
It is now enough to compare $e_{x'}$ and $e_{y'}$ using a standard syntactic equality test \code{exprG\_eq} because these two expressions are now supposed to be in normal form.

Now, if the two normalised expressions $e_{x'}$ and $e_{y'}$ are equal, then they necessary have the same type\footnote{We are working with the heterogeneous equality JMeq by default in Idris, but as always, the only way to have a proof of a:A = b:B is when A=B}, and therefore $x'=y'$, which implies $x' \simeq y'$ because the propositional equality is more restrictive than an equivalence relation.
By using the two equivalences $x \simeq x'$ and $y \simeq y'$ that we obtained during the normalisations in the new equivalence $x' \simeq y'$ with the use of \code{eq\_preserves\_eq}, we can get a proof of $x \simeq y$. This is what the function \code{buildProof} is doing :

\begin{lstlisting}
buildProofGroup : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {x : c} $\rightarrow$ {y : c} 
  $\rightarrow$ {x':c} $\rightarrow$ {y':c} $\rightarrow$ (ExprG p $\Gamma$ x') $\rightarrow$ (ExprG p $\Gamma$ y') 
  $\rightarrow$ (x $\simeq$ x') $\rightarrow$ (y $\simeq$ y') $\rightarrow$ (Maybe (x $\simeq$ y))
buildProofGroup p ex' ey' px py with (exprG_eq p ex' ey')
    buildProofGroup p ex' ex' px py | Just refl = 
                Just(eq_preserves_eq x y x' y' lp rp e1_equiv_e2)
    buildProofGroup p ex' ey' px py | Nothing = Nothing
\end{lstlisting}

The argument of type $\code{ExprG}\ p\ \Gamma\ x'$ is the normalised reflected left hand size of the equivalence, which represents the value $x'$. Before the normalisation, the reflected LHS was reflecting the value $x$. The same applies to the right-hand side. The function also expects proofs of $x \simeq x'$ and of $y \simeq y'$, that we will be able to provide because the normalisation function has produced them while it was building the normalised reflected terms.


Finally, the main function which tries to prove the equivalence $x \simeq y$ has to normalise the two reflected terms encoding the left and the right hand side, and to use the function \code{buildProof} in order to compose the two proofs that have been obtained :

\begin{lstlisting}
groupDecideEq : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {x : c} $\rightarrow$ {y : c} 
               $\rightarrow$ (ExprG p $\Gamma$ x) $\rightarrow$ (ExprG p $\Gamma$ y) $\rightarrow$ Maybe (x $\simeq$ y)
groupDecideEq p ex ey =
  let (x' ** (ex', px)) = groupNormalise p ex in
  let (y' ** (ey', py)) = groupNormalise p ey in
	     buildProofGroup p ex' ey' px py
\end{lstlisting}


The remaining part is to define the function \code{norm}. Each algebraic structure will have a function for reducing the reflected terms into their normal forms. The fact that all of these algebraic structures admit a canonical representative for any element is in fact a very nice property that we are using in order to decide equalities. Without this property, it would become a lot more complicated to decide the equivalence without brute-forcing a series of rewritings, that would have no guarantee to terminate.
As said before, the normalisation functions also produce the proof of equivalence between the initial real value (the index of the input term) and the final real value (the index of the produced term). Thus, the type of the reduction function for group is :


\begin{lstlisting}
groupNormalise : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} 
       $\rightarrow$ {x:c} $\rightarrow$ (ExprG p $\Gamma$ x) $\rightarrow$ (x' ** (ExprG p $\Gamma$ x', x $\simeq$ x'))
\end{lstlisting}


The function of normalisation has more work to do for structures with many axioms (commutative-monoids, groups, commutative-groups, semi-rings and rings), than for the easier structure (semi-groups and monoids).
The operations performed by the normalisation functions are detailed in the next section.


\subsection{Normalisation functions}
\label{sect:normalFormShape}

For the sake of completeness, we describe the normal form for rings which is our most sophisticated structure.
The normalisation function takes in input an expression expressed with sums, products, constants (zero, one...) and variables that belong to an ordered set $\mathcal{V}$ of variables. In short, the normalisation function takes in input a polynomial of multiple variables. In output, it must produce a normal form representing the same polynomial. Therefore, we have to decide a canonical representation of polynomials. Many canonical representations can be used. We decide to stick with classical mathematical conventions. The first one, is that the polynomial will be completely developed, ie, the distributivity of $*$ over $+$ will be applied until it can't be applied anymore. The advantage is the simplicity, as factorising would be significantly more complicated and we're looking for simplicity as the normalisation functions will also have to produce the proof of equivalence between the old and the new index. Because the polynomial is completely developed, at the toplevel, it is a sum :

\[
P = \sum_{i=1}^{a}\ (\prod_{j=1}^{b} Monomial_{i}^j)
\text{ where } 
Monomial_{i}^j = C_{i}^j * \prod_{k=1}^{c} Var_{i,k}^{j}
\]
with $C_{i}^j$ a constant, and $Var_{i,k}^{j}$ one of the variable that belong to $\mathcal{V}$.

One could be surprised by the fact that the normal form is a sum of product of monomial, and not directly a sum of monomials. The reason is the following. A monomial is a product of a constant $C_{i}^j$ (like $4$) and of a product of variables (like $x*y*z$). For example,  $5*(x*(y*z))$ is a monomial. Now let's consider the term $(5*(x*(y*z))) * (4*(z*z))$. This term is not a monomial, but we could be tempted to simplify it into the monomial $20*(x*(y*(z*(z*z))))$. However, that would assume that the product is always commutative, ie, that we can re-organize the subterms of a product as we want. This is the case in a commutative ring, but this does not hold for any ring. Therefore, after the full development, the polynomial is a sum of \emph{product of monomials}, and not directly a sum of monomials. The only rearrangement that can be done towards the multiplication is to check if two constants are consecutive in a product, and if so, to replace them by the constant that represents the product of them.

However, because $+$ is always a commutative operator in a ring, the different products of monomials themselves can be rearranged in different ways in this sum. That will be done at the underneath level for commutative groups if we can provide an ordering on products of monomials. This ordering will be defined by using an ordering on monomials, that we will call \code{isBefore\_mon}, which works by looking at the order of variables for comparing two monomials $Monomial_{i}^{j}$ and $Monomial_{i'}^{j'}$.
\[
Monomial_{i}^{j} = C_{i}^{j} * (Var_{i,1}^{j} * \prod_{k=2}^{c} Var_{i,k}^{j})
\text{ and }
Monomial_{i'}^{j'} = C_{i'}^{j'} * (Var_{i',1}^{j'} * \prod_{k=2}^{c'} Var_{i',k}^{j'})
\]

The order between these two monomials will be decided by looking at the order between the first variables $Var_{i,1}^{j}$ and $Var_{i',1}^{j'}$. In the case where both monomials start with the same variable, we continue by inspecting the remaining variables. If we can't continue because one of the two monomials has less variables than the other, then we decide that the one with fewer variables comes first (exactly like the word ``house" comes before the word ``housemate" with the lexicographic order).

We can now build the order on \emph{product of monomials} that we needed. We name it \code{isBefore}. Given two product of monomials $Prod_{i}$ and $Prod_{i'}$ we need to decide which one comes first. We will obviously use the order \code{isBefore\_mon} on the first monomials of these two products. If it says that $Monomial_{i}^{1}$ comes before $Monomial_{i'}^{1}$, then we decide that $Prod_{i}$ comes before $Prod_{i'}$. Conversely, if that's $Monomial_{i'}^{1}$ that comes first, then we decide that $Prod_{i'}$ comes first. However, if $Monomial_{i}^{1}$ and $Monomial_{i'}^{1}$ have exactly the same position in the order, then we continue by inspecting the remaining monomials with a recursive call. As previously, if we can't continue because one of the two products has less monomials than the other, then the one with fewer monomials will come first.

A few additional conventions had to be decided about the normal form :
\begin{itemize}

\item The top-level sum of the polynomial will be put in right-associative form : \\
$prodMon_1\ +\ (prodMon_2\ + (prodMon_3\ +\ (...\ +\ prodMon_a)))$
\item All the products that we have (the products of monomials and the products of variables), will also be written on the completely right-associative form.
\item We always simplify as much as possible with constants. That includes simplifying the addition with zero and the multiplication with the constants zero and one, doing the computations between two nearby constants, etc...
\item We always simplify the sum of an expression $e$ and its inverse $-e$ into the constant zero. 

\end{itemize}

\subsection{Reusing the provers}
\label{sect:reusabilityOfTheProvers}

One of the novelty of our work is that we haven't built just a prover for a specific algebraic structure, but we have instead built a hierarchy of provers for many algebraic structures. Morover, each prover is not isolated from the others, and the various simplifications are not duplicated at the different levels : the normalisation function of each structure uses as much as possible the normalisation function of the structure from which it inherits. For example, the normalisation on monoids reuses the normalisation on semi-groups in such a way that it does not have to deal with the associativity. For this case, it's not a problem since the datatype reflecting terms in semi-groups has the same expressivity power than the one for monoids. However, that's not the case with groups and monoids: the function of normalisation for groups that will need to call the normalisation for monoids will have to encode the negations, because they can't be directly expressed in a monoid. For this reason, we will develop some specific encodings.

The idea is that we will encode negations as variables, and we will let the monoid prover deal with them as ordinary variables. In order to achieve this goal, we need the following datatype that will help us to distinguish between a real variable and the encoding of a negation :

\begin{lstlisting}
data Variable : {c:Type} $\rightarrow$ {n:Nat} $\rightarrow$ (Vect n c) $\rightarrow$ c $\rightarrow$ Type where
  RealVariable : ($\Gamma$:Vect n c) $\rightarrow$ (i:Fin n) $\rightarrow$ Variable $\Gamma$ (index i $\Gamma$)
  EncodingNeg : ($\Gamma$:Vect n c) $\rightarrow$ (i:Fin n) $\rightarrow$ Variable $\Gamma$ (Neg (index i $\Gamma$))
\end{lstlisting}

We only need to encode negations of variables, as the negation of a constant can be simplified into a constant. Also, there can't be a negation of something different than an atom (a variable or a constant), because the normalisation in groups has first systematically propagated \code{Neg} inside the parenthesis, following the simplification\footnote{Note that we have to be careful and not simplify it to $(-a) + (-b)$ as it would assume that $+$ is commutative} $-(a+b) = -b + -a$.

All the constructors for variables now take a \code{Variable} as parameter, instead of taking directly an element of $(\code{Fin}\ n)$. That gives the following, for groups :

\begin{lstlisting}
    VarG : (p:Group c) $\rightarrow$ {$\Gamma$:Vect n c} $\rightarrow$ {val:c} 
           $\rightarrow$ (Variable $\Gamma$ val) $\rightarrow$ ExprG p $\Gamma$ val
\end{lstlisting}


Thanks to this encoding, we can now transform an \code{ExprG} from the group level to an \code{ExprMo} at the monoid level. A constant $(\code{ConstG}\ p\ \Gamma\ c1)$ will be transformed into the corresponding constant $(\code{ConstMo}\ p\ \Gamma\ c1)$, a \code{PlusG} into the corresponding \code{PlusMo}, a real variable into the same real variable, the negation of a constant into the resulting constant, and finally the negation of a variable $i$ into a $(\code{VarMo}\ p\ (\code{EncodingNegOfVar}\ \Gamma\ i))$.

We've used a similar technique for converting an expression from the ring level to the commutative-group level, where we have encoded the product of monomials, because the product is not an operation defined at the commutative-group level. That enables the function of normalisation for rings to benefit from the normalisation function for commutative-groups.


\subsection{Automatic reflection}
		
We've built an automatic reflection mechanism which enables to let the machine build automatically the reflected terms for us. This is not something absolutely vital, as it only simplifies the usage of the tactics, avoiding to write long encodings by hand. To do so, we've used Idris' reflection mechanism, which enables to do pattern matching on syntax, rather than on constructors. 



