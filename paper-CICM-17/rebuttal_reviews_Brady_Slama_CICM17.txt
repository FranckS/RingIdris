We thank the reviews for their careful reading of the paper and many helpful
comments, which we will address in a revision. In particular, we are sure we
can condense the paper according to Jacques' useful suggestions.

> although proofs of type equivalences are not the only use case

Indeed; the motivating example is an illustration of how these proofs
obligations occur naturally, but there are many other uses.

> One question that comes to mind is whether it easy to prove properties of the
> normalization functions
 
Things become difficult when we leave the ground of the algebraic
structures in which we work (monoids, groups and rings essentially) and
move to an arbitrary equivalence relation with arbitrary axioms. Such a system
can be seen as a rewriting system of deduction modulo. Determining if such a
rewriting system admits a normal form requires determining whether the rewriting
system is confluent and terminating. Both questions are difficult
to answer in the general case. Currently, systems like Dedukti (an
implementation of lambda-Pi-calculus modulo) can plug to an auxiliary
confluence checker such as CSI^ho
(http://cl-informatik.uibk.ac.at/software/csi/ho/) or ACPH
(http://coco.nue.riec.tohoku.ac.jp/2015/papers/acph.pdf).

It is interesting to note that the study of the properties of a proposed
normalisation function can only be meta-theoretic: it's not possible to prove
completeness within the language. Benjamin Grégoire and Assia Mahboubi
[cited in our paper] have also noticed it for their implementation of their
Ring prover for Coq. However, this does not affect the correctness of the
decision procedure.

> An interesting question for future work would be to exploit external provers

Agreed, e.g. this would allow dealing with arbitrary rewriting rules. 

> ... as soon as the hierarchy grows large enough, such
> 'natural' inheritance becomes very brittle ....

Agreed, but our hierarchy is not supposed to deal with more than the most
useful algebraic structures which arise in Idris programming. Coq for example
only has a Ring solver but we have also provers for semigroups, monoids, and
groups thanks to the modularity with which we compute the normal form.

> there seems to be an implicit assumption that algebraic theories
> have a canonical presentation 

To clarify: when we say groups admit a normal form, we mean structures that
*only* have the axioms of a group.  We focus on these "pure" algebraic
structures (semi-groups, monoids, groups, rings...) that do not have
additional axioms and which do have a normal form.
 
> The largest flaw though seems to be the implicit assumption interesting structures have normal forms.

Agreed, there are indeed many structures which do not have normal norms. But deduction modulo, 
although very interesting, was somehow out of scope for our goals. We initially wanted Idris 
to be equiped with some automations, which we do in a modular way which makes it easy to implement
provers for other structures.

We don't claim that every interesting structure can be treated by one prover of
our hierachy of tactics, although our hierarchy of provers is more general than
the automations available in e.g. Coq or Agda.

> If one is going
> to talk about a "correct by construction" approach, then the *definition* of what a
> normal form is (i.e. syntax where intensional equality is a decision procedure for
> equality).  This is implicit everywhere, but never explicit.  And it is not an
> invariant kept by the types!

Yes, the types only say that the original reflected term and the normalised one both represent the
same concrete value, and this is in fact the proof of *correctness* of the approach. 
The *completeness* is a meta property that we cannot prove in the language itself. 
Benjamin Grégoire and Assia Mahboubi [cited in our paper] have also noticed it for their implementation
of their Ring prover for Coq. It's not an issue as it does not affect the correctness of the decision procedure.

> For example, the word problem for groups is undecidable.  This implies that there are
> theories with no normal forms.

Yes, we agree, see our answer to the two previous questions.

> In section 2.5, an *order* is snuck in, onto the set of variables.  This is crucial!
> It should not be buried.  Hint: Groebner bases.  Term orders are important.
> But normal forms are extremely sensitive to term orders (as bad as exponential).

The set of variables V is said to be ordered in section 2.5 (page 12). Note that this is not a limitation
as it is always possible to build an adhoc ordering by using the order of appearance of the variables in the goal A = B
This is in fact exactly what our automatically reflection does (mentionned briefly in section 2.7).

> Furthermore, the discussion in this section implicitly assumes associativity.  There
> are many useful, interesting, non-associative algebras...

We agree that there are many useful non-associative algebras. However, our work
was focused on rings and all of the underneath structures. This is why, at the
very root, we have semi-groups. Therefore, the associativity is the first axiom
that we deal with, and our machinery is not intended for non-associative
algebras.

> buried assumption, that all terms over algebraic structures have decidable equality!

To clarify: the required decidable equality is only a decidable equality for
the constants, e.g, when proving equalities over Nat, the needed decidable
equality is the one that decides the equality between two natural numbers (with
no variables and no operations), called decEq in Idris.

> - The interface to a Group should have *either* unary or binary -, and then use a
>   *definitional* (and thus conservative) extensio to add the other.  This is again
>   what Realms are useful for.

Having both is useful if the user has something to prove with both the unary Neg and the binary Minus. 
Expressing in the interface something like "there is either a unary or binary -" 
would have been a hassle. However, this is not a limitation : it is always possible 
for the user to define one with the other (and the proof Minus_simpl that he/she 
has to provide then becomes trivial).

>   this isn't something you have a choice to do / not do.

Indeed, we *must* do it. We still wanted to describe what the normalisation function has to do,
and not to only mention the arbitrary decisions that we had to take (such as writing everything on
the complete left or complete right associative form).

These simplifications entail simplifying the addition with zero, and the multiplication 
with the constants zero and with one, doing the computations between two nearby constants, etc.
(mentionned briefly on page 13)

> The hacks involved in Variable (i.e. treat Neg as a pseudo-variable) don't scale
> well.

We agree, but our goal was to implement automatic provers for the albebraic
structures that are underneath rings, not to deal with all possible algebraic
structure. We'll make this clearer in the paper.


