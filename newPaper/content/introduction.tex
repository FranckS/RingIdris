\section{Introduction}


Proving that one term is equal to another within some theories is something quite common in formal certification. Of course, the two things are usually not syntactically equal --this case would be trivial--, but they are equal according to a set of properties. These properties are either obtained by implementation (the implementation of + for natural numbers satisfies the property of associativity for example), or by axioms if we are working within an axiomatised structure. 
These proofs are usually omitted "on the paper" for the ordinary every day maths, but they are required by proof assistants, no matter how obvious they are. Currently, in the Idris\footnote{Idris is a relatively new, purely functional, and dependently typed programming language with full dependent types, which also supports tactic based proofs  (Idris will be briefly defined the the [Section X] (at the end of the paper))} programming language, they are done by hand, and they are made of a potentially long sequence of rewriting, where each step of rewriting uses one of the available properties. 
These proofs are the every day routine when we are working in a dependently typed language, as Idris, and especially when we're intensively indexing types over values in order to capture certain logical properties.
The indices can be for example natural numbers, and in this case the lemmas that we have to prove about these indices are arithmetic statements which use the properties of natural numbers and their usual operations : symmetry and commutativity for +, existence of neutral element for +, etc.

\subsection{A motivating little example}
We introduce a little example which shows how these proofs appear when we index a type by values like natural numbers.
Let's define the types of $Bit$ and $Binary$ number, both indexed over the value they represent, expressed in base ten as a natural number.
\begin{code}[caption=Bit and binary number, captionpos=b, label=lst1:haskell2]
data Bit : Nat -> Type where
     b0 : Bit Z
     b1 : Bit (S Z)
     
data Binary : (width : Nat) -> (value : Nat) 
              -> Type where
     zero : Binary Z Z
     (#) : Binary w v -> Bit bit 
           -> Binary (S w) (bit + 2 * v)
\end{code}

This type of binary numbers\footnote{This representation allows to represent a binary number of width zero with the constructor $zero$, but instead we could have decided to replace this constructor with a constructor taking one bit as parameter and producing a binary number of width one. The representation we've chosen is a bit easier to manipulate.}  has two constructors, and the most interesting one is $\#$ which enables to construct bigger binary numbers : we can extend a binary number $bin$ by adding a digit to its right, and the value represented by this new binary number is two times the value represented by $bin$ plus the value represented by the added digit.

It will be useful to be able to pad a binary number expressed with $w$ bits to a binary number with $S\ w$ digits : 
\begin{code}[caption=Padding, captionpos=b, label=lst1:haskell2]
pad : Binary w n -> Binary (S w) n
pad zero = zero # b0
pad (num # x) = pad num # x
\end{code}
Of course, this operation preserves the value represented by the binary number : this function takes a $Binary\ w\ n$ and produces a $Binary\ (S\ w)\ n$, so there will be no need to prove the correctness of this function after its definition (the $value$ is still $n$).

Now, we're tempted to write the addition between two binary numbers.
We start with an auxiliary function which adds three bits (the third one will latter play the role of the carry), and produces the two bits of the result.

\begin{code}[caption=Addition of three bits, captionpos=b, label=lst1:haskell2]
addBit : Bit x -> Bit y -> Bit c ->
          (bX ** (bY ** 
              (Bit bX, Bit bY, 
                  c + x + y = bY + 2 * bX)))
addBit b0 b0 b0 = (_ ** (_ ** (b0, b0, _)))
addBit b0 b0 b1 = (_ ** (_ ** (b0, b1, _)))
addBit b0 b1 b0 = (_ ** (_ ** (b0, b1, _)))
addBit b0 b1 b1 = (_ ** (_ ** (b1, b0, _)))
addBit b1 b0 b0 = (_ ** (_ ** (b0, b1, _)))
addBit b1 b0 b1 = (_ ** (_ ** (b1, b0, _)))
addBit b1 b1 b0 = (_ ** (_ ** (b1, b0, _)))
addBit b1 b1 b1 = (_ ** (_ ** (b1, b1, _)))
\end{code}

In fact, this function produces a dependent pair. The first component is the value $bX$ represented by the first bit produced (of type $Bit\ bX$), and the second component is itself another dependent pair. This second dependent pair is composed of the value $bY$ represented by the second bit produced, and of a triple containing the two bits produced and the proof that these two bits are effectively encoding the result of the addition we had to perform.
For example, with the line corresponding to the computation $1_2 + 0_2 + 0_2 = (01)_2$, the function produces the bit $b0$ (encoding $0_2$) and the bit $b1$ (encoding $1_2$), and a proof that $1 + 0 + 0 = 1 + (2*0)$.

Again, we note that there is no need to produce any lemma of correctness about $addBit$ afterwards : the correct by construction style in which it is written already gives the property that we want : that the two bits produced effectively represent the addition of the three bits given in input.

Now, we want to define a function adding two binary numbers and a bit (used as a carry). We decide that this addition will only work for two binary numbers expressed with the same number of bits, and we produce a result with one more bit, and this result should of course represent the value $c + x + y$ if $c$ is representing the value of the input bit, and if $x$ and $y$ are representing the values of the two binary numbers given in input. Thus, we're tempted to write :
\begin{code}[caption=Addition of two binary numbers, captionpos=b, label=lst1:haskell2]
adc : Binary w x -> Binary w y -> Bit c 
     -> Binary (S w) (c + x + y)
adc zero zero carry = zero # carry
adc (numx # bX) (numy # bY) carry
   = let (v1 ** (v2 ** (carry0, lsb, _))) = 
      addBit bX bY carry in
          adc numx numy carry0 # lsb
\end{code}
Unfortunately, the types are mismatching for the two cases expressed by this pattern matching. The result of the first line $adc\ zero\ zero\ carry$ is expected to have the type\[Binary\ 1\ (plus\ (plus\ c\ 0)\ 0)\] and we're providing a term of type \[Binary\ 1\ (plus\ c\ (plus\ 0\ (plus\ 0\ 0)))\] [I AM SURPRISED HERE BY THE TYPE OF THE PROVIDED TERM].
The problem is even worse for the second line where the result is expected to have type \[plus\ (plus\ c\ (plus\ bit\ (plus\ v\ (plus\ v\ 0))))\ (plus\ bit\ (plus\ v\ (plus\ v\ 0)))\] while we're trying to provide a term of type \[plus\ v2\ (plus\ (plus\ (plus\ v1\ v)\ v)\ (plus\ (plus (plus\ v1\ v)\ v)\ 0))\]
For this kind of situation, Idris provides the possibility to write a provisional definition --provisional definitions are described with the brief overview of the language in page XX-- with the syntax "?=". Using a provisional definitions here will make the definition being accepted, but we will have to prove afterwards that the term provided can be converted into a term which has the right type.
However, for both of these two patterns, it is possible to prove the equality between the expected and the provided types, and this is even stronger than we need. Using this equality of types it will be trivial to produce a term of the expected type by using the provided term and by simply rewriting its type with this proof. These proofs will be made of a series of rewriting, and each rewriting will use a property about natural numbers and the usual operations.
For example, the proof for the second pattern look like this :
\begin{code}[caption=Proof by hand of the correspondence between expected and provided types for the second line of the definition of adc, captionpos=b, label=lst1:haskell2]
adc_lemma_2 = proof {
    intros;
    rewrite sym (plusZeroRightNeutral x);
    [...]
    rewrite sym (plusAssociative (plus c (plus bit0 (plus v v))) bit1 (plus v1 v1));
    rewrite (plusAssociative c (plus bit0 (plus v v)) bit1);
    rewrite plusCommutative bit1 (plus v v);
    [...]
    rewrite (plusAssociative (plus (plus x v) v1) (plus x v) v1);
    trivial;
}
\end{code}

Thus, this kind of proofs consists of a potentially long sequence of rewriting, every rewriting step using one property of the theory. Without some specific automation, this sequence of rewriting is done by the user of the proof assistant. This is time consuming, and just a little change in the left or the right hand side of the equality will most of the time invalidate completely the proof done by hand. This mean that just a little change in our datatypes, or in the definitions of $addBit$ and $adc$ will requires us to replace this proof by another one doing the same job : assuring the correspondance betwen the expected index and the given index. The re-usability of this kind of proofs is indeed very low, since they are performing rewritings for a very specific term.

If we look at the proof we've written by hand, we realize that we're only using the existence of neutral elements (this is the lemma $plusZeroRightNeutral$), and the associativity and commutativity of $+$. Thus, we're rewriting a term by using the properties of a commutative group.
If we've already proven that natural numbers follow the structure of commutative group, this is the code that we would like to be able to write :

\begin{code}[caption=Code we'd like to write for the addition of two binary numbers, captionpos=b, label=lst1:haskell2]
adc : Binary w x -> Binary w y -> Bit c 
     -> Binary (S w) (c + x + y)
adc zero zero carry = zero # carry 
        by CommutativeGroupSolver
adc (numx # bX) (numy # bY) carry
   = let (v1 ** (v2 ** (carry0, lsb, _))) = 
      addBit bX bY carry in
          adc numx numy carry0 # lsb 
        by CommutativeGroupSolver
\end{code}
We would like to let a solver for commutative groups to find the proof on its own, and thereby, there will be no extra lemmas to prove afterwards.
This is a something doable because algebraic structures like monoid, group, commutative group and ring have all a very nice property : every element in them has a canonical represent. Thus these kind of proofs can be automatically produced by a tactic by simply computing the normal forms for each side of the equality, and then comparing them with the syntactical equality (Leibniz equality).

\subsection{Our contributions}

Algebraic solvers, like ring solvers, are already implemented for various proof assistants, including Coq and Agda. In this paper, we describe a certified implementation of an automatic solver for equalities on algebraic structures, for the Idris language. The interesting thing that we're going to present in this paper lies in the way we construct it, in a "correct by construction" style.

In this paper, we focus on some specific theories, which are the following algebraic structures : magmas, semi-group, monoid, group and Abelian group.
The user will just have to prove that his structure satisfies the properties of an algebraic structure, and he will be allowed to invoke the corresponding tactic.

Each tactic will be implemented by reflection, and the reason for this will be discussed in section [Blabla]. Working by reflection for implementing tactics has been done several times, as in [paper Coq ring] for the implementation of a ring solver for the Coq proof assistant. We will compare our approach with other implementation of this kind of tactics in section [Blabla]. \\
\\
Our contribution mainly consists of three points :
\begin{itemize}
	\item 1) We follow a "correct by construction" approach for implementing the reduction procedures, instead of implementing a normalization procedure, and proving afterwards that this function effectively computes a normal form. The approach will be presented in section 2 on a smaller problem, and on section 3 for the complete hierarchy of algebraic structures. Details of the normalization procedures are given in section 4.
	\item 2) For achieving the first goal, we are extensively using dependent types in order to capture the interesting properties that matter for assuring the correctness of the method. We use an original representation of reflected terms which embeds the denoted --concrete-- value represented. The reflected terms become indexed over the concrete ones. These representations are presented in section 2 for the toy example, and in section 3 for the real problem.
	\item 3) We develop a hierarchy of tactics which enables to increase a lot the reusability of each solver : we reuse as much as possible each solver in the implementation of the other solvers for the above structures. For example simplifying neutral elements is only implemented at the monoid level, and each level above monoid reuse it. As it will be discussed in section 4, reusability can be quite tricky to get when we want to reuse a solver from a structure above which is less expressive. For example, reusing the monoid solver for the group solver is not trivial, since we lose the possibility to express the negations $-x$ and minus $x-y$ operations.
\end{itemize}