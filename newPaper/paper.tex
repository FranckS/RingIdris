%-----------------------------------------------------------------------------
%
%			Edwin's and Franck's Paper !
%  uses the template for sigplanconf LaTeX Class
%
%-----------------------------------------------------------------------------


\documentclass{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[literate]{idrislang} 
\newcommand{\cL}{{\cal L}}

%\lstset {captionpos=b}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{POPL â€™15}{January 15-17, 2015, Mumbai, India.}
\copyrightyear{2014} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\preprintfooter{Our paper for POPL '15}
\doi{nnnnnnn.nnnnnnn}




% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{A correct by construction hierarchy of reflective tactics for proving equalities in algebraic structures}
%\subtitle{An example for a collection of tactics solving equalities in algebraic structures}

%\authorinfo{Edwin Brady\and Franck Slama}
%           {University of St Andrews, United-Kingdom}
%           {ecb10@st-andrews.ac.uk \and fs39@st-andrews.ac.uk}
\authorinfo{Author's name omitted for submission}
           {University of ..., Country}
           {email address omitted for submission}

\bibliographystyle{abbrvnat}

\maketitle

\nocite{*}


\input{./content/abstract.tex}




\input{./content/introduction.tex}


         
\section {A smaller problem : a correct by construction tactic for the associativity of lists concatenation}

In order to better explain the way we are trying to solve our problem, we will first present it on a simplified version, in which we aim to deal with universally quantified natural numbers, the the properties of associativity and neutral element for the operation plus.
For example, we would like to be able to automatically generate proofs of facts like $\forall x1\ x2\ x3 : List,\ (x1 + l2) ++ (x3 + x4) = (x1 + (x2 + x3)) + x4$ [Example 1]. \\
For this smaller problem, we have decided to only work with the associativity of plus : $\forall x1\ x2\ x3,\ (x1 + x2) ++ x3 = x1 ++ (x2 + x3)$ and with the fact that Z is a neutral element on the right side of the plus : $\forall x, x ++ Z = x$. Of course, this work could be extended by dealing with more axioms, and this is exactly what we will do in the next sections when we will write a general hierarchy of solvers for different algebraic structures. \\
Thus, in definitive, in this section, we want to write a decision procedure, able to tell if two expressions composed of universally quantified natural numbers and additions of these numbers are equal, and to produce of proof of this equality if appropriate, when "equal" has the meaning "syntactically equal or equal thanks to associativity or thanks to the "neutrality" of Z". \\

When trying to prove this kind of equalities, the variables are abstracted, and they become part of the context. For the example 1, after abstraction of the variables, the goal becomes simply $(x1 + x2) ++ (x3 + l4) = (x1 + (x2 + x3)) + x4$, which is something of the general form $x=y$.
The general idea -which will also apply for the real bigger problem detailed in the next sections- will be to normalize both sides of the "potential equality" $x=y$, and afterwards to compare them using Leibniz syntactical equality.
The goal of the normalization is of course to compute a canonical representation for a list $x$, such that any other list provably equal to $x$ (by using the two available properties) will have the same canonical representation. That's why it will be possible to decide the equality by simply comparing the two normal forms with a week syntactical equality. \\
\\
Of course, the normalization will have to fulfil the following correctness property :
$\forall x, norm\ x = x$ [1], which means that after normalization we obtain a list which is provably equal to the input list (potentially using our properties multiple times). \\
And after normalization, we will compare the two resulting terms with a standard syntactical equality, and if they are equal, we will have obtained the desired proof.  \\
Indeed :
$norm\ x = norm\ y$ will imply $x=y$ by using the correctness lemma [1] of $norm$ two times. \\
\\
In fact, this is not exactly how it will work. This way would not be easily feasible, because in the LHS and RHS of $x=y$, we potentially have variables which have been universally quantified. And the normalization function would need to do different treatments for a "variable natural number" (a number which has been universally quantified) and for a constant $Z$. This is not possible yet, because once the variables are abstracted, nothing tells us where the variables are in the left and right hand side of the equality : this is a meta information of the AST that we can't access.
For this reason, we will work by reflection, as we will do for for the real problem later. That means that we will define a datatype which will be used as an encoding of natural numbers, or more precisely, an encoding of natural numbers composed of "variable numbers", $Z$, and additions of these. This datatype will allow us to know the internal structure of the number, ie, where the variables and constants are.
Indeed, this datatype will allow pattern matching. Previously, we were only able to pattern match a list against the constructor $Z$ and $S$, which wasn't interesting for our goal.  With the first approximation of the datatype $Expr$ presented in the Listing 1, we will be allowed to pattern match an encoding of number against the constructor Plus, Var and Zero, which gives us the informations we want.

\begin{code}[caption=First version of reflected numbers, captionpos=b, label=lst1:haskell2]
data Expr : (G : Vect n Nat) -> Type where
     Plus : {n:Nat} -> {G:Vect n Nat} -> 
            Expr G -> Expr G -> Expr G
     Var  : {n:Nat} -> {G:Vect n Nat} -> 
            (i : Fin n) -> Expr G
     Zero : {n:Nat} -> (G:Vect n Nat) -> 
            Expr G Z
\end{code}

For this smaller problem, we want to prove equalities in which numbers are always universally quantified, so we only need to represent variable (a universally quantified natural), the constant Z, and the addition.
Variables are represent using a De Brujin-like index : (Var fZ) denotes a variable, (Var (fS fZ)) another one, and so on.

The type Expr is indexed over a vector of numbers (we will usually name it $G$), that represents our context. For example, if in the example 1, we will have to encode $(x1 + x2) + (x3 + x4)$ and $(x1 + (x2 + x3)) + x4$ in a context where four elements are declared. The first element of this context denotes the variable $x1$, the second denotes $x2$, and so on.
Thus, the left hand side will be encoded by :
\begin{code}[caption=Reflected LHS of example 1, captionpos=b, label=lst1:haskell2]
App (App (Var G fZ) 
         (Var G (fS fZ))) 
    (App (Var G (fS (fS fZ))) 
         (Var G (fS (fS (fS fZ)))))
\end{code}

We now need to define what will be the normalization procedure.
If, as it is probably expected, this function simply takes an Expr and produces another Expr, then we will need to prove the following lemma afterwards, corresponding to the correctness property mentioned before : \\
$\forall\ e:Expr\ G,\ interpretation\ (reduce\ e)\ =\ interpretation\ e$ \\
where $interpretation$ is a function computing the interpretation of an Expr, that is to say, the list this Expr is encoding.
This proof can be quite tricky to make because it relies on the complete behaviour of the reduction and on the way we compute the interpretation.
For this little example, the reduction will not be too heavy, but in the next sections, when we will have more properties than only associativity and neutral element to deal with, it will certainly become more problematic to "unfold" the definition of a gigantic reduction procedure. \\
\\
To avoid these two sources of complexity, we will : \\
- Use dependent types in order to directly capture and embed the concrete number that an Expr is encoding. Thus, it won't be necessary to define the $interpretation$ function. \\
- Write a "correct by construction" reduction procedure. The proof of preservation of the interpretation will be computed "bit by bit" at the same time as the function will produce the normalized expression (by rewriting the term thanks to the properties of associativity and neutral element).

The type for reflected lists therefore becomes :
\begin{code}[caption=Second version of reflected lists with embedded denotation, captionpos=b, label=lst1:haskell2]
using (x : Nat, y : Nat, G : Vect n Nat)
  data Expr : (G : Vect n Nat) -> Nat -> Type where
       Plus : Expr G x -> Expr G y -> Expr G (x + y)
       Var  : (i : Fin n) -> Expr G (index i G)
       Zero : Expr G Z
\end{code}
For an expression $e\ :\ Expr\ G\ x$, we will say that "$e$ denotes (or encodes) the number $x$".
When an expression is a variable, the denoted number is simply the corresponding variable in the context, ie, $(index\ i\ G)$.
Also, the $Zero$ expression denotes the natural number $Z$.
Finally, if $e1$ is an expression encoding the number $x$, and $e2$ is an expression encoding the number $y$, then the expression $Plus\ e1\ e2$ denotes the number $(x + y)$.

The reduction procedure is now supposed to be written on a "correct by construction" way, which means that no additional proof should be required after the definition of the function. Thus, $reduce$ will produce the proof that the new Expr produced has the same interpretation as the original Expr, and this will be made easier by the fact that the datatype Expr is now indexed over the real --concrete-- list : a term of type $Expr\ G\ x$ is the encoding of the number $x$.
Thus, we can write the type of $reduce$ like this : \\
$reduce\ :\ Expr\ G\ x\ \rightarrow\ (x'\ **\ (Expr\ G\ x',\ x\ =\ x'))$ \\
The function $reduce$ produces a dependent pair : the new concrete number $x'$, and a pair made of an $Expr\ G\ x'$ which is the new encoded term indexed by the new concrete number we have just produced, and a proof that old and new --concrete-- numbers are equal.
Note that this function can't simply produce an $Expr\ G\ x$, because the number on which the resulting expression will be indexed is not necessary syntactically equal to the original number since it can use the two available properties. 
And in fact, what really interest us in this function is precisely the proof of $x\ =\ x'$.
The reason is simple : when we try to automatically prove $x=y$, these proofs $x=x'$ and $y=y'$ will be the crucial part for the construction of the desired proof. \\
\\
We will have an expression $e1$ encoding $x$, and an expression $e2$ encoding $y$\footnote{These encodings have to be produced by hand by the user for this small simple problem, but for the real collection of tactics, we will program an automatic metaification}.
We will normalize $e1$, and this will give a new number $x'$, a new expression $e1':Expr\ G\ x'$, and a proof of $x=x'$. We will do the same with $e2$, and we will get a new number $y$, an expression $e2':Expr\ G\ y'$, and a proof of $y=y'$. \\
Now, we can compare $e1'$ and $e2'$ using a standard syntactical equality because they are in their normal form :

\begin{code}[caption=Syntactical equality, captionpos=b, label=lst1:haskell2]
  eqExpr : (e : Expr G xs) -> (e' : Expr G ys) ->
           Maybe (e = e')
  eqExpr (Plus x y) (Plus x' y') with (eqExpr x x', eqExpr y y')
    eqExpr (Plus x y) (Plus x y) | (Just refl, Just refl) = Just refl
    eqExpr (Plus x y) (Plus x' y') | _ = Nothing
  eqExpr (Var i) (Var j) with (prim__syntactic_eq _ _ i j)
    eqExpr (Var i) (Var i) | (Just refl) = Just refl
    eqExpr (Var i) (Var j) | _ = Nothing
  eqExpr Zero Zero = Just refl
  eqExpr _ _ = Nothing
\end{code}


And of course, if $e1'$ and $e2'$ are equal, then they necessary have the same type\footnote{We are working with the heterogeneous equality JMeq, but as always, the only way to have a proof of a:A = b:B is when A=B}, and therefore $x'=y'$
By rewriting the two previously obtained equality $x=x'$ and $y=y'$ in the new equality $x'=y'$, we can get a proof of $x=y$.

\begin{code}[caption=Building the desired proof with the two proofs of equality, captionpos=b, label=lst1:haskell2]
  buildProof : {xs : Nat} -> {ys : Nat} ->
               Expr G ln -> Expr G rn ->
               (xs = ln) -> (ys = rn) -> Maybe (xs = ys)
  buildProof e e' lp rp with (eqExpr e e')
    buildProof e e lp rp | Just refl = ?bp1
    buildProof e e' lp rp | Nothing = Nothing
\end{code}

As we said, the proof for the metavariable is just the rewriting of the two equalities :

\begin{code}[caption=buildProof metavariable, captionpos=b, label=lst1:haskell2]
  bp1 = proof {
  intros;
  refine Just;
  rewrite sym p1;
  rewrite sym p2;
  exact refl;
}  
\end{code}
Finally, the main function which tries to prove the equality $x=y$ is simply made of the reduction of the two metaified terms reflecting the left and the right hand side, and of the composition of the two obtained proofs thanks to the function $buildProof$:
\begin{code}[caption=testEq, captionpos=b, label=lst1:haskell2]
  testEq : Expr G x -> Expr G y 
           -> Maybe (x = y)
  testEq l r = 
     let (x' ** (l', p1)) = reduce l in 
     let (y' ** (r', p2)) = reduce r in
        buildProof l' r' p1 p2
\end{code}
Now, we need to define the function reduce. To do that, we have to fix a canonical representation of associative natural numbers. We decide that the left associative form will be the canonical representation. Thus, the $reduce$ function will have to rewrite the metaified term by rearranging the parentheses in order to transform the underlying number in the form $(...((x1 + x2) + x3) ... + xn)$. To do so, one possibility is to define a new datatype which captures this property, and to write a function going from $Expr$ to this new type. Thus it will be easier to be certain that we are effectively computing the normal form : forcing properties to hold by the shape of a datatype is a good usage of dependent types when, like here, it doesn't introduce more complications.
\begin{code}[caption=Reflected left associative numbers, captionpos=b, label=lst1:haskell2]
  data LExpr : (G : Vect n Nat) -> Nat -> Type where
       LPlus : LExpr G x -> (i : Fin n) -> LExpr G (x + index i G)
       LZero : LExpr G Z
\end{code}
This datatype has only two constructors. In fact, it combines the previous $Var$ and $Plus$ constructors so that it becomes impossible to write an expression which isn't left associative.
 
As part of the normalization, we will write a function $expr\_l$ converting an $Expr\ G\ x$ to a $LExpr\ G\ x'$ and producing a proof that $x=x'$. This function will therefore use the property of associativity multiple times, in order to obtain the expected fully left associative form. Of course, because by using this new datatype $LExpr$ we've changed the representation of our encoded lists, we will need to convert back an $LExpr\ G\ x$ to an $Expr\ G\ x$. The function $l\_expr$ will do this easy task.

\begin{code}[caption=Production of the left associative form, captionpos=b, label=lst1:haskell2]
  expr_l : Expr G xs -
        > (xs' ** (LExpr G xs', xs = xs'))
  expr_l Zero = (_ ** (LZero, refl))
  expr_l (Var i) = (_ ** (LPlus LZero i, refl))
  expr_l (Plus ex ey) = let (xl ** (xr, xprf)) = expr_l ex in
                       let (yl ** (yr, yprf)) = expr_l ey in
                               PlusLExpr _ _ xr yr xprf yprf
    where
      PlusLExpr : (x', y' : Nat) ->
                 {G : Vect n Nat} -> {x, y : Nat} ->
                 LExpr G x -> LExpr G y -> (x' = x) -> (y' = y) ->
                 (w' ** (LExpr G w', x' + y' = w'))
      PlusLExpr x' y' rx (LPlus e i) xprf yprf
         = let (x ** (rec, prf)) = PlusLExpr _ _ rx e refl refl in
               (_ ** (LPlus rec i, ?PlusLExpr1))
      PlusLExpr x' y' rx LZero xprf yprf = (_ ** (rx, ?PlusLExpr2))

  l_expr : LExpr G xs -> Expr G xs
  l_expr LZero = Zero
  l_expr (LPlus x i) = Plus (l_expr x) (Var i)

\end{code}

We notice that for transforming the number into its left associative equivalent representation, we've effectively needed to know where the variables and the $Z$ constants are : the functions $expr\_l$ and $l\_expr$ are doing different treatments for these different possibilities. \\
\\
We've got two metavariables to prove. The metavariable $appLExpr1$ requires us to prove the goal : $x' + y' = x + index\ i\ G$ in a context where we've got, amongst other things,  $(xprf\ :\ x'\ =\ x)$, $(yprf\ :\ y'\ =\ x2\ ++\ index\ i\ G)$ and $(prf\ :\ x\ ++\ x2\ =\ x)$.
Proving this goal uses the property of associativity after rewriting the goal with these three proof of equality $xprf$, $yprf$ and $prf$.

\begin{code}[caption=Proof of the metavariable appLExpr1, captionpos=b, label=lst1:haskell2]
PlusRExpr1 = proof {
  intros;
  rewrite sym xprf;
  rewrite sym yprf;
  rewrite prf;
  rewrite sym 
       (plusAssociative x x2 (index i G));
  exact refl;
}
\end{code}

And the metavariable appRExpr2 uses the fact that $Z$ is a neutral element for the concatenation.

\begin{code}[caption=Proof of the metavariable appLExpr2, captionpos=b, label=lst1:haskell2]
PlusLExpr2 = proof
  intros
  rewrite xprf 
  rewrite (sym yprf)
  mrefine a_plus_zero
\end{code}

We can now define the reduction, which is just the composition of the two previous functions $expr\_l$ and $l\_expr$:

\begin{code}[caption=Reduction function, captionpos=b, label=lst1:haskell2]
  reduce : Expr G x -> 
           (x' ** (Expr G x', x = x'))
  reduce e = 
     let (x' ** (e', prf)) = expr_l e in
        (x' ** (l_expr e', prf))
\end{code}

At the moment, what we've got is not exactly a real tactic, in the sense that we only have a function which produces a value of type $Maybe (x = y)$. A real tactic would be a coating of this function, which could properly fail when the two terms are not equal. However, here, when $x\ne y$, the function $testEq$ will simply produce the value $Nothing$. \\
It's now time to see how to use this minimalist "tactic".
Let's define two expressions $e1$ and $e2$, respectively representing the numbers $((x + ys) + (x + z))$ and $(x + ((y + x) + z))$ in the context $[x, y, z]$ of three abstracted variables.

\begin{code}[caption=Two test expressions, captionpos=b, label=lst1:haskell2]
  e1 : (xs, ys, zs : Nat) -> 
           Expr [xs, ys, zs] ((xs + ys) + (xs + zs))
  e1 xs ys zs = Plus (Plus (Var fZ) (Var (fS fZ))) 
                        (Plus (Var fZ) (Var (fS (fS fZ))))

  e2 : (xs, ys, zs : Nat) -> 
           Expr [xs, ys, zs] (xs + ((ys + xs) + zs))
  e2 xs ys zs = Plus (Var fZ) 
         (Plus (Plus (Var (fS fZ)) (Var fZ)) (Var (fS (fS fZ))))
\end{code}

The numbers denoted by the expressions $e1$ and $e2$ are equal, and we can generate a proof of this by using $testEq$.
\begin{code}[caption=Test of equality betwen e1 and e2, captionpos=b, label=lst1:haskell2]
  e1_e2_testEq : (xs, ys, zs : Nat) ->
          Maybe (((xs + ys) + (xs + zs)) = (xs + ((ys + xs) + zs)))
  e1_e2_testEq xs ys zs = testEq (e1 xs ys zs) (e2 xs ys zs)
\end{code}

And if we ask for the evaluation of this term, we should obtain $Just$ and a proof of equality between the two underlying numbers.
\begin{code}[caption=Evaluation of the result, captionpos=b, label=lst1:haskell2]
\xs => \ys => \zs => e1_e2_testEq xs ys zs

\xs => \ys => \zs => Just (replace (sym (replace 
(sym (replace (replace (plusZeroRightNeutral xs)
 refl (replace (sym (plusAssociative xs [] ys)) 
 refl))) (replace (sym (replace (replace 
 (plusZeroRightNeutral [...] refl)))) refl)) 
: (xs : List Int) -> (ys : List Int) -> 
  (zs : List Int) -> 
  Maybe ((xs ++ ys) ++ xs ++ zs 
         = xs ++ (ys ++ xs) + zs)
\end{code}

And we effectively get the proof of equality we wanted. As expected, this proof uses the properties of associativity ($plusAssociative$) and the property of neutrality of $Z$ for $plus$ ($plusZeroRightAssociative$).

\section {Back to the general problem : A hierarchy of tactics (2/3 pages)}

Now that we have introduced our ideas on this little example, it is time to apply them for our main goal : the implementation of a hierarchy of tactics proving equalities in algebraic structures. Very often, the properties available on a given type are the one of a well known structure : magma, semi-group, monoid, group, commutative-group...  We will no longer only work with the properties of associativity and neutral elements for Lists as we did in the previous section, but we will have available the properties of different structures, and these tactics will usable for any type that satisfies these properties.
In a Magma, we only have a $Plus$ operation, and no axioms with it. With a Semi-Group, we will have one axiom about this Plus : the property of associativity. In a Monoid, we will have one more property : the fact that we have a neutral element. In fact, this property will be expressed as 3 facts : there is a value $Zero$, and we've got $\forall x, Zero + x = x$ and $\forall x, x + Zero = x$. \\

We will construct a hierarchy of typeclasses. All our tactics (the group solver, the monoid solver...) will be able to work on any type, being given that an instance of the corresponding typeclass is provided.

\subsection {Hierarchy of typeclass}

All our tactics will require to have a way of testing the equality between elements of the underlying set, that is to say, a way to test equality between constants. For this reason, we define a notion of "set", which only requires this equality. All the algebraic structures above will extend this typeclass :

\begin{code}[caption=Set, captionpos=b, label=lst1:haskell2]
class Set c where
    set_eq : (x:c) -> (y:c) -> Maybe (x=y)
\end{code}

Note that this equality is a "week" equality : it only produces a proof when the two elements are equal, but it doesn't produce a proof of dis-equality when they are different. That's quite natural, since we want to generate proof of equality, and not to generate counter examples for dis-equalities, which is another problem.

Of course, there will be no tactic associated to $Set$, since we have no operations and not properties associated to this structure. Therefore, the equality in a $Set$ is just the syntactical equality, and can simply be proven with $refl$.

The first real structure, almost trivial, is the magma. A magma has just a $Plus$ operation which let us computing constants, and has no axioms about this operation. Thus, as we will see later, the normalization will be simply a computation between constants inside parenthesis.

\begin{code}[caption=Magma, captionpos=b, label=lst1:haskell2]   
class Set c => Magma c where
    Plus : c -> c -> c
\end{code}

A bit more interesting is the Semi-Group typeclass. A semi-Group is a Magma (ie, it still has a $plus$ operation), but moreover it has the property of associativity for this operation.

\begin{code}[caption=Semi-Group, captionpos=b, label=lst1:haskell2]   
class Magma c => SemiGroup c where
    Plus_assoc : (c1:c) -> (c2:c) -> (c3:c) 
           -> (Plus (Plus c1 c2) c3 
                = Plus c1 (Plus c2 c3))
\end{code}

The Monoid, for its part, is a Semi-Group with the property of neutral element.

\begin{code}[caption=Monoid, captionpos=b, label=lst1:haskell2]   
class SemiGroup c => Monoid c where
    Zero : c    
    Plus_neutral_1 : (c1:c) 
            -> (Plus Zero c1 = c1)    
    Plus_neutral_2 : (c1:c) 
             -> (Plus c1 Zero = c1)
\end{code}	

A Group is a Monoid, with two new operations. The binary operation $Minus$, and the unary operation $Neg$. One axiom says that the $Minus$ can always be simplified with the $Neg$ and the $Plus$. That means that $Minus$ is not a "primitive" operation of a Group, since we can always rewrite $(a-b)$ into $(a\ +\ -b)$.
The second axiom, which is the most important one for a Group, is the fact that any $c1$ admits $Neg\ c1$ as an inverse, where being an inverse if the following property :

\begin{code}[caption=Group, captionpos=b, label=lst1:haskell2]   
-- This is just a conjunctive predicate
hasSymmetric : (c:Type) -> (p:dataTypes.Monoid c) 
                -> c -> c -> Type
hasSymmetric c p a b = 
         (Plus a b = Zero, Plus b a = Zero)    
  
class dataTypes.Monoid c => dataTypes.Group c where
	Minus : c -> c -> c
	Neg : c -> c
	Minus_simpl : (c1:c) -> (c2:c) -> 
	             Minus c1 c2 = Plus c1 (Neg c2) 
	Plus_inverse : (c1:c) -> 
	             hasSymmetric c (instance) c1 (Neg c1)
\end{code}	

This hierarchy can be extended without difficulty with Abelian groups (with the axiom of commutativity), Ring, Commutative Rings...

The important thing to remember is that these typeclass will somehow be used as predicates on types. In order to call a solver on a type $T$, the user of the system will have to satisfy the corresponding typeclass by providing an instance of it for his type $T$, ie, he will have to prove that the corresponding properties effectively hold for $T$.

Note that the properties required by these typeclass for a specific type $T$ will either be proven if the operations on $T$ are real computable definitions, or axiomatised if the user is working on an axiomatised theory where the operations ($Plus$, $Neg$) are just axioms.

As discussed in the section above, the algorithm of normalization will not directly use the concrete value of type $T$, but a reflected term, indexed over the concrete value.


	\subsection {Reflected terms}

We will have one datatype for reflected terms in each algebraic structure. \\
Each of these datatype is parametrised on a type $c$ : the real type on which we want to prove equalities. It is also indexed over an instance of the corresponding typeclass for $c$ (we usally call it $p$, because it looks like a proof saying that the structure $c$ has the desired properties), and indexed over a context (a vector $g$ of $n$ elements of type $c$), and also indexed over a value of type $c$, which is precisely the encoded value.
A magma is only equiped with one operation $Plus$. Thus, we only have three concepts to express in order to reflect terms in a Magma : constantes, variables, and addition.

\begin{code}[caption=Reflected terms in a Magma, captionpos=b, label=lst1:haskell2]  
data ExprMa : Magma c -> (Vect n c) -> c -> 
              Type where
    ConstMa : (p : Magma c) -> (g:Vect n c) 
         -> (c1:c)  -> ExprMa p g c1 
    PlusMa : {p : Magma c} -> {g:Vect n c}
         -> {c1:c} -> {c2:c} 
         -> ExprMa p g c1 
         -> ExprMa p g c2 
         -> ExprMa p g (Plus c1 c2) 
    VarMa : (p:Magma c) -> {g:Vect n c}
         -> {c1:c} 
         -> VariableA (get_eq_magma p) g c1
         -> ExprMa p g c1
\end{code}	

When we encode a constant $c1$ in a context $\Gamma$, we use the constructor $ConstMa$ to produce a term of type $ExprMa\ p\ \Gamma\ c1$ : the index representing the concrete value is simply the constant $c1$.
If e1 is an expression of type $ExprMa\ p\ \Gamma\ c1$ (ie, a term encoding the value $c1$), and $e2$ is an expression of type $ExprMa\ p\ \Gamma\ c2$ (a term encoding the value $c2$), then the term $PlusMa\ e1\ e2$ will have the type $ExprMa\ p\ \Gamma\ (Plus\ c1\ c2)$, ie, this term will encode the value $(Plus\ c1\ c2)$, where $Plus$ is the operation defined in the courant instance $p$.


We've used $get\_eq\_magma$ in the constructor $VarMa$, and this is just a function which returns the equality test of the $Set$ level.
For the moment, the definition of $Variable$ is just the following:

\begin{code}[caption=Reflected variables, captionpos=b, label=lst1:haskell2]  
data Variable : {c:Type} -> {n:Nat}
  -> (c_equal : (c1:c)->(c2:c)->Maybe(c1=c2)) 
  -> (Vect n c) -> c -> Type where
    RealVariable : (c_equal:(c1:c)->(c2:c)
                    ->Maybe(c1=c2))
          -> (g:Vect n c) -> (i:Fin n) 
          -> Variable c_equal g (index i g) 
\end{code}	

This type will be extended with some other constructors later on. For the moment, we can only express a "real variable". \\
\\
Because there is no more operations in a SemiGroup or in a Monoid, the datatypes for reflected terms in these two structures will have exactly the same shape as the one for Magma that we've given above.
However, the one for $Group$ will introduce two new constructors for the $Neg$ and $Minus$ operations :

\begin{code}[caption=Reflected terms in a Group, captionpos=b, label=lst1:haskell2]
data ExprG :  Group c -> (Vect n c) -> c -> 
              Type where
    ConstG : (p : dataTypes.Group c) 
       -> (g:Vect n c)
       -> (c1:c) -> ExprG p g c1
    PlusG : {p : Group c} -> {g:Vect n c} 
       -> {c1:c} -> {c2:c} 
       -> ExprG p g c1 
       -> ExprG p g c2 
       -> ExprG p g (Plus c1 c2)
    MinusG : {p : Group c} -> {g:Vect n c} 
       -> {c1:c} -> {c2:c} 
       -> ExprG p g c1 
       -> ExprG p g c2 
       -> ExprG p g (Minus c1 c2)
    NegG : {p : Group c} -> {g:Vect n c} 
       -> {c1:c} -> ExprG p g c1 
       -> ExprG p g (Neg c1)
    VarG : (p : Group c) -> {g:Vect n c} 
       -> {c1:c}
       -> Variable (get_eq_group p) Neg g c1 
       -> ExprG p g c1
\end{code}

Of course, the index of type $c$ (the real value represented by the expression) is always expressed with the oeprations available in the typeclass $p$, and here for a Group they are$Plus$, $Minus$ and $Neg$.

	\subsection { A bit of notation}
The equality we are trying to prove is $x=y$, where $x$ and $y$ are elements of the type $c$, which  simulates a set with some properties (making $c$ a Semi-Group, a Monoid, a Group...). The fact that $c$ fulfil the specification of an algebraic structure is expressed as an instance of the corresponding typeclass, and this instance will be denoted as $p$.
The reflected term for $x$ will be denoted $e1$, and this term will have the type $ExprG\ p\ g\ x$ : $e1$ is the encoding of $x$ and its type is precisely indexed over the real value $x$.
We've got similar thing for $y$, which is encoded by $e2$, and its type is indexed over the real value $y$.
Running the normalisation procedure on $e1$ will produce the normal form $e1'$ of type $ExprG\ p\ g\ x'$ and a proof of $p1:x=x'$ while running the normalisation procedure on $e2$ will produce the normal form $e2'$ of type $ExprG\ p\ g\ y'$ and a proof of $p2:y=y'$.

	\subsection {Deciding equality}
	
Each algebraic structure will have a function for reducing the reflected terms into their normal forms. This is fact a nice property that all these algebraic structures admit a canonical represent for any element. Without this property, it would become a lot more complicated to decide the equality without bruteforcing the 
These functions will compute this canonical represent, by rewriting the given term multiple times, and at the same time, it will produce the proof of equality between the real value indexing the original term and the real value indexing the produced term.

The profile of these function will always be the same :
\begin{code}[caption=Type of the reduction function for terms reflecting elements in a Group, captionpos=b, label=lst1:haskell2]
	groupReduce : {c:Type} -> {n:Nat} 
	  -> (p:Group c) -> {g:Vect n c} -> {x:c}
	  -> (ExprG p g x) 
	  -> (x' ** (ExprG p g x', x=x'))
\end{code}

This function will have more job to do in structures with multiple axioms (like Group), than it has in the easier structure underneath.
The detail of these functions will be given in the next section. For the moment, we just assume that we've got such a function for each structure, and we want to decide equality between elements. We continue to give the details for the Group, but the principle is exactly the same with the other structures.

We want to write the following function :
\begin{code}[caption=Type of the function for deciding equality between elements in a Group, captionpos=b, label=lst1:haskell2]
	groupDecideEq : (p:Group c) -> {g:Vect n c} 
	   -> {x : c} -> {y : c} 
	   -> (ExprG p g x) -> (ExprG p g y) 
	   -> Maybe (x = y)
\end{code}

The first thing this function has to do is to compute the normal form of the two expressions $e1$ and $e2$ respectively encoding $x$ and $y$.
Then, the only thing remaining to do will be to compare syntactically $e1'$ and $e2'$, and if they are equal, then we will get $x'=y'$, and we will be able to produce the desired proof of $x=y$ by "composing" the two proofs $p1$ and $p2$. 

\begin{code}[caption=Decides if two elements of a group are equal, captionpos=b, label=lst1:haskell2]
groupDecideEq p e1 e2 =
  let (x' ** (e1', p1)) = 
    groupReduce p e1 in
  let (y' ** (e2', p2)) = 
    groupReduce p e2 in
	     buildProofGroup p e1' e2' p1 p2
\end{code}

The syntactical test of equality between $e1'$ and $e2'$ and the composition of the two proofs is done in the auxiliary function $buildProofGroup$. 

\begin{code}[caption=Composes the two proofs if the NF are the same, captionpos=b, label=lst1:haskell2]
buildProofGroup : (p:Group c) -> {g:Vect n c} 
  -> {x : c} -> {y : c} 
  -> {x':c} -> {y':c} 
  -> (ExprG p g x') 
  -> (ExprG p g y') 
  -> (x = x') -> (y = y') 
  -> (Maybe (x = y))
buildProofGroup p e1 e2 p1 p2 
       with (exprG_eq p _ e1 e2)
	buildProofGroup p e1 e1 p1 p2 | Just refl = 
	         ?MbuildProofGroup
	buildProofGroup p e1 e2 p1 p2 | Nothing =
	          Nothing
\end{code}

The proof for the metavariable MbuildProofGroup is exactly the same as the corresponding one (called $bp$) in the smallest example showed in section 2.


\section {The details : normalization functions (4/5 pages)}

	\subsection {Normalization of terms in a Magma}

	\subsection {Normalization of terms in a Semi-Group}

	\subsection {Normalization of terms in a Monoid}

	\subsection {Normalization of terms in a Group}

\section {Auxiliary problems : Automatic metaification and coating (1/2 pages)}

	\subsection {Automatic metaification}

	\subsection {Coating : transforming this stuff into real tactics}

\section {Related work (1 page)}

\section {Conlusion (0,5/1 page)}


\acks

Acknowledgments : Chris, Mattus for discussions. Jan for help in latex.

% We recommend abbrvnat bibliography style.


\bibliography{biblio}

\appendix
\section{Appendix Title}

This is the text of the appendix, if you need one.

\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

