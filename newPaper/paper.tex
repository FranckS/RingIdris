%-----------------------------------------------------------------------------
%
%			Edwin's and Franck's Paper !
%  uses the template for sigplanconf LaTeX Class
%
%-----------------------------------------------------------------------------


\documentclass{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[literate]{idrislang} 
\newcommand{\cL}{{\cal L}}

%\lstset {captionpos=b}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{POPL ’15}{January 15-17, 2015, Mumbai, India.}
\copyrightyear{2014} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\preprintfooter{Our paper for POPL '15}
\doi{nnnnnnn.nnnnnnn}




% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{short description of paper}   % 'preprint' option specified.

\title{A correct by construction hierarchy of reflective tactics for proving equalities in algebraic structures}
%\subtitle{An example for a collection of tactics solving equalities in algebraic structures}

%\authorinfo{Edwin Brady\and Franck Slama}
%           {University of St Andrews, United-Kingdom}
%           {ecb10@st-andrews.ac.uk \and fs39@st-andrews.ac.uk}
\authorinfo{Author's name omitted for submission}
           {University of ..., Country}
           {email address omitted for submission}

\bibliographystyle{abbrvnat}

\maketitle

\nocite{*}


\input{./content/abstract.tex}




\input{./content/introduction.tex}


         
\section {A smaller problem : a correct by construction tactic for the associativity of lists concatenation}

In order to better explain the way we are trying to solve our problem, we will first present it on a simplified version, in which we aim to deal with universally quantified natural numbers, the the properties of associativity and neutral element for the operation plus.
For example, we would like to be able to automatically generate proofs of facts like $\forall x1\ x2\ x3:Nat,\ (x1 + l2) + (x3 + x4) = (x1 + (x2 + x3)) + x4$ [Example 1]. \\
For this smaller problem, we have decided to only work with the associativity of plus : $\forall x1\ x2\ x3,\ (x1 + x2) + x3 = x1 + (x2 + x3)$ and with the fact that Z is a neutral element on the right side of the plus : $\forall x, x + Z = x$. Of course, some structures can have more or less properties than these two, and this work will be extended in the next sections when we will write a general hierarchy of solvers for different algebraic structures. \\
Thus, in definitive, in this section, we want to write a decision procedure, able to tell if two expressions composed of universally quantified natural numbers and additions of these numbers are equal, and to produce of proof of this equality if appropriate, when "equal" has the meaning "syntactically equal or equal thanks to associativity or thanks to the neutrality of Z". \\


\subsection{Working by reflexion}

When trying to prove this kind of equalities, the variables are abstracted, and they become part of the context. On the example 1, after abstraction of the variables, the goal becomes simply $(x1 + x2) + (x3 + x4) = (x1 + (x2 + x3)) + x4$, which is something of the general form $x=y$.
The general idea --which will also apply for the more general and bigger problem detailed in the next sections-- will be to normalize both sides of the "potential equality" $x=y$, and afterwards to compare them using Leibniz syntactical equality.
The goal of the normalization is of course to compute a canonical representation for a number $x$, such that any other number provably equal to $x$ (by using the two available properties) will have the same canonical representation. That's why it will be possible to decide the equality by simply comparing the two normal forms with a week syntactical equality. \\
\\
Of course, the normalization will have to fulfil the following correctness property :
$\forall x, norm\ x = x$ [1], which means that after normalization we obtain a number which is provably equal to the input number (potentially using our properties multiple times). \\
And after normalization, we will compare the two resulting terms with a standard syntactical equality, and if they are equal, we will get the desired proof.  \\
Indeed :
$norm\ x = norm\ y$ will imply $x=y$ by using the correctness lemma [1] of $norm$ two times. \\
\\
In fact, this is not exactly how it will work. This way would not be easily feasible, because in the LHS and RHS of $x=y$, we potentially have variables which have been universally quantified. And the normalization function needs to do different treatments for a "variable natural number" (a number which has been universally quantified) and for the constant $Z$. This is not possible yet, because once the variables are abstracted, nothing tells us where the variables are in the left and right hand side of the equality : this is a meta information of the ASTs representing the two terms that we can't access.
For this reason, we will work by reflection, as we will do for the hierarchy of tactics in the next section. In this smaller example, it means that means that we will define a datatype which will be used as an encoding of natural numbers, or more precisely, an encoding of natural numbers composed of "variable numbers", $Z$, and additions of these. This datatype will allow us to know the internal structure of a number, ie, where the variables and constants are.
Indeed, this datatype will allow pattern matching. Previously, we were only able to pattern match a natural number against the constructors $Z$ and $S$, which wasn't interesting for our goal.  With the first approximation of the datatype $Expr$ presented in the Listing 1, we will be allowed to pattern match an encoding of number against the constructor $Plus$, $Var$ and $Zero$, which gives us the informations we want.

\begin{code}[caption=First version of reflected numbers, captionpos=b, label=lst1:haskell2]
data Expr : (gam : Vect n Nat) -> Type where
     Plus : {n:Nat} -> {gam:Vect n Nat} -> 
            Expr gam -> Expr gam -> Expr gam
     Var  : {n:Nat} -> {gam:Vect n Nat} -> 
            (i : Fin n) -> Expr gam
     Zero : {n:Nat} -> (gam:Vect n Nat) -> 
            Expr gam Z
\end{code}

For this smaller problem, we want to prove equalities in which numbers are always universally quantified, so we only need to represent variables (universally quantified natural numbers), the constant Z, and additions.
Variables are represent using a De Brujin-like index : (Var fZ) denotes a variable, (Var (fS fZ)) another one, and so on.

The type Expr is indexed over a vector of numbers (we will usually name it $\Gamma$), which represents our context. For example, in the example 1, we will have to encode $(x1 + x2) + (x3 + x4)$ and $(x1 + (x2 + x3)) + x4$ in a context where four elements are declared. The first element of this context denotes the variable $x1$, the second denotes $x2$, and so on.
Thus, the left hand side will be encoded by :
\begin{code}[caption=Reflected LHS of example 1, captionpos=b, label=lst1:haskell2]
App (App (Var gam fZ) 
         (Var gam (fS fZ))) 
    (App (Var gam (fS (fS fZ))) 
         (Var gam (fS (fS (fS fZ)))))
\end{code}

We now need to define what will be the normalization procedure.
If, as it is probably expected, this function simply takes an Expr and produces another Expr, then we will need to prove the following lemma afterwards, corresponding to the correctness property mentioned before : \\
$\forall\ e:Expr\ \Gamma,\ interpretation\ (reduce\ e)\ =\ interpretation\ e$ \\
where $interpretation$ is a function computing the interpretation of an Expr in a context $\Gamma$, that is to say, the number that this Expr is encoding.
This proof can be quite tricky to make because it relies on the complete behaviour of the reduction and on the way the interpretation is computed.
For this little example, the reduction will not be too heavy, but in the next sections, with more sophisticated algebraic structures, we will have more rewriting rules to deal with, it will certainly become more problematic to "unfold" the definition of a gigantic reduction procedure. \\
\\
To avoid these two sources of complexity, we will : \\
- Use dependent types in order to directly capture and embed the concrete number that an Expr is encoding. Thus, it won't be necessary to define the $interpretation$ function. \\
- Write a "correct by construction" reduction procedure. The proof of preservation of the interpretation will be computed bit by bit at the same time as the function will produce the normalized expression (by rewriting the term by using the two properties).

Thus, the type for reflected numbers becomes :
\begin{code}[caption=Second version of reflected number with embedded denotation, captionpos=b, label=lst1:haskell2]
using (x : Nat, y : Nat, gam : Vect n Nat)
  data Expr : (Vect n Nat) -> Nat 
               -> Type where
       Plus : Expr gam x -> Expr gam y -> 
              Expr gam (x + y)
       Var  : (i : Fin n) -> 
              Expr gam (index i gam)
       Zero : Expr gam Z
\end{code}
For an expression $e\ :\ Expr\ \Gamma\ x$, we will say that "$e$ denotes (or encodes) the number $x$ in the context $\Gamma$".
When an expression is a variable, the denoted number is simply the corresponding variable in the context, ie, $(index\ i\ \Gamma)$.
Also, the $Zero$ expression denotes the natural number $Z$.
Finally, if $e1$ is an expression encoding the number $x$, and $e2$ is an expression encoding the number $y$, then the expression $Plus\ e1\ e2$ denotes the number $(x + y)$.

\subsection{A correct by construction approach}

The reduction procedure is now supposed to be written on a "correct by construction" way, which means that no additional proof should be required after the definition of the function. Thus, $reduce$ will produce the proof that the new Expr produced has the same interpretation as the original Expr, and this will be made easier by the fact that the datatype Expr is now indexed over the real --concrete-- number : a term of type $Expr\ \Gamma\ x$ is the encoding of the number $x$.
Thus, we can write the type of $reduce$ like this : \\
$reduce\ :\ Expr\ \Gamma\ x\ \rightarrow\ (x'\ **\ (Expr\ \Gamma\ x',\ x\ =\ x'))$ \\
The function $reduce$ produces a dependent pair : the new concrete number $x'$, and a pair made of an $Expr\ \Gamma\ x'$ which is the new encoded term indexed over the new concrete number we have just produced, and a proof that old and new --concrete-- numbers are equal.
Note that this function can't simply produce an $Expr\ \Gamma\ x$, because the number on which the resulting expression will be indexed is not necessary syntactically equal to the original number since this equality can use the two available properties. 
And in fact, what really interest us in this function is precisely the proof of $x\ =\ x'$.
The reason is simple : when we try to automatically prove $x=y$, these proofs $x=x'$ and $y=y'$ will be the crucial part for the construction of the desired proof. \\
\\
We will have an expression $e1$ encoding $x$, and an expression $e2$ encoding $y$\footnote{These encodings have to be produced by hand by the user for this small simple problem, but for the real collection of tactics, we will program an automatic metaification}.
We will normalize $e1$, and this will give a new number $x'$, a new expression $e1':Expr\ \Gamma\ x'$, and a proof of $x=x'$. We will do the same with $e2$, and we will get a new number $y$, an expression $e2':Expr\ \Gamma\ y'$, and a proof of $y=y'$. \\
Now, we can compare $e1'$ and $e2'$ using a standard syntactical equality because these two expressions are in normal form :

\begin{code}[caption=Syntactical equality between reflected terms, captionpos=b, label=lst1:haskell2]
eqExpr : (e : Expr gam x) -> (e' : Expr gam y) 
         -> Maybe (e = e')
eqExpr (Plus x y) (Plus x' y') 
        with (eqExpr x x', eqExpr y y')
  eqExpr (Plus x y) (Plus x y) 
        | (Just refl, Just refl) = Just refl
  eqExpr (Plus x y) (Plus x' y') 
        | _ = Nothing
eqExpr (Var i) (Var j) with (decEq i j)
  eqExpr (Var i) (Var i) 
        | (Yes refl) = Just refl
  eqExpr (Var i) (Var j) 
        | _ = Nothing
eqExpr Zero Zero = Just refl
eqExpr _ _ = Nothing
\end{code}


Now, if the two normalised expressions $e1'$ and $e2'$ are equal, then they necessary have the same type\footnote{We are working with the heterogeneous equality JMeq, but as always, the only way to have a proof of a:A = b:B is when A=B}, and therefore $x'=y'$.
By rewriting the two equalities $x=x'$ and $y=y'$ (that we obtained during the normalisations) in the new equality $x'=y'$, we can get a proof of $x=y$.

\begin{code}[caption=Building the desired proof with the two proofs of equality, captionpos=b, label=lst1:haskell2]
buildProof : {x : Nat} -> {y : Nat} 
       -> Expr gam x' -> Expr gam y' 
       -> (x = x') -> (y = y') 
       -> Maybe (x = y)
buildProof e e' lp rp with (eqExpr e e')
  buildProof e e lp rp | Just refl = ?bp1
  buildProof e e' lp rp | Nothing = Nothing
\end{code}

As we mentioned, the proof for the metavariable is just a rewriting of the two equalities :

\begin{code}[caption=buildProof metavariable, captionpos=b, label=lst1:haskell2]
  bp1 = proof {
  intros;
  refine Just;
  rewrite sym p1;
  rewrite sym p2;
  exact refl;
}  
\end{code}
Finally, the main function which tries to prove the equality $x=y$ simply has to reduce the two metaified terms reflecting the left and the right hand side, and to use the function $buildProof$ in order to compose the two proofs we just obtained :
\begin{code}[caption=testEq, captionpos=b, label=lst1:haskell2]
  testEq : Expr gam x -> Expr gam y 
           -> Maybe (x = y)
  testEq l r = 
     let (x' ** (l', p1)) = reduce l in 
     let (y' ** (r', p2)) = reduce r in
        buildProof l' r' p1 p2
\end{code}
Now, we need to define the function reduce. To do that, we have to fix a canonical representation of associative natural numbers. We decide that the left associative form will be the canonical representation. Thus, the $reduce$ function will have to rewrite the metaified term by rearranging the parentheses in order to transform the underlying number in the form $(...((x1 + x2) + x3) ... + xn)$. To do so, one possibility is to define a new datatype which captures this property, and to write a function going from $Expr$ to this new type. Thus it will be easier to be certain that we are effectively computing the normal form : forcing properties to hold by the shape of a datatype is a good usage of dependent types when, like here, it doesn't introduce more complications.
\begin{code}[caption=Reflected left associative numbers, captionpos=b, label=lst1:haskell2]
data LExpr : (gam : Vect n Nat) -> Nat 
             -> Type where
     LPlus : LExpr gam x -> (i : Fin n) 
             -> LExpr gam (x + index i gam)
     LZero : LExpr gam Z
\end{code}
This datatype has only two constructors. In fact, it combines the previous $Var$ and $Plus$ constructors so that it becomes impossible to write an expression which isn't left associative.
 
As part of the normalization, we will write a function $expr\_l$ which converts an $Expr\ \Gamma\ x$ to a $LExpr\ \Gamma\ x'$ and which produces a proof of $x=x'$. This function will therefore use the two available properties multiple times --and especially the property of associativity--, in order to obtain the expected fully left associative form. Of course, because by using this new datatype $LExpr$ we've changed the representation of our encoded lists, we will need to convert back an $LExpr\ \Gamma\ x$ to an $Expr\ \Gamma\ x$. The function $l\_expr$ will do this easy task.

\begin{code}[caption=Production of the left associative form, captionpos=b, label=lst1:haskell2]
expr_l : Expr gam x 
         -> (x' ** (LExpr gam x', x = x'))
expr_l Zero = (_ ** (LZero, refl))
expr_l (Var i) = (_ ** (LPlus LZero i, refl))
expr_l (Plus ex ey) = 
  let (xl ** (xr, xprf)) = expr_l ex in
  let (yl ** (yr, yprf)) = expr_l ey in
    plusLExpr _ _ xr yr xprf yprf
      where 
      plusLExpr : (x', y' : Nat)
            -> {gam : Vect n Nat} -> {x, y : Nat} 
            -> LExpr gam x -> LExpr gam y 
            -> (x' = x) -> (y' = y) 
            -> (w' ** (LExpr gam w', x'+y'=w'))
      plusLExpr x' y' rx (LPlus e i) xprf yprf =
        let (xrec ** (rec, prf)) = 
          plusLExpr _ _ rx e refl refl in
          (_ ** (LPlus rec i, ?plusLExpr1))
      plusLExpr x' y' rx LZero xprf yprf =
        (_ ** (rx, ?plusLExpr2))

l_expr : LExpr gam x -> Expr gam x
l_expr LZero = Zero
l_expr (LPlus x i) = Plus (l_expr x) (Var i)
\end{code}

We notice that for transforming the expression into its left associative equivalent representation, we've effectively needed to know where the variables and the $Z$ constants are : the functions $expr\_l$ and $l\_expr$ are doing different treatments for these different possibilities. \\
\\
We've got two metavariables to prove. The metavariable $plusLExpr1$ requires us to prove the goal : $x' + y' = xrec + index\ i\ \Gamma$ in a context where we've got, amongst other things,  $(xprf\ :\ x'\ =\ x)$, $(yprf\ :\ y'\ =\ x1\ +\ index\ i\ \Gamma)$ and $(prf\ :\ x\ +\ x1\ =\ xrec)$.
Proving this goal uses the property of associativity after rewriting the goal with these three proof of equality $xprf$, $yprf$ and $prf$.

\begin{code}[caption=Proof of the metavariable plusLExpr1, captionpos=b, label=lst1:haskell2]
plusLExpr1 = proof {
  intros;
  rewrite sym xprf;
  rewrite sym yprf;
  rewrite prf;
  mrefine plusAssociative;
}
\end{code}

And the metavariable plusLExpr2 uses the fact that $Z$ is a neutral element for the addition.

\begin{code}[caption=Proof of the metavariable plusLExpr2, captionpos=b, label=lst1:haskell2]
plusLExpr2 = proof
  intros
  rewrite xprf 
  rewrite (sym yprf)
  mrefine a_plus_zero
\end{code}

We can now define the reduction, which is just the composition of the two previous functions $expr\_l$ and $l\_expr$:

\begin{code}[caption=Reduction function, captionpos=b, label=lst1:haskell2]
  reduce : Expr gam x -> 
           (x' ** (Expr gam x', x = x'))
  reduce e = 
     let (x' ** (e', prf)) = expr_l e in
         (x' ** (l_expr e', prf))
\end{code}

At the moment, what we've got is not exactly a real tactic, in the sense that we only have a function which produces a value of type $Maybe (x = y)$. A real tactic would be a coating of this function, which could properly fail when the two terms are not equal. However, here, when $x\ne y$, the function $testEq$ will simply produce the value $Nothing$. \\

\subsection{Usage of the "tactic"}

It's now time to see how to use this minimalist "tactic".
Let's define two expressions $e1$ and $e2$, respectively representing the numbers $((x + y) + (x + z))$ and $(x + ((y + x) + z))$ in the context $[x, y, z]$ of three abstracted variables.

\begin{code}[caption=Two test expressions, captionpos=b, label=lst1:haskell2]
e1 : (x, y, z : Nat) 
    -> Expr [x, y, z] ((x+y) + (x+z))
e1 x y z = Plus (Plus (Var fZ) 
                      (Var (fS fZ))) 
                (Plus (Var fZ) 
                      (Var (fS (fS fZ))))

e2 : (x, y, z : Nat) 
     -> Expr [x, y, z] (x + ((y + x) + z))
e2 x y z = Plus (Var fZ) 
                (Plus (Plus (Var (fS fZ)) 
                            (Var fZ)) 
                      (Var (fS (fS fZ))))
\end{code}

The numbers denoted by the expressions $e1$ and $e2$ are equal, and we can generate a proof of this by using $testEq$.
\begin{code}[caption=Test of equality betwen e1 and e2, captionpos=b, label=lst1:haskell2]
e1_e2_testEq : (x, y, z : Nat) 
       -> Maybe (((x + y) + (x + z)) 
                  = (x + ((y + x) + z)))
e1_e2_testEq x y z = testEq (e1 x y z)
                            (e2 x y z)
\end{code}

And if we ask for the evaluation of this term, we should obtain $Just$ and a proof of equality between the two underlying numbers.
\begin{code}[caption=Obtained proof, captionpos=b, label=lst1:haskell2]
#\x => \y => \z => e1_e2_testEq x y z

\x => \y => \z => Just (replace (sym (replace 
  (sym (replace (plusZeroRightNeutral x) 
  (plusAssociative x 0 y))) (replace 
  (sym (replace (plusZeroRightNeutral x) 
  (plusAssociative x 0 z))) (replace (replace 
  (plusZeroRightNeutral (plus xy)) 
  (plusAssociative (plus x y) 0 x)) [...]
: (x : Nat) -> (y : Nat) -> (z : Nat) 
  -> Maybe ((x + y) + x + z 
            = x+ (y + x) + z)
\end{code}

And we effectively get the proof of equality we wanted. As expected, this proof uses the properties of associativity ($plusAssociative$) and the property of neutrality of $Z$ for $plus$ ($plusZeroRightNeutral$).

\section {Back to the general problem : A hierarchy of tactics (2/3 pages)}

Now that we have introduced our ideas on this little example, it is time to apply them for our main goal, more general : the implementation of a hierarchy of tactics proving equalities in algebraic structures. Very often, the properties available on a given type are the one of a well known structure : magma, semi-group, monoid, group, commutative-group...  We will no longer only work with the properties of associativity and of right neutral element for natural numbers as we did in the previous section, but we will have available the properties of different structures, and these tactics will usable for any type that satisfies these properties.

We will construct a hierarchy of typeclasses. All our tactics (the group solver, the monoid solver...) will be able to work on any type, being given that an instance of the corresponding typeclass is provided.

\subsection {Hierarchy of typeclass}

All our tactics will require to have a way of testing the equality between elements of the underlying set, that is to say, a way to test equality between constants. For this reason, we define a notion of "set", which only requires this equality. All the algebraic structures above will extend this typeclass :

\begin{code}[caption=Set, captionpos=b, label=lst1:haskell2]
class Set c where
    set_eq : (x:c) -> (y:c) -> Maybe (x=y)
\end{code}

Note that this equality is a "week" equality : it only produces a proof when the two elements are equal, but it doesn't produce a proof of dis-equality when they are different -- instead, it simply produces the value $Nothing$--. That's quite natural, since we want to generate proof of equality, and not to generate counter examples for proving dis-equalities, which is another problem.

Of course, there will be no tactic associated to $Set$, since we have no operations and not properties associated to this structure. Therefore, the equality in a $Set$ is just the syntactical equality, and can simply be proven with $refl$.

The first real structure, almost trivial, is the magma. A magma has just a $Plus$ operation which let us computing constants, and has no axioms about this operation. Thus, as we will see later, the normalization will be simply a computation between constants inside parenthesis.

\begin{code}[caption=Magma, captionpos=b, label=lst1:haskell2]   
class Set c => Magma c where
    Plus : c -> c -> c
\end{code}

A bit more interesting is the Semi-Group typeclass. A semi-Group is a Magma (ie, it still has a $plus$ operation), but moreover it has the property of associativity for this operation.

\begin{code}[caption=Semi-Group, captionpos=b, label=lst1:haskell2]   
class Magma c => SemiGroup c where
    Plus_assoc : (c1:c) -> (c2:c) -> (c3:c) 
           -> (Plus (Plus c1 c2) c3 
                = Plus c1 (Plus c2 c3))
\end{code}

The Monoid, for its part, is a Semi-Group with the property of neutral element.

\begin{code}[caption=Monoid, captionpos=b, label=lst1:haskell2]   
class SemiGroup c => Monoid c where
    Zero : c    
    Plus_neutral_1 : (c1:c) 
            -> (Plus Zero c1 = c1)    
    Plus_neutral_2 : (c1:c) 
             -> (Plus c1 Zero = c1)
\end{code}	

A Group is a Monoid, with two new operations. The binary operation $Minus$, and the unary operation $Neg$. One axiom says that the $Minus$ can always be simplified with the $Neg$ and the $Plus$. That means that $Minus$ is not a "primitive" operation of a Group, since we can always rewrite $(a-b)$ into $(a\ +\ -b)$.
The second axiom, which is the most important one for a Group, is the fact that any value $c1$ admits $Neg\ c1$ for inverse, where being an inverse is the following property :

\begin{code}[caption=Group, captionpos=b, label=lst1:haskell2]   
-- This is just a conjunctive predicate
hasSymmetric : (c:Type) -> (p:Monoid c) 
                -> c -> c -> Type
hasSymmetric c p a b = 
         (Plus a b = Zero, Plus b a = Zero)    
  
class Monoid c => Group c where
	Minus : c -> c -> c
	Neg : c -> c
	Minus_simpl : (c1:c) -> (c2:c) -> 
	             Minus c1 c2 = Plus c1 (Neg c2) 
	Plus_inverse : (c1:c) -> 
	             hasSymmetric c _ c1 (Neg c1)
\end{code}	

This hierarchy can be extended without difficulty with Abelian groups (with the axiom of commutativity), Ring, Commutative Rings...

The important thing to remember is that these typeclass will somehow be used as predicates on types. In order to call a solver on a type $T$, the user of the system will have to satisfy the corresponding typeclass by providing an instance of it for his type $T$, ie, he will have to prove that the corresponding properties effectively hold for $T$.

Note that the properties required by these typeclass for a specific type $T$ will either be proven if the operations on $T$ are real computable definitions, or axiomatised if the user is working on an axiomatised theory where the operations ($Plus$, $Neg$) are just axioms.

As discussed in the section 2.1 on the smaller problem, the algorithm of normalization will not directly use the concrete value of type $T$, but a reflected term, indexed over the concrete value.


	\subsection {Reflected terms}

We will have one datatype for reflecting terms in each algebraic structure. \\
Each of these datatype is parametrised over a type $T$, which is the real type on which we want to prove equalities. It is also indexed over an instance of the corresponding typeclass for $T$ (we usually call it $p$, because it looks like a proof saying that the structure $T$ has the desired properties), and indexed over a context (a vector $\Gamma$ of $n$ elements of type $T$), and also indexed over a value of type $T$, which is precisely the encoded value.
A magma is only equipped with one operation $Plus$. Thus, we only have three concepts to express in order to reflect terms in a Magma : constants, variables, and addition.

\begin{code}[caption=Reflected terms in a Magma, captionpos=b, label=lst1:haskell2]  
data ExprMa : Magma T -> (Vect n T) -> T -> 
              Type where
    ConstMa : (p : Magma T) -> (gam:Vect n T) 
         -> (c1:T)  -> ExprMa p gam c1 
    PlusMa : {p : Magma c} -> {gam:Vect n c}
         -> {c1:T} -> {c2:T} 
         -> ExprMa p gam c1 
         -> ExprMa p gam c2 
         -> ExprMa p gam (Plus c1 c2) 
    VarMa : (p:Magma T) -> {gam:Vect n T}
         -> {c1:T} 
         -> Variable (get_eq_magma p) gam c1
         -> ExprMa p gam c1
\end{code}	

When we encode a constant $c1$ in a context $\Gamma$, we use the constructor $ConstMa$ to produce a term of type $ExprMa\ p\ \Gamma\ c1$ : the index representing the concrete value is simply the constant $c1$.
If e1 is an expression of type $ExprMa\ p\ \Gamma\ c1$ (ie, a term encoding the value $c1$), and $e2$ is an expression of type $ExprMa\ p\ \Gamma\ c2$ (a term encoding the value $c2$), then the term $PlusMa\ e1\ e2$ will have the type $ExprMa\ p\ \Gamma\ (Plus\ c1\ c2)$, ie, this term will encode the value $(Plus\ c1\ c2)$, where $Plus$ is the operation defined in the current instance $p$.


We've used $get\_eq\_magma$ in the constructor $VarMa$, and this is just a function which returns the equality test of the $Set$ level.
For the moment, the definition of $Variable$ is just the following:

\begin{code}[caption=Reflected variables, captionpos=b, label=lst1:haskell2]  
data Variable : {c:Type} -> {n:Nat}
  -> (c_equal : (c1:c)->(c2:c)->Maybe(c1=c2)) 
  -> (Vect n c) -> c -> Type where
    RealVariable : (c_equal:(c1:c)->(c2:c)
                    ->Maybe(c1=c2))
          -> (gam:Vect n c) -> (i:Fin n) 
          -> Variable c_equal gam (index i gam) 
\end{code}	

This type will be extended with some other constructors later on. For the moment, we can only express a "real variable", referred by its index $i$. \\
\\
Because there is no more operations in a SemiGroup or in a Monoid, the datatypes for reflected terms in these two structures will have exactly the same shape as the one for Magma that we've given above.
However, the one for $Group$ will introduce two new constructors for the $Neg$ and $Minus$ operations :

\begin{code}[caption=Reflected terms in a Group, captionpos=b, label=lst1:haskell2]
data ExprG :  Group c -> (Vect n c) -> c -> 
              Type where
    ConstG : (p : dataTypes.Group c) 
      -> (gam:Vect n c)
      -> (c1:c) -> ExprG p gam c1
    PlusG : {p : Group c} -> {gam:Vect n c} 
      -> {c1:c} -> {c2:c} 
      -> ExprG p gam c1 
      -> ExprG p gam c2 
      -> ExprG p gam (Plus c1 c2)
    MinusG : {p : Group c} -> {gam:Vect n c} 
      -> {c1:c} -> {c2:c} 
      -> ExprG p gam c1 
      -> ExprG p gam c2 
      -> ExprG p gam (Minus c1 c2)
    NegG : {p : Group c} -> {gam:Vect n c} 
      -> {c1:c} -> ExprG p gam c1 
      -> ExprG p gam (Neg c1)
    VarG : (p : Group c) -> {gam:Vect n c} 
      -> {c1:c}
     -> Variable (get_eq_group p) Neg gam c1 
      -> ExprG p gam c1
\end{code}

Of course, the index of type $T$ (the real value represented by the expression) is always expressed with the operations available in the typeclass $p$, and here for a Group they are$Plus$, $Minus$ and $Neg$.

	\subsection { A bit of notation}
The equality we are trying to prove is $x=y$, where $x$ and $y$ are elements of the type $T$, which  simulates a set with some properties (which make $T$ being a Semi-Group, a Monoid, a Group...). The fact that $T$ fulfil the specification of an algebraic structure is expressed as an instance of the corresponding typeclass, and this instance will be denoted as $p$.
The reflected term for $x$ will be denoted $e1$, and this term will have the type $ExprG\ p\ \Gamma\ x$. $e1$ is the encoding of $x$ and its type is precisely indexed over the real value $x$.
We've got similar thing for $y$, which is encoded by $e2$, and its type is indexed over the real value $y$.
Running the normalisation procedure on $e1$ will produce the normal form $e1'$ of type $ExprG\ p\ \Gamma\ x'$ and a proof $p1$ of $x=x'$ while running the normalisation procedure on $e2$ will produce the normal form $e2'$ of type $ExprG\ p\ \Gamma\ y'$ and a proof $p2$ of $y=y'$.

	\subsection {Deciding equality}
	
Each algebraic structure will have a function for reducing the reflected terms into their normal forms. This is fact a nice property that all these algebraic structures admit a canonical represent for any element. Without this property, it would become a lot more complicated to decide the equality without bruteforcing a serie of rewriting.
These functions will compute this canonical represent, by rewriting the given term multiple times, and at the same time, it will produce the proof of equality between the real value indexing the original term and the real value indexing the produced term.

The profile of these function will always be the same :
\begin{code}[caption=Type of the reduction function for terms reflecting elements in a Group, captionpos=b, label=lst1:haskell2]
	groupReduce : {c:Type} -> {n:Nat} 
	  -> (p:Group c) -> {gam:Vect n c} -> {x:c}
	  -> (ExprG p gam x) 
	  -> (x' ** (ExprG p gam x', x=x'))
\end{code}

This function will have more job to do in structures with multiple axioms (like Group), than it has to do in the easier structure underneath.
The detail of these functions will be given in the next section. For the moment, we just assume that we've got such a function for each structure, and we want to decide equality between elements. We continue to give here the details for the Group structure, but the principle is exactly the same with the other structures.

We want to write the following function :
\begin{code}[caption=Type of the function for deciding equality between elements in a Group, captionpos=b, label=lst1:haskell2]
	groupDecideEq : (p:Group c) 
	   -> {gam:Vect n c} -> {x : c} -> {y : c} 
	   -> (ExprG p gam x) -> (ExprG p gam y) 
	   -> Maybe (x = y)
\end{code}

The first thing this function has to do is to compute the normal form of the two expressions $e1$ and $e2$ respectively encoding $x$ and $y$.
Then, the only thing remaining to do will be to compare syntactically $e1'$ and $e2'$, and if they are equal, then we will get $x'=y'$, and we will be able to produce the desired proof of $x=y$ by using the two proofs $p1$ and $p2$. 

\begin{code}[caption=Decides if two elements of a group are equal, captionpos=b, label=lst1:haskell2]
groupDecideEq p e1 e2 =
  let (x' ** (e1', p1)) = 
    groupReduce p e1 in
  let (y' ** (e2', p2)) = 
    groupReduce p e2 in
	     buildProofGroup p e1' e2' p1 p2
\end{code}

The syntactical test of equality between $e1'$ and $e2'$ and the composition of the two proofs is done in the auxiliary function $buildProofGroup$. 

\begin{code}[caption=Composes the two proofs if the NF are the same, captionpos=b, label=lst1:haskell2]
buildProofGroup : (p:Group c) -> {gam:Vect n c} 
  -> {x : c} -> {y : c} -> {x':c} -> {y':c} 
  -> (ExprG p gam x') -> (ExprG p gam y') 
  -> (x = x') -> (y = y') 
  -> (Maybe (x = y))
buildProofGroup p e1 e2 p1 p2 
       with (exprG_eq p _ e1 e2)
	buildProofGroup p e1 e1 p1 p2 | Just refl = 
	         ?MbuildProofGroup
	buildProofGroup p e1 e2 p1 p2 | Nothing =
	          Nothing
\end{code}

The proof for the metavariable MbuildProofGroup is exactly the same as the corresponding one (called $bp$) in the smallest example showed in section 2.2.


\section {The details : normalization functions (4/5 pages)}

	\subsection {Normalization of terms in a Magma}

The normalization in a Magma nearly has nothing to do. It simply need to use the operation $Plus$ between constants under brackets. Indeed, we don't have any property of rewriting here, so we can only compute constants.

\begin{code}[caption=Normalization function for Magma, captionpos=b, label=lst1:haskell2]
magmaReduce : {c:Type} -> {n:Nat} 
              -> {p:Magma c} -> {gam:Vect n c} 
              -> {c1:c} -> (ExprMa p gam c1) 
              -> (c2 ** (ExprMa p gam c2, c1=c2))
magmaReduce (PlusMa
      (ConstMa p _ const1) 
      (ConstMa p _ const2)) = 
         (_ ** (ConstMa p g 
               (Plus const1 const2), refl))
magmaReduce (PlusMa neg e1 e2) = 
    let (r_ih1 ** (e_ih1, p_ih1)) 
        = (magmaReduce e1) in
    let (r_ih2 ** (e_ih2, p_ih2)) 
        = (magmaReduce e2) in
    ((Plus r_ih1 r_ih2) ** 
        (PlusMa neg e_ih1 e_ih2, ?MmagmaReduce1))                    
magmaReduce e = (_ ** (e, refl))
\end{code}

This function returns the same value for constant or for a variable. For a $Plus$, we apply the treatment recursively, except if it's a $Plus$ between two constants, in which case we compute the addition of these two constants.


	\subsection {Normalization of terms in a Semi-Group}

In a semigroup, we have to deal with the new property of associativity. We have to use the $Plus$ on constants computable thanks to associativity. Three possible patterns need this treatment : $(x+c1)+(c2+y)$ , $(x + c1) + c2$  and $c1 + (c2 + x)$ when $c1$ and $c2$ denote constants. We simply rearrange the parentheses, and call the normalization procedure for Magma on the constants, which is denoted [c1+c2].
The first pattern is rewritten into $(x + [c1+c2]) + y$, the second into $x + [c1+c2]$ and the third one into $[c1 + c2] + x$.
We show the code for the first pattern.
\begin{code}[caption=Computing with associativity in a Semi-Group, captionpos=b, label=lst1:haskell2]
assoc : (p:SemiGroup c) -> (gam:Vect n c) 
        -> {c1:c} -> (ExprSG p gam c1) 
        -> (c2 ** (ExprSG p gam c2, c1=c2))
assoc p gam (PlusSG 
             (PlusSG e1 
                        (ConstSG _ _ c1)) 
             (PlusSG (ConstSG _ _ c2) e2)) =
    let (r_ih1 ** (e_ih1, p_ih1)) 
       = (assoc p gam e1) in
    let (r_ih2 ** (e_ih2, p_ih2)) 
       = (assoc p gam e2) in
    let (r_3 ** (e_3, p_3)) 
       = magmaReduce (semiGroup_to_magma 
            (PlusSG (ConstSG _ _ c1) 
            (ConstSG _ _ c2))) in
    let e_3' = magma_to_semiGroup p e_3 in
    (_ ** ((PlusSG (PlusSG e_ih1 e_3') e_ih2), 
          ?Massoc1))
[...]
\end{code}

Note that we use function of conversions between the different levels : $semiGroup\_to\_magma$ and $magma\_to\_semiGroup$.

After simplifying the constants computable thanks to associativity, the normalization for SemiGroup also has to rearrange the brackets on a systematic way. The fully right or fully left associative forms can both be chosen for this purpose, and the implementation is very similar to the function $expr_l$ previously described on section 2.2.

	\subsection {Normalization of terms in a Monoid}

In a monoid, we simply have to eliminate zeros, thanks to the two properties of left and right neutrality. Because zeros will be computed with constants automatically at the level underneath, we only have to simplify the addition between the constant zero and a variable. There is in fact two cases, since the variable can come first, or the $Zero$ can come first.

\begin{code}[caption=Eliminating zeros in a Monoid, captionpos=b, label=lst1:haskell2]
elimZero : (p:Monoid c)->(gam:Vect n c)->{c1:c} 
            -> (ExprMo p gam c1) 
            -> (c2 ** (ExprMo p gam c2, c1=c2))
elimZero c p gam 
 (PlusMo (ConstMo _ _ const1) (VarMo _ v)) 
        with (eq_on_set p Zero const1)
   elimZero c p gam (PlusMo (ConstMo _ _ Zero) 
                 (VarMo _ v)) | (Just refl) 
          = (_ ** (VarMo _ v, ?MelimZero1))
   elimZero c p gam (PlusMo (ConstMo _ _ const1) 
                 (VarMo _ v)) | _ 
          = (_ ** (PlusMo (ConstMo _ _ const1) 
               (VarMo _ v), refl)) 
elimZero c p gam 
 (PlusMo (VarMo _ v) (ConstMo _ _ const2)) 
      with (eq_as_elem_of_set p Zero const2) 
   elimZero c p gam (PlusMo (VarMo _ v) 
            (ConstMo _ _ Zero)) | (Just refl) 
          = (_ ** (VarMo _ _ v, ?MelimZero2))
   elimZero c p gam (PlusMo (VarMo _ v) 
            (ConstMo _ _ const2)) | _ 
          = (_ ** (PlusMo (VarMo _ v) 
                 (ConstMo _ _ const2), refl))
[...]
\end{code}

Now the normalization of reflected expressions in a monoid is simply made of a call to the function reducing terms on a semigroup, followed by a call to elimZero which will eliminate the remaining zeros. 

	\subsection {Normalization of terms in a Group}

TO BE WRITTEN

\section{Completeness}

Mentionning that we hope to be complete, and giving intuitive arguments briefly, but explaining that this can't be proven inside the language.



\section {Auxiliary problems : Automatic metaification and coating (a bit less than a page!)}

	\subsection {Automatic metaification}

How to compute automatically the metaified expression, by using the reflection stuff (0,5 page)

	\subsection {Coating : transforming this stuff into real tactics}

What we have to add to Idris in order to invoke this stuff as a "real tactic" (which can fail). (0,25 page)


\section {Related work and comparison with other work(less than 1 page)}

TO BE CONTINUED

There exists implementation of ring solver for the Coq proof assistant and for Agda. The first one, implemented by Benjamin Grégoire and Assia Mahboubi and described in their paper "Proving equalities in a commutative ring done right in coq", is also implemented by reflection.
The main difference is that they write a function of normalization for their reflected terms, and latter on prove that this normalization preserves the interpretation, ie, the concrete value reflected.
This approach is quite normal for the Coq proof assistant, since it does not have a good support for dependent types. For example, there is no dependent pattern matching in Coq. However, in Idris is designed as a programming language with full support for dependent types, and that's why the temptation was high to use this good support in order to write correct by construction tactics.
Also, in most implementations, only one algebraic level is treated. In the best case, they deal with rings and commutative rings. But is a given structure only has the properties of a group, their tactic is not usable. One could of course write a solver for group, but it will have a big part similar to the ring solver, but this part will have to be duplicated.
However, in our work, each solver uses the solver of the algebraic structures underneath, increasing drastically the re-usability of code.

\section {Conlusion and futher work (0,5 page)}
TO BE CONTINUED

We've constructed a correct by construction hierarchy of tactics. This hierarchy is going to be extended  with structures like ring and commutative ring. The main contribution of our work was to intensively use dependent types in order to capture the concrete value reflected by an expression. This enabled us to write the implementation of the tactics on a "correct by construction" style, where the proof is almost trivial and computed at the same time as the result.
It is true that dependent types often bring more complexity, and for example, the lemmas that we have to proven as soon as one is playing with dependent types. Pleasantly, if the problem comes with dependent types, the solution is also accompanied with the use dependent types which helped us to write a collection of tactics without pain.


\acks

Acknowledgments : Chris, Mattus for discussions. Jan for help in latex.

% We recommend abbrvnat bibliography style.

\bibliography{biblio}

\appendix
\section{Appendix : A little overview of Idris}

\input{./content/idrisOverview.tex}


\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

