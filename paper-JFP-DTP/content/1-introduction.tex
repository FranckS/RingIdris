\section{Introduction}

Proving that one term is equal to another within some theories is quite common
in formal certification. The two terms are usually not syntactically 
equal---this case would be trivial---but they are equal relative to some
equivalence properties. These properties are either obtained by lemmas
(the implementation of $+$ for natural numbers satisfies the property of
associativity for example), or by axioms.

For example, when working with natural numbers, one might need to prove the
lemma $\forall\ x, y:Nat,\ x*x\ +\ 3*x\ +\ x*y\ =\ x*(3+y+x)$. Or,
when working
with lists and concatenation of lists, one might need to prove the lemma
$\forall\ l1,\ l2,\ l3:List A,\ ((l1\ ++\ (l2 ++ l3))\ ++\ l4\ =\ (l1++l2)\ ++\
(l3++l4)$. Proving this kind of auxiliary lemmas is time consuming,
typically not reusable due to being very specific, and is rarely
the interesting part of formal
certification. Therefore, we would like to be able to automatize as far as
possible the constructions of this kind of proofs. We have implemented
the machinery to do so in Idris, a
dependently typed language~\cite{brady2013idris}.

When doing formal certification, we often require equality proofs between
terms.
For example, say we want to certify the function $f\ :\ A \rightarrow
B$. We have two functions $sem_A\ :\ A \rightarrow SEM$ and $sem_B\ : \ B
\rightarrow SEM$ which associate a semantics to each type A
and B. If this semantics describes all of the important properties of
the behaviour of $f$,
then the lemma of correctness of $f$ can be expressed as $forall\ a:A,\ semA\ a =
semB (f\ a)$. Very often, the type $SEM$ has some algebraic
algebraic structure, and it becomes thus possible to
decide equalities in it by following some standard rewriting rules.  
All the equality symbols we have
encountered so far refer to the propositional equality. In the next section,
we will see how this is defined in Idris, and how it differs from the more
primitive judgemental equality.

\subsection{Propositional and Judgemental Equalities in Intentional Type Theories}
In Mathematics, equality is a proposition, which can hold or otherwise, or be disproved.
Since in type theory, propositions are seen as types~\cite{How80}, the
proposition that two elements $x$ and $y$ are equal corresponds to a type.
Thus, if $x$ and $y$ are of type $a$, then the type
$Id_a(x, y)$ represents the proposition "$x$ is equal to $y$". If this type is
inhabited, then $x$ is said to be provably equal to $y$.  Thus, $Id$ is a type
family (parametrised by the type $a$) indexed over two elements of $a$, giving
$Id (a:Type)\ :\ a \rightarrow a \rightarrow Type$. For
convenience, we write $Id_a\ x\ y$ as $x\ =_a\ y$.  This equality is the
equality which can be manipulated in the language. 

There is another more
primitive notion of equality in Intentional Type Theories, called judgemental
equality, or definitional equality. This second equality means ``equal by
definition'' The judgemental equality cannot be negated or
assumed; we cannot talk about this primitive equality inside the theory.
Whether or not two expressions are equal by definition is a matter of
evaluating the definitions. For example, if $f\ :\ \mathbb{N}\ \rightarrow\
\mathbb{N}$ is defined by $f\ x\ \equiv\ x\ +\ 2$, then $f\ 5$ is
definitionally equal to $7$. Definitional equality entails unfolding
of functions and reductions, until no more reduction can be
performed. We denote definitional equality by $\equiv$.

The judgemental equality has to be included in the propositional equal, because
what is equal by definition should be provably equal.  This is accomplished by
giving constructors to the type $Id(a,a)$ and nothing when "$a$ is not $b$".
One way to implement $Id$ in Idris is:

\begin{lstlisting}
data Id : a -> a -> Type where
     Refl : (x : a) -> Id x x
\end{lstlisting}

The only way for $(Id_a\ x\ y)$ to be inhabited is therefore that $x$ should be
(by definition!) $y$. In this case, the constructor $Refl$ helps to create a
proof of this equality : $Refl\ x$ is precisely the proof which says that
$x=_ax$. 

When reading these definitions, one could think that the
propositional equality is a simple wrapper of the judgemental equality. This is
not the case: the propositional equality does not only contain the judgemental
equality, because in these type theories, a principle of induction is
associated with each inductive type. This induction principle says that if a
proposition holds for the base cases, and if it can be showed that when it
holds for some terms then it will also hold for the bigger terms obtained by
using the recursive constructors, then this proposition will hold for any term
of this type.

Induction principles work for any proposition, so it also works with the
propositional equality. That means that the axiom of induction principle has
made the type $Id(a,b)$ bigger than it looks like (in the definition 1), and
there are therefore things which are provably equal which unfortunately aren't
equal syntactically after an expanding of all the definitions. The activity of
proving equalities is therefore in these theories something which isn't
automatically decidable by the type-checker in the general case.

For example, we can prove that $n+0 = n$ by induction, even if $n+0 \not\equiv
n$ with the usual definition of $+$, recursive on its first argument. 

These proofs are usually omitted ``on paper'' for everyday mathematics, but
they are required by proof assistants, no matter how obvious they are to the
human reader.
Currently, in the Idris programming language, proofs must be completed
by hand, and consist
of a potentially long sequence of rewriting, where each step of
rewriting uses one of the available properties. These proofs can be
an everyday
routine when working in a dependently typed language, and especially
when intensively indexing types over values in order to capture certain logical
properties.  For example, If a type $T$ is indexed over natural numbers, then
the definition of a function which has codomain $T$ will often require a
proof that the index of the produced term is equal to the expected index. This
proof may use properties of natural numbers and their usual operations:
symmetry and commutativity for +, existence of neutral element for +, etc.

\subsection{Motivating example: Verified Binary Arithmetic}

We revisit an example from previous work~\cite{bradytfp07} 
which shows how proof obligations might appear
when we index a type by natural numbers.  We define the types of
$Bit$ and $Binary$ number in Figure~\ref{binarynums}, 
both indexed over the value they represent, expressed as a natural number.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
data Bit : Nat $\rightarrow$ Type where
     b0 : Bit Z
     b1 : Bit (S Z)
     
data Binary : (width : Nat) $\rightarrow$ (value : Nat) $\rightarrow$ Type where
     zero : Binary Z Z
     (#) : Binary w v $\rightarrow$ Bit bit $\rightarrow$ Binary (S w) (bit + 2 * v)
\end{lstlisting}
\end{center}
\caption{Bit and binary number}
\label{binarynums}
\figrule
\end{figure}

It allows representing a
binary number of width zero with the constructor $zero$, 
and representing larger numbers with $\#$.
We
can extend a binary number $bin$ by adding a digit to its right, and the value
represented by this new binary number is two times the value ($v$) represented
by $bin$ plus the value ($bit$) represented by the added digit.

To write a function performing the addition between two binary numbers,
we start with an auxiliary function (Figure~\ref{addbit}) which adds three bits
(the third one plays the role of a carry flag), and produces the two bits of
the result.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
addBit : Bit x $\rightarrow$ Bit y $\rightarrow$ Bit c $\rightarrow$ (bX ** (bY ** 
              (Bit bX, Bit bY, c + x + y = bY + 2 * bX)))
addBit b0 b0 b0 = (_ ** (_ ** (b0, b0, Refl)))
addBit b0 b0 b1 = (_ ** (_ ** (b0, b1, Refl)))
addBit b0 b1 b0 = (_ ** (_ ** (b0, b1, Refl)))
addBit b0 b1 b1 = (_ ** (_ ** (b1, b0, Refl)))
addBit b1 b0 b0 = (_ ** (_ ** (b0, b1, Refl)))
addBit b1 b0 b1 = (_ ** (_ ** (b1, b0, Refl)))
addBit b1 b1 b0 = (_ ** (_ ** (b1, b0, Refl)))
addBit b1 b1 b1 = (_ ** (_ ** (b1, b1, Refl)))
\end{lstlisting}
\end{center}
\caption{Addition of three bits}
\label{addbit}
\figrule
\end{figure}

This function produces a dependent pair. The first component is the value $bX$
(expressed as a natural number) represented by the first bit produced (of type
$Bit\ bX$), and the second component is another dependent pair. This
second dependent pair is composed of the value $bY$ represented by the second
bit produced, and of a triple containing the two bits produced and a
proof that these two bits correctly encode the result of the addition.
For example, with the line corresponding to the computation
$1_2 + 0_2 + 0_2 = (01)_2$, the function produces the bit $b0$ (encoding $0_2$)
and the bit $b1$ (encoding $1_2$), and a proof that $1 + 0 + 0 = 1 + (2*0)$.

It should be pointed that the proof component of this function is established without any effort, as we always give the "Refl" for the proof. Indeed, for each of the eight patterns, a simple computation reduces the given and expected values to the same value, and "Refl" can therefore be used. We also note that
there is no need to produce any lemma of correctness about $addBit$ afterwards.
The correct by construction style in which it is written already gives the
property that we want: the two bits produced effectively represent the
addition of the three bits given in input.

We finally define the function adding two binary numbers and a carry flag.
This addition works for two binary numbers
expressed with the same number of bits, and it produces a result with one more
bit. This result represents the value $c + x + y$, where $c$ 
representis the value of the input carry bit, and $x$ and $y$ represent
the values of the two binary numbers given in input. Thus, we'd like to
be able to write:

\begin{lstlisting}
adc : Binary w x $\rightarrow$ Binary w y $\rightarrow$ Bit c $\rightarrow$ Binary (S w) (c + x + y)
adc zero zero carry = zero # carry
adc (numx # bX) (numy # bY) carry
   = let (vCarry0 ** (vLsb ** (carry0, lsb, _))) = addBit bX bY carry in
          adc numx numy carry0 # lsb
\end{lstlisting}

Unfortunately, the types do not match for either of the two cases. 
The result of the first line $adc\ zero\ zero\ carry$ is
expected to have the type\[Binary\ 1\ (plus\ (plus\ c\ 0)\ 0)\] but we
provide a term of type \[Binary\ 1\ (plus\ c\ (plus\ 0\ (plus\ 0\ 0))).\]
The problem is even worse for the second case where the expected index is 
\[(plus\ (plus\ c\ (plus\ bit2\ (plus\ v1\ (plus\ v1\ 0))))\ (plus\ bit\ (plus\ v\ (plus\ v\ 0))))\] while we're trying to provide a term indexed over 
\[plus\ vLsb\ (plus\ (plus\ (plus\ vCarry0\ v1)\ v)\ (plus\ (plus\ (plus\ vCarry0\ v1)\ v)\ 0))\]

For
this situation, Idris provides \emph{provisional definitions} with the 
syntax \texttt{?=}. Using a provisional definition here will make
the definition acceptable, but it also generates two proof obligations, one
for each pattern, both of which say that the provided term can be
converted into a term which has the expected type.  For both of these
patterns, it is possible to prove the equality between the expected and
the provided types. These proofs
consist of a series of rewriting steps, where each step uses a property about the
operation $+$ on natural numbers.  For example, the proof for the second
pattern looks like this (with some detail elided):

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
adc_lemma_2 = proof {
    intros;
    rewrite sym (plusZeroRightNeutral x);
    [...]
    rewrite sym (plusAssociative (plus c (plus bit0 (plus v v))) 
                 bit1 (plus v1 v1));
    rewrite (plusAssociative c (plus bit0 (plus v v)) bit1);
    rewrite plusCommutative bit1 (plus v v);
    [...]
    rewrite (plusAssociative (plus (plus x v) v1) (plus x v) v1);
    trivial;
\end{lstlisting}
\end{center}
\caption{Proof by hand of the correspondence between expected and provided types for the second pattern of the definition of adc}
\figrule
\end{figure}


Thus, this kind of proof consists of a potentially long sequence of rewriting
steps,
where each step uses one of the available properties. Without some
specific automation, this sequence of rewriting must be done by the programmer.
This is time consuming, and perhaps more importantly, even a small
change in the left
or the right hand side of the equality may invalidate the proof.
This mean that a minor change in our
datatypes, or in the definition of $addBit$ or $adc$ will require us to
replace this proof by another one doing the same job.
This kind
of proof is therefore not reusable, because it performs rewritings for
very specific terms.

If we look at the proof we have written by hand, we realize that we only use
the existence of neutral elements (this is the lemma $plusZeroRightNeutral$),
and the associativity and commutativity of $+$. Thus, we're rewriting a term by
using the properties of a commutative monoid.

We would like to let a generic prover for commutative monoid find a proof on its own.
This should be achievable, because algebraic structures like monoids, groups and
rings (commutative or otherwise) have a very useful property, that
every expression can be normalised to a canonical representation. Thus
these kind of proofs can be automatically produced by computing the
normal forms for each side of the equality, and then comparing them using
syntactical equality. When they are in normal
form, being equal is a matter of being syntactically the same entity.

\subsection{Our contributions}

Ring solvers are already implemented for various proof
assistants, including Coq~\cite{Coq2005} and Agda\footnote{\url{http://wiki.portal.chalmers.se/agda/\%22?n=Libraries.UsingTheRingSolver}}. 
In this paper, we describe a certified
implementation of an automatic prover for equalities in a
\emph{hierarchy} of algebraic
structures, including Monoid, Groups and Rings (all potentially commutative),
for the Idris language. 

The principal novelty is in the approach that we follow, using a
new kind of \emph{type-safe reflection}.  Working by reflection for implementing
tactics has been done several times, for the implementation of a
ring solver for the Coq proof assistant, but without
providing any guarantees, contrary to our type-safe reflection. We will
compare our approach with other implementations in Section~\ref{sect:discuss}.

We make the following contributions:

\begin{itemize}
        \item We present a new type-safe reflection mechanism, where the
        reflected terms are indexed over the concrete inputs, thus providing a
        direct way to pull out the proofs, and providing two guarantees.
        The first guarantee is that the reflection of a term is guaranteed to be a
        faithful representation of the term. The second is that any generated
        proof is guaranteed to be a proof of the required property. The basic
        ideas of the technique are first presented in Section~\ref{sect:ideas} on a
        smaller problem with only natural numbers and addition, and are later
        adapted for the hierarchy of tactics in Section~\ref{sect:general}. 
        Section~\ref{sect:reuse} contains some implementation details.	

        \item The normalisation procedures are implemented by following a
        correct by construction approach, instead of implementing a
        normalization procedure, and proving afterwards that this function
        effectively computes a normal form. This approach is much more suitable
        for programming languages like Idris. Again, this approach will be
        presented in Section~\ref{sect:ideas} on the smaller problem, 
        and in Sections~\ref{sect:general} and~\ref{sect:reuse}
        for the complete hierarchy of algebraic structures.

        \item We develop a hierarchy of tactics where each tactic reuses the
        rewriting machinery from the structure immediately above in the hierarchy.
        For example,
        simplifying neutral elements is only implemented at the monoid level,
        and each level above monoid reuses it. Re-usability can be quite tricky
        to obtain when we want to reuse a prover from a structure which is less
        expressive. For example, reusing the monoid prover for building the
        group prover is not trivial, since we lose the possibility to express
        negations ($-x$) and minus operations ($x-y$). Some specific encodings
        will be presented in Section~\ref{sect:reuse} for solving this problem.

\end{itemize}
