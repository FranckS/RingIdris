\section {Discussion}

\label{sect:discuss}

	\subsection{An alternative approach, more traditional}

Without our type-safe reflection mechanism, the traditional way to go for this problem would be to have functions doing only computations, and some external lemmas about them. It would start with the definition of the function $reify\ :\ Expr\ \rightarrow\ c$. As we did in our approach, there would still be a function for reflecting terms $reflect\ :\ c\ \rightarrow\ Expr$. Because $reflect$ is a function based on syntax, it needs to use Idris' reflection mechanism. We would need to prove the correctness of these two functions, and it turns out that one lemma is enough for completely specifying the expected behaviour of these two functions at the same time : $reflect\_and\_reify\_correct\ :\ \forall\ x:c,\ reify\ (reflect\ x)\  \simeq \ x$.

Then comes the normalisation function : $normalize\ :\ Expr\ \rightarrow\ Expr$. Because this function is weekly typed compared to our approach (we no longer have an index giving a guarantee about the concrete values), we would need to provide a lemma of correctness about it after its definition. This lemma of correctness must say that the interpretation of the original (reflected) term is equal to the interpretation of the normalised (reflected) term. $normalize\_correct\ :\ \forall\ e:Expr,\ reify\ (normalize\ e)\  \simeq\ reify\ e$. We have avoided the need of such a --complicated-- proof in our correct by construction approach enabled by our type-safe reflection mechanism.

Now, similarly to what we've done, we would need a syntactical equality test, checking whether two reflected expressions are syntactically equal. The difference here is that, as often with this kind of "traditional" approach, we would produce something that belongs to the world of computations, usually a (pretty uninformative) boolean.
$beq_{Expr}\ :\ Expr\ \rightarrow\ Expr\ \rightarrow\ bool$. Because this boolean on its own is pretty uninformative, we now need a proof of correctness for this function as well, which says that if this function decides that two given terms are syntactically equal, then their interpretation should also be (syntactically) equal. $beq_{Expr}\_correct\ :\ \forall (e1\ e2:Expr), beq_{Expr}\ e1\ e2\ =\ true\ \rightarrow\ reify\ e1\  \simeq\ reify\ e2$. We did not have to do this proof either with our approach.

We could now build the kernel of the tactic, which is the following decision procedure. 
\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
decideEq : c $\rightarrow$ c $\rightarrow$ bool 
decideEq x y =
      let ex = reflect x in 
      let ey = reflect y in
      $beq_{Expr}$ (normalise ex) (normalise ey)
\end{lstlisting}
\end{center}
\caption{Decision procedure of the tactic with a "traditional" approach}
\figrule
\end{figure}
This function on its own would not  be enough. We would need to add a correctness theorem, and this is precisely this main theorem that would help us to build the tactic we want.	
$decideEq\_correct\ :\ \forall\  x\ y:c,\ decideEq\ x\ y\ =\ true\ \rightarrow\ x\  \simeq\ y$. This proof can can made by combining, in this order, the three lemmas $beq_{Expr}\_correct$, $normalize\_correct$ (two times, one on each side of the assumed equality), and finally $reflect\_and\_reify\_correct$ (again two times).

Now, the tactic could be built by using this correctness theorem. When trying to prove a goal $a\ \simeq \ b$, the tactic would apply this theorem $decideEq\_correct$, which means that $x$ will be unified to $a$ and $y$ to $b$. The desired proof is thus produced if the premise $decideEq\ a\ b\ =\ true$ holds. But whether this premise holds or not can simply be tested by running the function $decideEq$ on the two arguments $a$ and $b$. 

Both with this "traditional" approach and with our type-safe reflection, the activity of proving has effectively been replaced by the task of running a function, and a simple evaluation is now enough to produce the desired proof. The main advantage of our approach compared to this kind of traditional approach is that we don't need external lemmas, and more generally that the proof of correctness is obtained a lot more easily. Let's now see the differences more precisely with an actual implementation that almost follows the paradigm of the traditional approach.

	\subsection {Comparison with Coq's approach}
	
		\subsubsection{Description of Coq's implementation}	
	
In Coq's latest implementation of a Ring solver, described in~\cite{Coq2005}, they almost followed what has just been desribed as a "traditional" approach with the use of many auxiliary lemmas. The few differences essentially lie in the way the function $reflect$ is defined. In Coq, they use the language LTac to program this automatic reflection\footnote{This function --that they call mkPolexpr-- can be found in $plugins/ring/Ring\_tac.v$}. Ltac is an untyped tactic language. It is untyped in the sense that an Ltac "function" produces something which might not have a valid  --and unique-- type. Because of this, Ltac definitions can only be used in the context of goals. Thus, applying a tactic defined with LTac might work, and then it makes progress to the current goal, or it might fail, and in this case the goal is kept unchanged. And anyway, the safety of the proof done is going to be checked during the final QED. So, Ltac functions can't be used in the statement of a lemma, and it is therefore not possible to reason about them. That means that it is not possible to write the lemma $reflect\_and\_reify\_correct$ of the "traditional" approach described in the previous subsection, because it is even impossible to state it as it uses a function defined in LTac. Because they can't even state this property, they can't finish the proof of the main theorem $decideEq\_correct\ :\ \forall\  x\ y:c,\ decideEq\ x\ y\ =\ true\ \rightarrow\ x \simeq\ y$. Indeed, the last step of this proof was to apply two times --one for the LHS, one for the RHS-- the fact that $\forall\ x:c,\ reify\ (reflect\ x)\ \simeq\ x$. One possibility would have been to add this axiom, but this is particularly unsightly, and potentially harmful. Also, it would imply that anyone using the ring prover would have this axiom added to his development, and would be forced to believe in it. Some proofs would be done automatically for the user of the system, but at the overly-expensive price of adding some uncertainty.
Instead, what they did for Coq's implementation of the Ring prover was to replace the main theorem $\forall\  x\ y:c,\ decideEq\ x\ y\ =\ true\ \rightarrow\ x\ \simeq\ y$ by the following weaker --but still powerful enough to build the desired tactic-- lemma\footnote{This lemma --that they call $setpolynomial\_simplify\_ok$-- can be found in $plugins/ring/Setoid\_ring\_normalize.v$}  : $f\_correct\ :\ \forall\ (e1\ e2\ :\ Expr),\ beq_{Expr}\ (norm\ e1)\ (norm\ e2)\ =\ true\ \rightarrow\ reify\ e1\ \simeq\ reify\ e2$.
Now, the tactic works like this. When trying to prove the goal $a \simeq b$, it computes $(reflect\ a)$ and $(reflect\ b)$. It then tries to apply $(f\_correct\ (reflect\ a)\ (reflect\ b))$ to the goal. 
\itemize
\item
If it can unify the goal $a \simeq b$ with $reify\ (reflect\ a)\ \simeq\ reify\ (reflect\ b)$ then it only has to check the validity of the premise by running the decision procedure. More precisely, if $beq_{Expr}\ (norm\ (reflect\ a))\ (norm\ (reflect\ b))$ is evaluated to $true$, then it has built the premise of $f\_correct$ and there's nothing left to prove. However, if the result was false, then it means that the automatic ring prover hasn't been able to prove this goal, because the LHS and RHS don't reduce to the same thing. If this ring prover is complete, it means that $a$ is not\footnote{Note that in this case, it hasn't produced such a formal proof of disequality, which isn't the goal of a ring prover} equal to $b$, or at least, they aren't equal only with the properties of a ring.
\item
if it couldn't unify the goal $a \simeq b$ with $reify\ (reflect\ a)\ \simeq\ reify\ (reflect\ b)$, then it means that there is something wrong in the definitions of the functions $reflect$ and $reify$, and it could print something like "The ring prover has failed unexpectedly. There's something wrong with the implementations of the $reflect$ and $reify$ functions". Nothing really bad happened, as no inconsistent proof has been produced, nor no axioms added. That would just mean that the implementation of the ring prover is slightly broken, and that some goals that should be automatically provable aren't automatically proved. Thus, it would only decrease the completeness of the prover. That would be of course bad --because in the extreme case, the prover never succeeds in a generating a proof and is thus completely useless--, but that would only limit the scope of usage of the prover. \\

		\subsubsection{Differences with Coq's implementation}

Coq's ring prover follows the traditional\footnote{By ``conventional" we have in mind the proof engineering style developed in the Coq'Art ~\cite{BertotC04}}  approach of defining functions, and latter on proving many auxiliary lemmas about them. This is particularly adapted to Coq, which has many facilities for the construction of proofs, and a powerful proof mode. However, this approach kind of duplicate the work : they first tell the machine how it works, and latter, why it works. This separation between the world of computations and the world of logic becomes smaller in our approach with a fine use of dependent types, that allows to write more specific types, and thus to capture logical properties. The writing of functions can therefore be guided by this more precise type, and this is one of the main benefit of the approach we've followed. Moreover, we almost get the proof of correctness for free, because every little bit of rewriting done for the normalisation of the reflected terms is accompanied by the logic justification which tells why this rewriting can be done --and locally, this justification is always simple !--. In the end, all the proofs we have to do are systematically straightforward, since they only contain rewritings with the available hypothesis and the use of the properties of the corresponding algebraic structure. \\

Another difference with our implementation is that we've implemented a hierachy of tactics for many algebraic structures, but Coq's implementation only deals with ring and semi ring. If someone wants to prove equalities in a commutative group that isn't a ring, he simply can't use their prover. A dedicated prover for commutative groups would be needed, and Coq currently doesn't has one. The worst is that such a prover for commutative group would do very similar treatments, which means a lot of code could have been factorised. This is what we've obtained by taking this in account from the start. Our monoid prover used the underneath semi-group prover, our group prover uses the underneath monoid prover, and so on. \\

[Mentioning performances, that might be in favour of Coq]

	\subsection {Correctness and completeness}

		\subsubsection{Correctness}
		
The correctness of the tactic is in fact obtained by construction. Actually, what we really needed to produce was the proof of the equality $a \simeq b$, and not the normalised terms. The normalised term --and more precisely their construction-- was just a support for building the desired proof of $a \simeq b$, which is precisely the proof of correctness. Thus, there isn't any possibility that we've generated a wrong proof as long as we ensure that :
\begin{itemize}
\item We haven't introduced any axioms
\item We haven't introduced any non total\footnote{Contrary to Coq, Idris allows the definition of non-total functions for helping the development of non-finished real-world projects} function or proof used for the building of the main proof $a \simeq b$. Note that the normalization itself could contain some non total functions without breaking the correctness. That would just mean that when using this tactic, the tactic could potentially loop infinitely and the user would never get his answer. That would of course be bad, but that would not produce any inconsistency --as long as none uses these definitions maliciously to prove something false--, because this non total function would be only used in the world of computations and not in the world of logic.
\end{itemize}

Because we've meet these two conditions, we simply can't produce a false proof of a statement, because the typechecker would complain if that would be the case. If Idris internal type theory is consistent and if Idris' internal implementation follows it (and we believe in both!), then one can't produce a false lemma without either introducing an axiom nor introducing a non total function and latter using it maliciously to create an inconsistency in the logic.

		
		\subsubsection{Completeness}
		
Completeness means that when given an equality which is provable by using the properties of the concerned algebraic structure, the adapted prover should be able to generate a proof of this equality. Having a complete tactic is important, because otherwise the tactic is not always usuable -- and in the worst case it is never usable--, and thus is not really valuable. However, having a formal proof of the completeness in the system itself isn't really interesting. As long as we do not meet an equality that should be provable, and which isn't proved by our tactic, there is no problem. And if this situation does happen one day, nothing really bad happened : no inconsistencies would have been introduced in the system. The tactic would just refuse to do it automatically, but the user could still attempt it. \\

In definitive, we want the tactic to be complete, without requiring a formal proof of it. If we believe that the normal form computed by the reduction function is indeed a normal form --which means that if two terms are equal according to the properties of the algebraic structure, then their normal form should be equal-- then the tactic is complete.
Indeed, 
$isNormalForm\ :\ \forall\ a\ b,\ a \simeq b \rightarrow norm\ (reflect\ a) = norm\ (reflect\ b)$ is logically equivalent to

$completeness : \ \forall\ a\ b,\ norm\ (reflect\ a) \neq norm(reflect\ b) \rightarrow a \not{\simeq} b$, which is in fact :
$\ \forall\ a\ b,\ decideEq\ (reflect\ a)\ (reflect\ b) = Nothing \rightarrow a \not{\simeq} b$.
This equivalence between $isNormalForm$ and $completeness$ can only be done in the meta theory because it is a proof by contradiction, which is equivalent to the law of excluded middle, that we do not have in constructive logics, on which Idris is based.
\\

So far, we haven't encountered any goal that should be provable but which failed to be proven automatically. However, if this situation would appear one day, it would certainly come from a small kind of simplification that would have been forgotten somewhere in the hierarchy, and it would be easy to fix it by adding a sub-function that deals with this specific simplification at the appropriated level. And all the more specialised provers that reuse this prover will also be fixed at the same time thanks to the compositionality of our provers. 
\\

Another little advantage of our approach, compared to Coq's approach, is that we have the guarantee that we can't lose any totality because of the $reflect$ function. The reason is that the produced reflected term is indexed over the original --concrete-- value. By this fact, we don't need a $reify$ function. If we want to go from the reflected term to the original --concrete-- value, we can simply return the index. Thus, unlike Coq, we have the formal guarantee that $(reflect\ a)$ is always a faithful representation of $a$ with our approach, which means that we can't lose any totality because of the $reflect$ function.

