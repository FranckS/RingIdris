\section{A Simplified Problem: Proving Equalities with Natural Numbers and Additions}

\label{sect:ideas}

We will first present the basic ideas of our technique on a simplified problem, in which we aim to deal with universally quantified natural numbers and additions.
For example, we would like to be able to automatically generate proofs of goals like $\forall x1\ x2\ x3:Nat,\ (x1 + l2) + (x3 + x4) = (x1 + (x2 + x3)) + x4$ [Example 1]. \\
For this smaller problem, we have decided to only work with the associativity of $+$ : $plusAssociative : \forall x1\ x2\ x3,\ (x1 + x2) + x3 = x1 + (x2 + x3)$ and with the fact that Z is a right neutral element for the $+$ operation : $plusZeroRightNeutral : \forall x, x + Z = x$. Note that $Z$ is also a left neutral element, but this property is not needed because we have this behaviour by reduction, as $+$ is defined recursively on its first argument. Of course, some datatypes can have more or less properties than these two, and this work will be extended in sections 3 and 4 where we will present a very general hierarchy of provers for multiple algebraic structures. \\
Thus, in definitive, in this section, we want to write a decision procedure, able to tell if two expressions composed of universally quantified natural numbers and additions of these numbers are equal, and to produce a proof of this equality if appropriate, where ``equal" has the meaning syntactically equal or equal thanks to the associativity and right neutral properties.


\subsection{Working by reflection}

When trying to prove this kind of equalities, the variables are abstracted, and they become part of the context. On the example 1 just above, after abstraction of the variables, the goal becomes simply $(x1 + x2) + (x3 + x4) = (x1 + (x2 + x3)) + x4$, which is something of the general form $x=y$.
The general idea --that will also apply for the more general problem detailed in the next sections-- will be to normalize both sides of the "potential equality" $x=y$, and afterwards to compare them using Leibniz syntactical equality.
The goal of the normalization is to compute a canonical representation for a number $x$, such that any other number provably equal to $x$ (by using the two available properties) will have the same canonical representation. For example, the normalisation might transform $x+((y+Z)+z)$ into $(x+y)+z$ if we decide that the normal form will be completely left associative, and that every addition between an element $a$ and zero should be simplified to $a$. It will then be possible to decide the equality by simply comparing the normalised left and right hand sides with a simple and strict syntactical equality. This is in fact what we do all the time when we have to decide if two things, written differently, are equal or not. When a human is given two mathematical polynomials and has to decide the equality of these functions, a technique that always works is to decide once and for all a canonical representation of polynomials, and to put both polynomials in this form. If the normalised forms are the same, then the two original polynomials are equal, otherwise they do not represent the same computation.\\
\\
In fact, such a normalisation function can't be written directly, because in the LHS and RHS of $x=y$, we potentially have variables which have been universally quantified. And the normalization function needs to do different treatments for a ``variable natural number" (a number which has been universally quantified) and for the constant $Z$. This is not possible yet, because once the variables are abstracted, they are just ordinary values of type $Nat$, and there is no way to distinguish them. Indeed, this information only exists at the level of the ASTs representing the two terms, and we do not have a direct access to these syntax trees.
For this reason, we will work by reflection. In this little example, it means that we will define a datatype that will be used as an encoding of natural numbers, or more precisely, an encoding of natural numbers composed of ``variable numbers", $Z$, and additions of these things. This datatype will let us inspect the internal structure of a number by pattern matching.
Previously, we were only able to pattern match a natural number against the constructors $Z$ and $S$, which wasn't what we needed.  With the first approximation of the data type $Expr$ presented in the figure~\ref{reflectedNaturalNumbers0}, we will be allowed to pattern match an encoding of number against the constructors $Plus$, $Var$ and $Zero$.


\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
data Expr : ($\Gamma$ : Vect n Nat) $\rightarrow$ Type where
     Plus : {n:Nat} $\rightarrow$ {$\Gamma$:Vect n Nat} $\rightarrow$ 
            Expr $\Gamma$ $\rightarrow$ Expr $\Gamma$ $\rightarrow$ Expr $\Gamma$
     Var  : {n:Nat} $\rightarrow$ {$\Gamma$:Vect n Nat} $\rightarrow$ 
            (i : Fin n) $\rightarrow$ Expr $\Gamma$
     Zero : {n:Nat} $\rightarrow$ ($\Gamma$:Vect n Nat) $\rightarrow$ 
            Expr $\Gamma$ Z
\end{lstlisting}
\end{center}
\caption{First version of reflected natural numbers}
\label{reflectedNaturalNumbers0}
\figrule
\end{figure}


Variables are represent using a De Brujin-like index : (Var FZ) denotes a variable, (Var (FS FZ)) another one, and so on.

The type Expr is indexed over a vector of numbers $\Gamma$, which is the context of all universally quantified variables. In the example 1, we will encode $(x1 + x2) + (x3 + x4)$ and $(x1 + (x2 + x3)) + x4$ in a context where four elements are present. The first element of this context denotes the variable $x1$, the second denotes $x2$, and so on.
Thus, the left hand side will be encoded by :

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
e1 : (x, y, z : Nat) $\rightarrow$ 
           Expr [x, y, z]
e1 x y z = Plus (Plus (Var FZ) (Var (FS FZ))) 
                (Plus (Var FZ) (Var (FS (FS FZ))))
\end{lstlisting}
\end{center}
\caption{Reflected LHS with the first version of reflected numbers showed in \ref{reflectedNaturalNumbers0}}
\figrule
\end{figure}

\subsection{Type safe reflection}

If we continue with this first definition of $Expr$, the normalisation function will certainly take an Expr and produce another Expr, and we will need to prove the following correctness lemma afterwards : \\
$\forall\ e:Expr\ \Gamma,\ reify\ (reduce\ e)\ =\ reify\ e$ \\
where $reify$ is a function computing the interpretation of an Expr in a context $\Gamma$, that is to say, the natural number that this Expr is encoding.
This proof can be quite tricky to build because it relies on the complete behaviour of the reduction and on the way the interpretation is computed.
For this little example, the reduction procedure won't be too heavy, but in the next sections, with more sophisticated algebraic structures, we will have more properties to deal with and it will certainly become more problematic to unfold the definition of a gigantic reduction procedure. We want to avoid pain as much as possible, and for this reason, we want to avoid the writing of big proofs, and even to avoid the writing of proofs as much as possible. \\
\\
To avoid this source of complexity, we add an index to the type $Expr$, and this index is precisely the concrete number that the Expr is encoding. This is the first brick to our type-safe reflection mechanism. Thus, it won't be necessary to define the $reify$ function, as we will know directly the concrete element reflected by a term of type $Expr\ \Gamma\ x$ just by looking at its index $x$. We also get the guarantee that the reflection of an expression $e$ is indeed a faithful representation of $e$ \\

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
using (x : Nat, y : Nat, $\Gamma$ : Vect n Nat)
  data Expr : (Vect n Nat) $\rightarrow$ Nat $\rightarrow$ Type where
       Plus : Expr $\Gamma$ x $\rightarrow$ Expr $\Gamma$ y $\rightarrow$ 
              Expr $\Gamma$ (x + y)
       Var  : (i : Fin n) $\rightarrow$ Expr $\Gamma$ (index i $\Gamma$)
       Zero : Expr $\Gamma$ Z
\end{lstlisting}
\end{center}
\caption{Second version of reflected number with embedded denotation}
\label{reflectedNaturalNumbers}
\figrule
\end{figure}

For an expression $ex\ :\ Expr\ \Gamma\ x$, we will say that ``$ex$ denotes (or encodes) the number $x$ in the context $\Gamma$".
When an expression is a variable, the denoted number is simply the corresponding variable in the context, ie, $(index\ i\ \Gamma)$.
Also, the $Zero$ expression denotes the natural number $Z$.
Finally, if $ex$ is an expression encoding the number $x$, and $ey$ is an expression encoding the number $y$, then the expression $Plus\ ex\ ey$ denotes the concrete number $(x + y)$.


\subsection{A correct by construction approach}

We want to write the reduction function on a ``correct by construction" way, which means that no additional proof should be required after the definition of the function. Thus, $reduce$ will produce the proof that the new Expr freshly produced has the same interpretation as the original Expr, and this will be made easier by the fact that the data type Expr is now indexed over the real --concrete-- number : a term of type $Expr\ \Gamma\ x$ is the encoding of the number $x$.
Thus, we can write the type of $reduce$ like this : \\
$reduce\ :\ Expr\ \Gamma\ x\ \rightarrow\ (x'\ **\ (Expr\ \Gamma\ x',\ x\ =\ x'))$ \\
The function $reduce$ produces a dependent pair : the new concrete number $x'$, and a pair made of an $Expr\ \Gamma\ x'$ which is the new encoded term indexed over the new concrete number we have just produced, and a proof that old and new --concrete-- numbers are equal.
Note that this function can't simply produce an $Expr\ \Gamma\ x$, because the concrete number on which the resulting expression will be indexed is not necessary syntactically equal to the original number since this equality can use the two available properties. Said differently, even if we can prove $x=x'$ (if the function is correctly defined), we do not have $x \equiv x'$.
And in fact, what really interests us in this function is precisely this proof of $x\ =\ x'$.
The reason is that when we try to automatically prove $x=y$, these proofs will be the crucial part for the construction of the desired proof. \\
\\
The tactic will work as follow.
We have an expression $ex$ encoding $x$, and an expression $ey$ encoding $y$. We will normalize $ex$, and this will give a new concrete number $x'$, a new expression $ex':Expr\ \Gamma\ x'$, and a proof of $x=x'$. We will do the same with $ey$, and we will get a new concrete number $y'$, an expression $ey':Expr\ \Gamma\ y'$, and a proof of $y=y'$. \\
It is now enough to simply compare $ex'$ and $ey'$ using a standard syntactical equality because these two expressions are now supposed to be in normal form :

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
eqExpr : (e : Expr $\Gamma$ x) $\rightarrow$ (e' : Expr $\Gamma$ y) $\rightarrow$ Maybe (e = e')
eqExpr (Plus x y) (Plus x' y') with (eqExpr x x', eqExpr y y')
  eqExpr (Plus x y) (Plus x y) | (Just Refl, Just Refl) = Just Refl
  eqExpr (Plus x y) (Plus x' y') | _ = Nothing
eqExpr (Var i) (Var j) with (decEq i j)
  eqExpr (Var i) (Var i) | (Yes Refl) = Just Refl
  eqExpr (Var i) (Var j) | _ = Nothing
eqExpr Zero Zero = Just Refl
eqExpr _ _ = Nothing
\end{lstlisting}
\end{center}
\caption{Syntactical equality between reflected terms}
\label{syntacticalEq}
\figrule
\end{figure}


Now, if the two normalised expressions $ex'$ and $ey'$ are equal, then they necessary have the same type\footnote{We are working with the heterogeneous equality JMeq by default in Idris, but as always, the only way to have a proof of a:A = b:B is when A=B}, and therefore $x'=y'$.
By rewriting the two equalities $x=x'$ and $y=y'$ (that we obtained during the normalisations) in the new equality $x'=y'$, we can get a proof of $x=y$. This is what the function $buildProof$ is doing.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
buildProof : {x : Nat} $\rightarrow$ {y : Nat} $\rightarrow$ Expr $\Gamma$ x' $\rightarrow$ Expr $\Gamma$ y' 
           $\rightarrow$ (x = x') $\rightarrow$ (y = y') $\rightarrow$ Maybe (x = y)
buildProof ex' ey' lp rp with (eqExpr ex' ey')
  buildProof ex' ex' lp rp | Just Refl = ?MbuildProof
  buildProof ex' ey' lp rp | Nothing = Nothing
\end{lstlisting}
\end{center}
\caption{Building the desired proof with the two proofs of equality}
\label{buildProof}
\figrule
\end{figure}

The argument of type $Expr\ \Gamma\ x'$ is the normalised reflected left hand size of the equality, which represents the value $x'$. Before the normalisation, the reflected LHS was reflecting the value $x$. The $Expr\ \Gamma\ y'$ is the normalised reflected right hand size, which now represents the value $y'$, but which was encoding $y$ before the normalisation. The function also expects the proofs that $x=x'$ and $y=y'$, and we should be able to pass them because the normalisation function has also produced the proof of equality between the original and the new concrete values.

As mentioned, the proof for the metavariable $MbuildProof$ is just a rewriting of the two equalities :

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
  MbuildProof = proof {
  intros; refine Just; rewrite sym p1; rewrite sym p2; exact Refl;
}  
\end{lstlisting}
\end{center}
\caption{buildProof metavariable}
\label{MbuildProof}
\figrule
\end{figure}

Finally, the main function which tries to prove the equality $x=y$ simply has to reduce the two reflected terms encoding the left and the right hand side, and to use the function $buildProof$ in order to compose the two proofs that we just obtained :

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
  testEq : Expr $\Gamma$ x $\rightarrow$ Expr $\Gamma$ y $\rightarrow$ Maybe (x = y)
  testEq ex ey = 
     let (x' ** (ex', px)) = reduce ex in 
     let (y' ** (ey', py)) = reduce ey in
        buildProof ex' ey' px py 
\end{lstlisting}
\end{center}
\caption{testEq}
\label{testEq}
\figrule
\end{figure}

We now need to define the function reduce. To do that, we have to decide a canonical representation of associative natural numbers. We decide that the left associative form will be the canonical representation. Thus, the $reduce$ function has to rewrite the reflected terms by rearranging the parentheses in order to transform the underlying concrete number in the form $(...((x1 + x2) + x3) ... + xn)$. To do so, one possibility is to define a new datatype which captures this property, and to write a function going from $Expr$ to this new type. Thus it will be easier to be certain that we are effectively computing the normal form : forcing properties to hold by the shape of a datatype is a good usage of dependent types when, like here, it doesn't introduce more complications.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
data LExpr : ($\Gamma$ : Vect n Nat) $\rightarrow$ Nat $\rightarrow$ Type where
     LPlus : LExpr $\Gamma$ x $\rightarrow$ (i : Fin n) 
             $\rightarrow$ LExpr $\Gamma$ (x + index i $\Gamma$)
     LZero : LExpr $\Gamma$ Z
\end{lstlisting}
\end{center}
\caption{Reflected left associative numbers}
\label{LExpr}
\figrule
\end{figure}

This datatype has only two constructors. In fact, it combines the previous $Var$ and $Plus$ constructors so that it becomes impossible to write an expression which isn't left associative (because LPlus is only recursive on its first argument).
 
As part of the normalization, we write a function $expr\_l$ which converts an $Expr\ \Gamma\ x$ to a $LExpr\ \Gamma\ x'$ and which produces a proof of $x=x'$. This function will therefore use the two available properties multiple times, while rewriting the term until the fully left associative desired form is obtained.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
expr_l : Expr $\Gamma$ x 
         $\rightarrow$ (x' ** (LExpr $\Gamma$ x', x = x'))
expr_l Zero = (_ ** (LZero, Refl))
expr_l (Var i) = (_ ** (LPlus LZero i, Refl))
expr_l (Plus ex ey) = 
  let (x' ** (ex', px)) = expr_l ex in
  let (y' ** (ey', py)) = expr_l ey in
  let (res ** (normRes, Pres)) = plusLExpr ex' ey' in
    (res ** (normRes, rewrite px in (rewrite py in Pres)))
      where 
      plusLExpr : {$\Gamma$ : Vect n Nat} $\rightarrow$ {x, y : Nat} 
            $\rightarrow$ LExpr $\Gamma$ x -> LExpr $\Gamma$ y  
            $\rightarrow$ (z ** (LExpr $\Gamma$ z, x+y=z))
      plusLExpr {x=x} ex LZero =
        (_ ** (ex, rewrite (plusZeroRightNeutral x) in Refl))            
      plusLExpr ex (LPlus e i) =
        let (xRec ** (rec, prfRec)) = plusLExpr ex e in
            (_ ** (LPlus rec i, ?MplusLExpr))

\end{lstlisting}
\end{center}
\caption{Production of the left associative form}
\label{expr_l}
\figrule
\end{figure}
In the case of an addition $Plus\ ex\ ey$, the function $expr\_l$ does the job of normalisation recursively on $ex$ and on $ey$, and then it uses the sub-function $plusLExpr$ to normalise the addition of these two --already normalised-- terms. This sub-function $plusLExpr$ has two kind of simplifications to do. When the second argument is an $LZero$, it simply returns its first arguments along with the justification for this rewriting, which obviously uses $plusZeroRightNeutral$. However, when the second argument is an $LPlus\ e\ i$, it continues recursively by computing $plusLExpr\ ex\ e$, and it finally adds $i$ to it. That had for effect to move the parenthesis on the left, and this treatment is going to be justified during the proof of the meta variable $MplusLExpr$ by the use of $plusAssociative$.

This metavariable $MplusLExpr$ requires us to prove the goal : $x1 + (x2 + index\ i\ \Gamma) = xrec + index\ i\ \Gamma$ in a context where we've got, amongst other things, $prfRec : x1 + x2 = xrec$.
By using the property of associativity on the goal, we now need to prove $(x1 + x2) + index\ i\ \Gamma$, which can be done by rewriting the proof $prfRec$ obtained recursively.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
MplusLExpr = proof {
  intros
  rewrite (sym (plusAssociative x1 x2 (index i $\Gamma$))); rewrite prfRec; 
  exact Refl;
}
\end{lstlisting}
\end{center}
\caption{Proof of the metavariable MplusLExpr}
\label{MplusExpr}
\figrule
\end{figure}

It is really important to understand that the root of the automatic building of the desired proof of $x=y$ happens precisely in these nested use of $plusZeroRightNeutral$ and $plusAssociative$ that we've seen in the definition of $expr\_l$ and of its meta-variable $MplusLExpr$. These proofs replace the arithmetical proofs that we were doing previously by hand, as it was the case in section 1 with the lemma $adc\_lemma\_2$. \\
\\
Using this new datatype $LExpr$ has changed the representation of our encoded lists, so we need to convert back an $LExpr\ \Gamma\ x$ to an $Expr\ \Gamma\ x$. The function $l\_expr$ does this easy task.
\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
l_expr : LExpr $\Gamma$ x $\rightarrow$ Expr $\Gamma$ x
l_expr LZero = Zero
l_expr (LPlus x i) = Plus (l_expr x) (Var i)
\end{lstlisting}
\end{center}
\caption{Going back from LExpr to Expr}
\label{l_expr}
\figrule
\end{figure}


We notice that in order to transform the expression into its left associative equivalent representation, we've effectively needed to know where the variables and the $Z$ constants are : the functions $expr\_l$ and $l\_expr$ are doing different treatments for these two possibilities. \\
\\

We can now define the reduction, which is just the composition of the two previous functions $expr\_l$ and $l\_expr$:

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
  reduce : Expr $\Gamma$ x $\rightarrow$ (x' ** (Expr $\Gamma$ x', x = x'))
  reduce e = 
     let (x' ** (e', prf)) = expr_l e in
         (x' ** (l_expr e', prf))
\end{lstlisting}
\end{center}
\caption{Reduction function}
\label{reduce}
\figrule
\end{figure}


At the moment, what we've got is not exactly a real tactic, in the sense that we only have a function which produces a value of type $Maybe (x = y)$. A real tactic would be a wrapper of this function that could properly fail with an error message when the two terms are not equal. However, here, when $x\ne y$, the function $testEq$ will simply produce the value $Nothing$. \\

\subsection{Usage of the ``tactic"}

It's now time to see how to use this minimalist ``tactic".
Let's define two expressions $e1$ and $e2$, respectively representing the numbers $((x + y) + (x + z))$ and $(x + ((y + x) + z))$ in the context $[x, y, z]$ of three abstracted variables.


\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
e1 : (x, y, z : Nat) 
    $\rightarrow$ Expr [x, y, z] ((x+y) + (x+z))
e1 x y z = Plus (Plus (Var FZ) 
                      (Var (FS FZ))) 
                (Plus (Var FZ) 
                      (Var (FS (FS FZ))))

e2 : (x, y, z : Nat) 
     $\rightarrow$ Expr [x, y, z] (x + ((y + x) + z))
e2 x y z = Plus (Var FZ) 
                (Plus (Plus (Var (FS FZ)) 
                            (Var FZ)) 
                      (Var (FS (FS FZ))))
\end{lstlisting}
\end{center}
\caption{Two test expressions}
\label{e1_e2}
\figrule
\end{figure}

The numbers denoted by the expressions $e1$ and $e2$ are equal, and we can generate a proof of this fact by using $testEq$.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
e1_e2_testEq : (x, y, z : Nat) 
             $\rightarrow$ Maybe (((x + y) + (x + z)) = (x + ((y + x) + z)))
e1_e2_testEq x y z = testEq (e1 x y z) (e2 x y z)
\end{lstlisting}
\end{center}
\caption{Generating automatically the desired proof}
\label{e1_e2_testEq}
\figrule
\end{figure}


And if we ask for the evaluation of this term, we should obtain $Just$ and a proof of equality between the two underlying numbers.

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting}
#\x => \y => \z => e1_e2_testEq x y z

\x => \y => \z => Just (replace (sym (replace (sym (replace 
(sym (plusAssociative x 0 y)) (replace (replace (sym 
(plusZeroRightNeutral x)) Refl) Refl))) (replace (sym 
(replace (sym (plusAssociative x 0 z)) (replace (replace 
(sym (plusZeroRightNeutral x)) Refl) Refl))) (replace (sym 
(plusAssociative (x+y) x z)) [...]
: (x : Nat) -> (y : Nat) -> (z : Nat) 
  -> Maybe ((x + y) + x + z 
            = x + (y + x) + z)
\end{lstlisting}
\end{center}
\caption{Obtained proof}
\label{obtainedProof}
\figrule
\end{figure}

And we effectively get the proof of equality we wanted. As expected, this proof uses the properties of associativity ($plusAssociative$) and the property of neutrality of $Z$ for $+$ ($plusZeroRightNeutral$).


\subsection{Construction of the reflected terms}
\label{sect:ReflectNat}

For the moment, even if what we have is perfectly usable and works, we had to create these encodings $e1$ and $e2$ by hand, which is easy but time consuming. We have replaced the (potentially hard) problem of proving something by the simplest problem of simply creating some encodings. This is already a huge simplification, because as it can be seen in the definitions of $e1$ and $e2$, the reflected terms completely follow the structure of the expression to encode : there's absolutely no creativity needed for this task, unlike the proving activity. The way to create the encodings is in fact so systematic that, of course, we would like to automatize it in order to get a real and completely automatic tactic.

However, we can also note that even when done by hand, there is no room for making mistakes in this simple task of encoding : we simply can't generate a wrong encoding : if $e1$ and $e2$ are not respectively reflecting $((x+y) + (x+z))$ and $(x + ((y + x) + z))$ then these definitions won't typecheck because the expected and real index won't match.

We still want an automatic way of constructing these reflected terms because what we have so far is enough to demonstrate that our prover works, but that's not yet enough for being used intensively. We want to program an automatic way of going from the concrete values (of type $Nat$), to the reflected terms (of type $Expr$). The only way to do that is to inspect the abstract syntax tree of the concrete value.
By using Idris's reflection mechanism, we can tag a function with the keyword "\%reflection", which means that this function runs on syntax instead of values. 

\begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
%reflection
total
reflectNat : {n:Nat} $\rightarrow$ ($\Gamma$ : Vect n Nat) $\rightarrow$ (x:Nat) 
             $\rightarrow$ (m ** ($\Gamma'$ : Vect m Nat ** (Expr ($\Gamma$ ++ $\Gamma'$) x)))
\end{lstlisting}
\end{center}
\caption{Type of the function for reflecting natural numbers}
\label{reflectNatType}
\figrule
\end{figure}

This function will reflect the natural number $x$ in the context of $\Gamma$, which contains $n$ already abstracted variables. This function will compute an extension --of arbitrary size $m$-- to the context, called $\Gamma'$. This extension will contain the variables used in $x$ that weren't already present in $\Gamma$. It will also produce the reflected term, which is expressed in the complete context $\Gamma ++\ \Gamma'$, and which is, of course, indexed over the concrete value $x$.

If $x$ is the natural number $Z$, then we don't have any variable to add to $\Gamma$, so the extension will be the empty vector, and the reflected expression is simply $Zero$.

 \begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
reflectNat {n=n} G Z = (Z ** ([] ** (Zero {n=n+0} {G=G++[]})))
\end{lstlisting}
\end{center}
\caption{Reflecting natural numbers - pattern for zero}
\label{reflectNat_pattern1}
\figrule
\end{figure}

If the natural number to reflect is an addition $x+y$, then we will start by reflecting $x$ in the context of $\Gamma$. That will give us an extension $\Gamma'$ and an expression $ex$ of type $Expr\ (\Gamma ++\ \Gamma')\ x$. We continue by reflecting $y$, but this time in the context $(\Gamma ++\ \Gamma')$ of $n+m$ already abstracted variables, because the reflection of $x$ has potentially abstracted some new variables, and we don't want to abstract them a second time. That will give us a second extension $\Gamma''$ and an expression $ey$ of type $Expr\ ((\Gamma ++\ \Gamma')\ ++\ \Gamma'')\ y$. We now simply want to return the $Plus$ of $ex$ and $ey$, but we can't immediately, because these encodings aren't defined on the same context. The context in which $ex$ makes sense is $(\Gamma ++\ \Gamma')$, but the context in which $ey$ makes sense is $((\Gamma ++\ \Gamma')\ ++\ \Gamma'')$. We will therefore need a function of weakening that takes a reflected expression $ex$, and returns the same expression, but expressed in an augmented context.

 \begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
reflectNat $\Gamma$ (x + y) =
     let (_ ** ($\Gamma'$ ** ex)) = (reflectNat $\Gamma$ x) in
     let (_ ** ($\Gamma''$ ** ey)) = (reflectNat ($\Gamma$ ++ $\Gamma'$) y) in
     let result = Plus (weaken $\Gamma''$ ex) ey in 
        (_ ** (($\Gamma'$ ++ $\Gamma''$) ** ?MreflectNat_1))
\end{lstlisting}
\end{center}
\caption{Reflecting natural numbers - pattern for a plus}
\label{reflectNat_pattern2}
\figrule
\end{figure}

The total extension that has been computed is $\Gamma' ++\ \Gamma''$, and the metavariable $MreflectNat\_1$ simply uses the associativity of the append operation to prove that the provided context $((\Gamma ++\ \Gamma')\ ++\ \Gamma'')$ is equal to the expected one $(\Gamma ++\ (\Gamma'\ ++\ \Gamma''))$. \\
\\
The function $weaken$ is easy to write because we're adding the extension $\Gamma'$ on the right of $\Gamma$. For example, extending the context $[v,\ w,\ x]$ with $[y,\ z]$ produces the context $[v,\ w,\ x,\ y,\ z]$. The variables $v$, $w$ and $x$, which were respectively refereed to as the first, second and third variables in $\Gamma$, are still the first, second and third variables in the complete context. That means that a variable $Var\ i$ will still be a $Var\ i$ after the weakening, the position $i$ hasn't changed by augmenting the context.

 \begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
weaken : {n:Nat} $\rightarrow$ {m:Nat} $\rightarrow$ {$\Gamma$:Vect n Nat} $\rightarrow$ {x:Nat} 
          $\rightarrow$ ($\Gamma'$:Vect m Nat) $\rightarrow$ (Expr $\Gamma$ x) $\rightarrow$ (Expr ($\Gamma$ ++ $\Gamma'$) x)
weaken $\Gamma'$ Zero = Zero
weaken $\Gamma'$ (Plus e1 e2) = Plus (weaken $\Gamma'$ e1) (weaken $\Gamma'$ e2)
weaken {n=n} {m=m} {$\Gamma$=$\Gamma$} $\Gamma'$ (Var i) = Var (convertFin _ i m)    
\end{lstlisting}
\end{center}
\caption{Weakening function}
\label{weaken}
\figrule
\end{figure}


The position $i$ hasn't changed but however, the original $i$ had type $Fin\ n$, but the new $i$ must have type $Fin\ (n+m)$. This is why we've used $convertFin$, which returns the same element, but seen in a bigger $Fin$.

 \begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
convertFin : (n:Nat) $\rightarrow$ (i:Fin n) $\rightarrow$ (x:Nat) $\rightarrow$ Fin (n+x)
\end{lstlisting}
\end{center}
\caption{Conversion of a Fin}
\label{convertFin}
\figrule
\end{figure}

We've treated the case of the constant $Z$ and the case of the addition. We now have to deal with the last possibility of a variable. For encoding a variable, we must see if this variable is already present in the context $\Gamma$ of already abstracted variables. If it is already present there, then there is no extension to build, and the reflected term is simple $Var\ (convertFin\ n\ i\ Z)$, where $i$ is the position of the variable in this context $\Gamma$. The conversion was needed because the original context was $\Gamma$, and the new one is $(\Gamma ++\ [\ ])$, which aren't automatically unifiable. However, if if this variable is missing, then we should add it, which means that we will return an extension containing this single variable. The original context $\Gamma$ had a size of $n$, and we've built an extension of size one, so the complete context has therefore size $(n+1)$. The variable that we've added is currently the last one of these  $(n+1)$ variables. If we use a function\footnote{This function doesn't have the type $(n:Nat)\ \rightarrow Fin\ n$ because there is no last element of a $Fin$ of size zero as there's no element at all}  $lastElement'\ :\ (pn:Nat)\ \rightarrow\ Fin\ (pn+1)$ to construct the last element of a $Fin$ of size $pn+1$, then the reflected term that we need to produce is simply $Var\ (lastElement'\ n)$. 

 \begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
reflectNat {n=n} $\Gamma$ t with (isElement t $\Gamma$)
  | Just (i ** p) = let result = Var {$\Gamma$=$\Gamma$++[]} (convertFin n i Z) in
                        (Z ** ([] ** ?MreflectNat_2))
  | Nothing ?= (((S Z) ** ([t] ** Var {$\Gamma$=$\Gamma$++[t]} (lastElement' n))))  
            
\end{lstlisting}
\end{center}
\caption{Reflecting natural numbers - pattern for a variable}
\label{reflectNat_pattern3}
\figrule
\end{figure}

$isElement$ is the function which checks whether an element $x$ belongs to a vector $\Gamma$, and if so, returns $Just$ and a dependent pair containing the index of $x$ in this vector, and a proof that $index\ i\ \Gamma\ =\ x$.

 \begin{figure}[H]
\figrule
\begin{center}
\begin{lstlisting} [mathescape]
isElement : {n:Nat} $\rightarrow$ (x : a) $\rightarrow$ ($\Gamma$ : Vect n a) 
            $\rightarrow$ Maybe (i:Fin n ** (index i $\Gamma$ = x))
\end{lstlisting}
\end{center}
\caption{Returns the index and the proof that this index is the right one}
\label{isElement}
\figrule
\end{figure}

The metavariable $MreflectNat\_2$ will require us to prove that the provided term of type $Expr\ (\Gamma ++\ [\ ])\ (index\ (convertFin\ n\ i\ Z)\ (\Gamma ++\ [\ ]))$ is transformable into a term of the expected type $Expr\ (\Gamma ++\ [\ ])\ t$. This is doable by using the proof $p$ --returned by $isElement$-- which says that $index\ i\ \Gamma\ =\ t$, together with the fact that $convertFin$ does not change the index, but only converts its type.

As for the case where $t$ wasn't in the original context (the $Nothing$ case), we will need to prove that we can convert $(index\ (lastElement'\ n)\ (\Gamma ++\ [t]))$ into $t$. This is doable because we can prove the lemma $indexOfLastElem\ :\ \{T:Type\}\ \rightarrow\ \{n:Nat\}\ \rightarrow\ (v:Vect\ n\ T)\ \rightarrow\ (x:T)\ \rightarrow\ index\ (lastElement'\ n)\ (v++[x])\ =\ x$ 	

            
That finishes the automatic reflection for this specific prover. The encoding of $((x+y) + (x+z))$ can now be produced automatically with $reflectNat\ [\ ]\ ((x+y)\ +\ (x+z))$.
            

            
