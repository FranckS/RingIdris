\section{A simplified problem : type safe reflection for proving equalities with universally quantified natural numbers and additions}


We will first present the basic ideas of our technique on a simplified problem, in which we aim to deal with universally quantified natural numbers and additions.
For example, we would like to be able to automatically generate proofs of goals like $\forall x1\ x2\ x3:Nat,\ (x1 + l2) + (x3 + x4) = (x1 + (x2 + x3)) + x4$ [Example 1]. \\
For this smaller problem, we have decided to only work with the associativity of $+$ : $plusAssociative : \forall x1\ x2\ x3,\ (x1 + x2) + x3 = x1 + (x2 + x3)$ and with the fact that Z is a right neutral element for the $+$ operation : $plusZeroRightNeutral : \forall x, x + Z = x$. Note that $Z$ is also a left neutral element, but this property is not needed because we have this behaviour by reduction, as $+$ is defined recursively on its first argument. Of course, some structures can have more or less properties than these two, and this work will be extended in the next sections when we will write a general hierarchy of provers for different algebraic structures. \\
Thus, in definitive, in this section, we want to write a decision procedure, able to tell if two expressions composed of universally quantified natural numbers and additions of these numbers are equal, and to produce a proof of this equality if possible, when "equal" has the meaning "syntactically equal or equal thanks to the associative and neutral properties".


\subsection{Working by reflection}

When trying to prove this kind of equalities, the variables are abstracted, and they become part of the context. On the example 1, after abstraction of the variables, the goal becomes simply $(x1 + x2) + (x3 + x4) = (x1 + (x2 + x3)) + x4$, which is something of the general form $x=y$.
The general idea --that will also apply for the more general and bigger problem detailed in the next sections-- will be to normalize both sides of the "potential equality" $x=y$, and afterwards to compare them using Leibniz syntactical equality.
The goal of the normalization is to compute a canonical representation for a number $x$, such that any other number provably equal to $x$ (by using the two available properties) will have the same canonical representation. For example, the normalisation might transform $x+((y+Z)+z)$ into $(x+y)+z$ if we decide that the normal form will be completely left associative, and every addition between an element $a$ and zero should be simplified to $a$. It will then be possible to decide the equality by simply comparing the normalised left and right hand sides with a simple syntactical equality. This is in fact what we do all the time that we have to decide if two things, written differently, are equal or not. When a human is given two mathematical polynomials and has to decide the equality of these functions, a technique that always works is to decide once and for all a canonical representation of polynomials, and to put both polynomials in this form. If the normalised forms are the same, then the two original polynomials are equal, otherwise they do not represent the same computation.\\
\\
In fact, such a normalisation function can't be written directly, because in the LHS and RHS of $x=y$, we potentially have variables which have been universally quantified. And the normalization function needs to do different treatments for a "variable natural number" (a number which has been universally quantified) and for the constant $Z$. This is not possible yet, because once the variables are abstracted, they are just ordinary values of type $Nat$, and there is no way to distinguish them. Indeed, this information only exists at the level of the ASTs representing the two terms, and we do not have a direct access to these syntax trees.
For this reason, we will work by reflection. In this little example, it means that we will define a data type which will be used as an encoding of natural numbers, or more precisely, an encoding of natural numbers composed of "variable numbers", $Z$, and additions of these things. This data type will allow us to know the internal structure of a number, ie, where the variables and constants are.
Indeed, this data type will allow pattern matching. Previously, we were only able to pattern match a natural number against the constructors $Z$ and $S$, which wasn't what we needed.  With the first approximation of the data type $Expr$ presented in the Fig 6, we will be allowed to pattern match an encoding of number against the constructor $Plus$, $Var$ and $Zero$, which gives us the informations we want.


\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
data Expr : (gam : Vect n Nat) -> Type where
     Plus : {n:Nat} -> {gam:Vect n Nat} -> 
            Expr gam -> Expr gam -> Expr gam
     Var  : {n:Nat} -> {gam:Vect n Nat} -> 
            (i : Fin n) -> Expr gam
     Zero : {n:Nat} -> (gam:Vect n Nat) -> 
            Expr gam Z
\end{verbatim}
\end{center}
\caption{First version of reflected natural numbers}
\figrule
\end{figure}


Variables are represent using a De Brujin-like index : (Var fZ) denotes a variable, (Var (fS fZ)) another one, and so on.

The type Expr is indexed over a vector of numbers $\Gamma$, which is the context of all universally quantified variables. In the example 1, we will encode $(x1 + x2) + (x3 + x4)$ and $(x1 + (x2 + x3)) + x4$ in a context where four elements are present. The first element of this context denotes the variable $x1$, the second denotes $x2$, and so on.
Thus, the left hand side will be encoded by :

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
Plus (Plus (Var gam fZ) 
           (Var gam (fS fZ))) 
     (Plus (Var gam (fS (fS fZ))) 
           (Var gam (fS (fS (fS fZ)))))
\end{verbatim}
\end{center}
\caption{Reflected LHS of example 1}
\figrule
\end{figure}

\subsection{Type safe reflection}

If we continue with this first definition of $Expr$, the normalisation function will certainly take an Expr and produce another Expr, and we will need to prove the following lemma afterwards : \\
$\forall\ e:Expr\ \Gamma,\ reify\ (reduce\ e)\ =\ reify\ e$ \\
where $reify$ is a function computing the interpretation of an Expr in a context $\Gamma$, that is to say, the natural number that this Expr is encoding.
This proof can be quite tricky to build because it relies on the complete behaviour of the reduction and on the way the interpretation is computed.
For this little example, the reduction procedure will not be too heavy, but in the next sections, with more sophisticated algebraic structures, we will have more rewriting rules to deal with and it will certainly become more problematic to "unfold" the definition of a gigantic reduction procedure. We want to avoid pain as much as possible, and for this reason, we don't want to have to write huge proofs. \\
\\
To avoid this source of complexity, we add an index to the type $Expr$, and this index is precisely the concrete number that the Expr is encoding. This is the first brick to our type-safe reflection mechanism. Thus, it won't be necessary to define the $reify$ function, as we will know directly the concrete element reflected by a term of type $Expr\ \Gamma\ x$ just by looking at its index $x$. Something more important is that we will also have a guarantee that the reflection of an expression $e$ is indeed a faithful representation of $e$ \\

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
using (x : Nat, y : Nat, gam : Vect n Nat)
  data Expr : (Vect n Nat) -> Nat -> Type where
       Plus : Expr gam x -> Expr gam y -> 
              Expr gam (x + y)
       Var  : (i : Fin n) -> Expr gam (index i gam)
       Zero : Expr gam Z
\end{verbatim}
\end{center}
\caption{Second version of reflected number with embedded denotation}
\figrule
\end{figure}

For an expression $ex\ :\ Expr\ \Gamma\ x$, we will say that "$ex$ denotes (or encodes) the number $x$ in the context $\Gamma$".
When an expression is a variable, the denoted number is simply the corresponding variable in the context, ie, $(index\ i\ \Gamma)$.
Also, the $Zero$ expression denotes the natural number $Z$.
Finally, if $ex$ is an expression encoding the number $x$, and $ey$ is an expression encoding the number $y$, then the expression $Plus\ ex\ ey$ denotes the concrete number $(x + y)$.


\subsection{A correct by construction approach}

We want to write the reduction function on a "correct by construction" way, which means that no additional proof should be required after the definition of the function. Thus, $reduce$ will produce the proof that the new Expr freshly produced has the same interpretation as the original Expr, and this will be made easier by the fact that the data type Expr is now indexed over the real --concrete-- number : a term of type $Expr\ \Gamma\ x$ is the encoding of the number $x$.
Thus, we can write the type of $reduce$ like this : \\
$reduce\ :\ Expr\ \Gamma\ x\ \rightarrow\ (x'\ **\ (Expr\ \Gamma\ x',\ x\ =\ x'))$ \\
The function $reduce$ produces a dependent pair : the new concrete number $x'$, and a pair made of an $Expr\ \Gamma\ x'$ which is the new encoded term indexed over the new concrete number we have just produced, and a proof that old and new --concrete-- numbers are equal.
Note that this function can't simply produce an $Expr\ \Gamma\ x$, because the concrete number on which the resulting expression will be indexed is not necessary syntactically equal to the original number since this equality can use the two available properties. Said differently, even if we can prove $x=x'$ (if the function is correctly defined), we do not have $x \equiv x'$.
And in fact, what really interests us in this function is precisely this proof of $x\ =\ x'$.
The reason is that when we try to automatically prove $x=y$, these proofs will be the crucial part for the construction of the desired proof. \\
\\
The tactic will work as follow.
We have an expression $ex$ encoding $x$, and an expression $ey$ encoding $y$. We will normalize $ex$, and this will give a new concrete number $x'$, a new expression $ex':Expr\ \Gamma\ x'$, and a proof of $x=x'$. We will do the same with $ey$, and we will get a new concrete number $y'$, an expression $ey':Expr\ \Gamma\ y'$, and a proof of $y=y'$. \\
It is now enough to simply compare $ex'$ and $ey'$ using a standard syntactical equality because these two expressions are now supposed to be in normal form :

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
eqExpr : (e : Expr gam x) -> (e' : Expr gam y) -> Maybe (e = e')
eqExpr (Plus x y) (Plus x' y') with (eqExpr x x', eqExpr y y')
  eqExpr (Plus x y) (Plus x y) | (Just Refl, Just Refl) = Just Refl
  eqExpr (Plus x y) (Plus x' y') | _ = Nothing
eqExpr (Var i) (Var j) with (decEq i j)
  eqExpr (Var i) (Var i) | (Yes Refl) = Just Refl
  eqExpr (Var i) (Var j) | _ = Nothing
eqExpr Zero Zero = Just Refl
eqExpr _ _ = Nothing
\end{verbatim}
\end{center}
\caption{Syntactical equality between reflected terms}
\figrule
\end{figure}


Now, if the two normalised expressions $ex'$ and $ey'$ are equal, then they necessary have the same type\footnote{We are working with the heterogeneous equality JMeq by default in Idris, but as always, the only way to have a proof of a:A = b:B is when A=B}, and therefore $x'=y'$.
By rewriting the two equalities $x=x'$ and $y=y'$ (that we obtained during the normalisations) in the new equality $x'=y'$, we can get a proof of $x=y$. This is what the function $buildProof$ is doing.

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
buildProof : {x : Nat} -> {y : Nat} -> Expr gam x' -> Expr gam y' 
           -> (x = x') -> (y = y') -> Maybe (x = y)
buildProof ex' ey' lp rp with (eqExpr ex' ey')
  buildProof ex' ex' lp rp | Just Refl = ?MbuildProof
  buildProof ex' ey' lp rp | Nothing = Nothing
\end{verbatim}
\end{center}
\caption{Building the desired proof with the two proofs of equality}
\figrule
\end{figure}

The argument of type $Expr\ \Gamma\ x'$ is the normalised reflected left hand size of the equality, which represents the value $x'$. Before the normalisation, the reflected LHS was reflecting the value $x$. The $Expr\ \Gamma\ y'$ is the normalised reflected right hand size, which now represents the value $y'$, but which was encoding $y$ before the normalisation. The function also expects the proofs that $x=x'$ and $y=y'$, and we should be able to pass them because the normalisation function has also produced the proof of equality between the original and the new concrete values.

As mentioned, the proof for the metavariable $MbuildProof$ is just a rewriting of the two equalities :

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
  MbuildProof = proof {
  intros; refine Just; rewrite sym p1; rewrite sym p2; exact Refl;
}  
\end{verbatim}
\end{center}
\caption{buildProof metavariable}
\figrule
\end{figure}

Finally, the main function which tries to prove the equality $x=y$ simply has to reduce the two reflected terms encoding the left and the right hand side, and to use the function $buildProof$ in order to compose the two proofs that we just obtained :

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
  testEq : Expr gam x -> Expr gam y 
           -> Maybe (x = y)
  testEq ex ey = 
     let (x' ** (ex', px)) = reduce ex in 
     let (y' ** (ey', py)) = reduce ey in
        buildProof ex' ey' px py 
\end{verbatim}
\end{center}
\caption{testEq}
\figrule
\end{figure}

We now need to define the function reduce. To do that, we have to decide a canonical representation of associative natural numbers. We decide that the left associative form will be the canonical representation. Thus, the $reduce$ function has to rewrite the reflected terms by rearranging the parentheses in order to transform the underlying concrete number in the form $(...((x1 + x2) + x3) ... + xn)$. To do so, one possibility is to define a new datatype which captures this property, and to write a function going from $Expr$ to this new type. Thus it will be easier to be certain that we are effectively computing the normal form : forcing properties to hold by the shape of a datatype is a good usage of dependent types when, like here, it doesn't introduce more complications.

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
data LExpr : (gam : Vect n Nat) -> Nat -> Type where
     LPlus : LExpr gam x -> (i : Fin n) 
             -> LExpr gam (x + index i gam)
     LZero : LExpr gam Z
\end{verbatim}
\end{center}
\caption{Reflected left associative numbers}
\figrule
\end{figure}

This datatype has only two constructors. In fact, it combines the previous $Var$ and $Plus$ constructors so that it becomes impossible to write an expression which isn't left associative (because LPlus is only recursive on its first argument).
 
As part of the normalization, we write a function $expr\_l$ which converts an $Expr\ \Gamma\ x$ to a $LExpr\ \Gamma\ x'$ and which produces a proof of $x=x'$. This function will therefore use the two available properties multiple times, while rewriting the term until the fully left associative desired form is obtained.

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
expr_l : Expr gam x 
         -> (x' ** (LExpr gam x', x = x'))
expr_l Zero = (_ ** (LZero, Refl))
expr_l (Var i) = (_ ** (LPlus LZero i, Refl))
expr_l (Plus ex ey) = 
  let (x' ** (ex', px)) = expr_l ex in
  let (y' ** (ey', py)) = expr_l ey in
  let (res ** (normRes, Pres)) = plusLExpr ex' ey' in
    (res ** (normRes, rewrite px in (rewrite py in Pres)))
      where 
      plusLExpr : {gam : Vect n Nat} -> {x, y : Nat} 
            -> LExpr gam x -> LExpr gam y  
            -> (z ** (LExpr gam z, x+y=z))
      plusLExpr {x=x} ex LZero =
        (_ ** (ex, rewrite (plusZeroRightNeutral x) in Refl))            
      plusLExpr ex (LPlus e i) =
        let (xRec ** (rec, prfRec)) = plusLExpr ex e in
            (_ ** (LPlus rec i, ?MplusLExpr))

\end{verbatim}
\end{center}
\caption{Production of the left associative form}
\figrule
\end{figure}
In the case of an addition $Plus\ ex\ ey$, the function $expr\_l$ does the job of normalisation recursively on $ex$ and on $ey$, and then it uses the sub-function $plusLExpr$ to normalise the addition of these two --already normalised-- terms. This sub-function $plusLExpr$ has two kind of simplifications to do. When the second argument is an $LZero$, it simply returns its first arguments along with the justification for this rewriting, which obviously uses $plusZeroRightNeutral$. However, when the second argument is an $LPlus\ e\ i$, it continues recursively by computing $plusLExpr\ ex\ e$, and it finally adds $i$ to it. That had for effect to move the parenthesis on the left, and this treatment is going to be justified during the proof of the meta variable $MplusLExpr$ by the use of $plusAssociative$.

This metavariable $MplusLExpr$ requires us to prove the goal : $x1 + (x2 + index\ i\ \Gamma) = xrec + index\ i\ \Gamma$ in a context where we've got, amongst other things, $prfRec : x1 + x2 = xrec$.
By using the property of associativity on the goal, we now need to prove $(x1 + x2) + index\ i\ \Gamma$, which can be done by rewriting the proof $prfRec$ obtained recursively.

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
MplusLExpr = proof {
  intros
  rewrite (sym (plusAssociative x1 x2 (index i gam))); rewrite prf; 
  exact Refl;
}
\end{verbatim}
\end{center}
\caption{Proof of the metavariable plusLExpr1}
\figrule
\end{figure}

It is really important to understand that the root of the automatic building of the desired proof of $x=y$ happens precisely in these nested use of $plusZeroRightNeutral$ and $plusAssociative$ that we've seen in the definition of $expr\_l$ and of its meta-variable $MplusLExpr$. These proofs replace the arithmetical proofs that we were doing previously by hand, like in section 1 with the lemma $adc\_lemma\_2$. \\
\\
Using this new datatype $LExpr$ has changed the representation of our encoded lists, so we need to convert back an $LExpr\ \Gamma\ x$ to an $Expr\ \Gamma\ x$. The function $l\_expr$ does this easy task.
\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
l_expr : LExpr gam x -> Expr gam x
l_expr LZero = Zero
l_expr (LPlus x i) = Plus (l_expr x) (Var i)
\end{verbatim}
\end{center}
\caption{Going back from LExpr to Expr}
\figrule
\end{figure}


We notice that in order to transform the expression into its left associative equivalent representation, we've effectively needed to know where the variables and the $Z$ constants are : the functions $expr\_l$ and $l\_expr$ are doing different treatments for these two possibilities. \\
\\

We can now define the reduction, which is just the composition of the two previous functions $expr\_l$ and $l\_expr$:

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
  reduce : Expr gam x -> (x' ** (Expr gam x', x = x'))
  reduce e = 
     let (x' ** (e', prf)) = expr_l e in
         (x' ** (l_expr e', prf))
\end{verbatim}
\end{center}
\caption{Reduction function}
\figrule
\end{figure}


At the moment, what we've got is not exactly a real tactic, in the sense that we only have a function which produces a value of type $Maybe (x = y)$. A real tactic would be a wrapper of this function that could properly fail with an error message when the two terms are not equal. However, here, when $x\ne y$, the function $testEq$ will simply produce the value $Nothing$. \\

\subsection{Usage of the "tactic"}

It's now time to see how to use this minimalist "tactic".
Let's define two expressions $e1$ and $e2$, respectively representing the numbers $((x + y) + (x + z))$ and $(x + ((y + x) + z))$ in the context $[x, y, z]$ of three abstracted variables.


\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
e1 : (x, y, z : Nat) 
    -> Expr [x, y, z] ((x+y) + (x+z))
e1 x y z = Plus (Plus (Var fZ) 
                      (Var (fS fZ))) 
                (Plus (Var fZ) 
                      (Var (fS (fS fZ))))

e2 : (x, y, z : Nat) 
     -> Expr [x, y, z] (x + ((y + x) + z))
e2 x y z = Plus (Var fZ) 
                (Plus (Plus (Var (fS fZ)) 
                            (Var fZ)) 
                      (Var (fS (fS fZ))))
\end{verbatim}
\end{center}
\caption{Two test expressions}
\figrule
\end{figure}

The numbers denoted by the expressions $e1$ and $e2$ are equal, and we can generate a proof of this by using $testEq$.

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
e1_e2_testEq : (x, y, z : Nat) 
             -> Maybe (((x + y) + (x + z)) = (x + ((y + x) + z)))
e1_e2_testEq x y z = testEq (e1 x y z) (e2 x y z)
\end{verbatim}
\end{center}
\caption{Test of equality betwen e1 and e2}
\figrule
\end{figure}


And if we ask for the evaluation of this term, we should obtain $Just$ and a proof of equality between the two underlying numbers.

\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}
#\x => \y => \z => e1_e2_testEq x y z

\x => \y => \z => Just (replace (sym (replace (sym (replace 
(sym (plusAssociative x 0 y)) (replace (replace (sym 
(plusZeroRightNeutral x)) Refl) Refl))) (replace (sym 
(replace (sym (plusAssociative x 0 z)) (replace (replace 
(sym (plusZeroRightNeutral x)) Refl) Refl))) (replace (sym 
(plusAssociative (x+y) x z)) [...]
: (x : Nat) -> (y : Nat) -> (z : Nat) 
  -> Maybe ((x + y) + x + z 
            = x + (y + x) + z)
\end{verbatim}
\end{center}
\caption{Obtained proof}
\figrule
\end{figure}

And we effectively get the proof of equality we wanted. As expected, this proof uses the properties of associativity ($plusAssociative$) and the property of neutrality of $Z$ for $+$ ($plusZeroRightNeutral$).


\subsection{Construction of the reflected terms}

Note that for the moment, even if what we have is perfectly usable and works, we had to create these encodings $e1$ and $e2$ by hand, which is easy but time consuming. We have replaced the (potentially hard) problem of proving something by the simplest problem of simply creating some encodings. This is already a huge simplification, because as it can be seen in the definitions of $e1$ and $e2$, the reflected terms completely follow the structure of the expression to encode : there's absolutely no creativity needed for this task, unlike the proving activity. The way to create the encodings is in fact so systematic that, of course, we would like to automatize it in order to get a real and completely automatic tactic.

However, we can also note that even when done by hand, there is no room for making mistakes in this simple task of encoding : we simply can't generate a wrong encoding : if $e1$ and $e2$ are not respectively reflecting $((x+y) + (x+z))$ and $(x + ((y + x) + z))$ then these definitions won't typecheck because the expected and real index won't match. [perhaps giving example of rejected wrong definition for $e1$ here]

We still want an automatic way of constructing these reflected terms because what we have so far is enough to demonstrate that our "prover" works, but that's not enough for being used intensively. We want to program an automatic way of going from the concrete values (of type $Nat$), to the reflected terms (of type $Expr$). The only way to do that is to inspect the abstract syntax tree of the concrete value.
By using Idris's reflection mechanism, we can tag a function with the keyword "\%reflection", which means that this function runs on syntax instead of values. That implies that when applying this function to some arguments, the arguments aren't reduce before the evaluation of this function.


\begin{figure}[H]
\figrule
\begin{center}
\begin{verbatim}

%reflection
reflectList : (G : List Nat) ->
          (x : Nat) -> (G' ** Expr (G ++ G') x)
reflectList G Z = ([] ** Zero)

...

\end{verbatim}
\end{center}
\caption{Reflection for natural numbers}
\figrule
\end{figure}





