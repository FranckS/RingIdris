\section{Introduction}

There are multiple ways for increasing confidence in the software we build. One of them is to formally prove its correctness, using a proof assistant. These proofs assistants enable to write code, logical statements and proof in the same language, and offer the guarantee that every proof will be automatically checked. The appearance of these languages, and the way to write programs in them, varies from one to another. Many of them are functional programming languages, like Coq, Idris and Agda, and others, like the B-Toolkit belong to the imperative paradigm. These different paradigm are internally supported by different logic. Systems like Coq, Idris and Agda are based on various higher order logic (CoC, a variant of ML and LUO respectively) and are realisations of the Curry-Howard correspondence, while the formal B method is based on Hoare logic. These different foundations lead to different philosophies and different ways to implement and verify a software, but all of them greatly increase the confidence of the produced software. However, these guarantee tend to be too often consider as perfect, when they are in fact far from it. Knuth was --certainly ironically-- saying "Beware of bugs in the above code; I have only proved it correct, not tried it". The reality is precisely that a proof isn't enough. When we've proved the correctness of a software, we've only gained the guarantee expressed by the proven lemma, and nothing more. Thus, if we want to trust the proven software, we're now forced to believe that there is adequacy between the formal specification, written in some formal logic, and the informal requirements. A switch has occurred. We used to have to trust code, but we now have to trust logical statements and predicates. But when the specification is too often as complicated as the code, why should we blindly believe in it, when we've first refused to blindly trust the code? Too few people realize that the proving activity has created this silent assumption of adequacy between the formalised specification and the original real requirements, usually written in english. Of course, the proving activity has undeniably increased the confidence in the produced software, and the aim of this paper isn't to say otherwise. Many successful applications of these formal methods [CompCert ref, etc] have showed the value of developing a machined-checked proof of correctness. Instead, the primary aim of this paper is to raise awareness of the adequacy concern, and to see how heterogeneous approach, that mix both proofs and tests, can help to go a step forward in the certification process, in the context of proof assistants based on type theory.
More precisely, we :
\begin{itemize}
\item Show how a formal specification can be wrong or under specified, leading to a proof that does not convey the expected guarantee, and the usual and naive approaches to this problem (section 2)
\item Show the distinction between structural properties, and the properties on the content, and how to identify what needs to be tested for the former category (section 3).
\item Show how to conduct these tests in the proof assistant itself, by automatically generating terms, and how to automate these tests as far as possible (section 4)
\end{itemize}




\


To add in the intro :

- Many companies doing critical software don't believe in 
(one sentence)

2) How a formal specification can go wrong

- How a predicate can be insufficient, leading to an under-specified behaviour
(it is then unfortunately very easy to produce a "formally-verified" wrong implementation)
[a quarter of a page : from 1.75 to 2.00]

- Different known approach for enhancing the confidence in a predicate : defining many of them and proving their equivalence, doing small tests on some data that we know belong or not to the predicate, etc... 
[half a page : from 2.00 to 2.30]


3) "Structural" VS "on content" properties

- differences between "structural" properties, and properties about the "content".
[half a page : from 2.30 to 3.00]


4) Testing the predicates inside the proof assistant

- it is often easier to generate things rather than to characterise them 
[a quarter of a page : from 3.00 to 3.15]

- testing the structural properties by automatic generation of the terms that should satisfy them, and running a decision procedure on them.
[half a page : from 3.15 to 3.45]

- Going a step further : "test automation by proof", which automatises all of them at once, by doing (another) formal proof, which shows that there is equivalence between all the terms generated and the original predicate that we wanted to certify correct according to the original specification.
[half a page : from 3.45 to 4.15]


5) Conclusions, related work, and future work (a quarter of a page) 
[from 4.15 to 4.30]


6) References 
[a quarter of a page : from 4.30 to 4.45]



